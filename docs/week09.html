<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="John D. Storey" />
  <title>QCB 508 – Week 9</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="libs/reveal.js-3.3.0.1/css/reveal.css"/>



<link rel="stylesheet" href="libs/reveal.js-3.3.0.1/css/theme/simple.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }


  </style>

    <style type="text/css">code{white-space: pre;}</style>

    <link rel="stylesheet" href="customization/custom.css"/>

<!-- Printing and PDF exports -->
<script id="paper-css" type="application/dynamic-css">

/* Default Print Stylesheet Template
   by Rob Glazebrook of CSSnewbie.com
   Last Updated: June 4, 2008

   Feel free (nay, compelled) to edit, append, and
   manipulate this file as you see fit. */


@media print {

	/* SECTION 1: Set default width, margin, float, and
	   background. This prevents elements from extending
	   beyond the edge of the printed page, and prevents
	   unnecessary background images from printing */
	html {
		background: #fff;
		width: auto;
		height: auto;
		overflow: visible;
	}
	body {
		background: #fff;
		font-size: 20pt;
		width: auto;
		height: auto;
		border: 0;
		margin: 0 5%;
		padding: 0;
		overflow: visible;
		float: none !important;
	}

	/* SECTION 2: Remove any elements not needed in print.
	   This would include navigation, ads, sidebars, etc. */
	.nestedarrow,
	.controls,
	.fork-reveal,
	.share-reveal,
	.state-background,
	.reveal .progress,
	.reveal .backgrounds {
		display: none !important;
	}

	/* SECTION 3: Set body font face, size, and color.
	   Consider using a serif font for readability. */
	body, p, td, li, div {
		font-size: 20pt!important;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		color: #000;
	}

	/* SECTION 4: Set heading font face, sizes, and color.
	   Differentiate your headings from your body text.
	   Perhaps use a large sans-serif for distinction. */
	h1,h2,h3,h4,h5,h6 {
		color: #000!important;
		height: auto;
		line-height: normal;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		text-shadow: 0 0 0 #000 !important;
		text-align: left;
		letter-spacing: normal;
	}
	/* Need to reduce the size of the fonts for printing */
	h1 { font-size: 28pt !important;  }
	h2 { font-size: 24pt !important; }
	h3 { font-size: 22pt !important; }
	h4 { font-size: 22pt !important; font-variant: small-caps; }
	h5 { font-size: 21pt !important; }
	h6 { font-size: 20pt !important; font-style: italic; }

	/* SECTION 5: Make hyperlinks more usable.
	   Ensure links are underlined, and consider appending
	   the URL to the end of the link for usability. */
	a:link,
	a:visited {
		color: #000 !important;
		font-weight: bold;
		text-decoration: underline;
	}
	/*
	.reveal a:link:after,
	.reveal a:visited:after {
		content: " (" attr(href) ") ";
		color: #222 !important;
		font-size: 90%;
	}
	*/


	/* SECTION 6: more reveal.js specific additions by @skypanther */
	ul, ol, div, p {
		visibility: visible;
		position: static;
		width: auto;
		height: auto;
		display: block;
		overflow: visible;
		margin: 0;
		text-align: left !important;
	}
	.reveal pre,
	.reveal table {
		margin-left: 0;
		margin-right: 0;
	}
	.reveal pre code {
		padding: 20px;
		border: 1px solid #ddd;
	}
	.reveal blockquote {
		margin: 20px 0;
	}
	.reveal .slides {
		position: static !important;
		width: auto !important;
		height: auto !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 0 !important;
		zoom: 1 !important;

		overflow: visible !important;
		display: block !important;

		text-align: left !important;
		-webkit-perspective: none;
		   -moz-perspective: none;
		    -ms-perspective: none;
		        perspective: none;

		-webkit-perspective-origin: 50% 50%;
		   -moz-perspective-origin: 50% 50%;
		    -ms-perspective-origin: 50% 50%;
		        perspective-origin: 50% 50%;
	}
	.reveal .slides section {
		visibility: visible !important;
		position: static !important;
		width: auto !important;
		height: auto !important;
		display: block !important;
		overflow: visible !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 60px 20px !important;
		z-index: auto !important;

		opacity: 1 !important;

		page-break-after: always !important;

		-webkit-transform-style: flat !important;
		   -moz-transform-style: flat !important;
		    -ms-transform-style: flat !important;
		        transform-style: flat !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;

		-webkit-transition: none !important;
		   -moz-transition: none !important;
		    -ms-transition: none !important;
		        transition: none !important;
	}
	.reveal .slides section.stack {
		padding: 0 !important;
	}
	.reveal section:last-of-type {
		page-break-after: avoid !important;
	}
	.reveal section .fragment {
		opacity: 1 !important;
		visibility: visible !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;
	}
	.reveal section img {
		display: block;
		margin: 15px 0px;
		background: rgba(255,255,255,1);
		border: 1px solid #666;
		box-shadow: none;
	}

	.reveal section small {
		font-size: 0.8em;
	}

}  
</script>


<script id="pdf-css" type="application/dynamic-css">
    
/**
 * This stylesheet is used to print reveal.js
 * presentations to PDF.
 *
 * https://github.com/hakimel/reveal.js#pdf-export
 */

* {
	-webkit-print-color-adjust: exact;
}

body {
	margin: 0 auto !important;
	border: 0;
	padding: 0;
	float: none !important;
	overflow: visible;
}

html {
	width: 100%;
	height: 100%;
	overflow: visible;
}

/* Remove any elements not needed in print. */
.nestedarrow,
.reveal .controls,
.reveal .progress,
.reveal .playback,
.reveal.overview,
.fork-reveal,
.share-reveal,
.state-background {
	display: none !important;
}

h1, h2, h3, h4, h5, h6 {
	text-shadow: 0 0 0 #000 !important;
}

.reveal pre code {
	overflow: hidden !important;
	font-family: Courier, 'Courier New', monospace !important;
}

ul, ol, div, p {
	visibility: visible;
	position: static;
	width: auto;
	height: auto;
	display: block;
	overflow: visible;
	margin: auto;
}
.reveal {
	width: auto !important;
	height: auto !important;
	overflow: hidden !important;
}
.reveal .slides {
	position: static;
	width: 100%;
	height: auto;

	left: auto;
	top: auto;
	margin: 0 !important;
	padding: 0 !important;

	overflow: visible;
	display: block;

	-webkit-perspective: none;
	   -moz-perspective: none;
	    -ms-perspective: none;
	        perspective: none;

	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
	   -moz-perspective-origin: 50% 50%;
	    -ms-perspective-origin: 50% 50%;
	        perspective-origin: 50% 50%;
}

.reveal .slides section {
	page-break-after: always !important;

	visibility: visible !important;
	position: relative !important;
	display: block !important;
	position: relative !important;

	margin: 0 !important;
	padding: 0 !important;
	box-sizing: border-box !important;
	min-height: 1px;

	opacity: 1 !important;

	-webkit-transform-style: flat !important;
	   -moz-transform-style: flat !important;
	    -ms-transform-style: flat !important;
	        transform-style: flat !important;

	-webkit-transform: none !important;
	   -moz-transform: none !important;
	    -ms-transform: none !important;
	        transform: none !important;
}

.reveal section.stack {
	margin: 0 !important;
	padding: 0 !important;
	page-break-after: avoid !important;
	height: auto !important;
	min-height: auto !important;
}

.reveal img {
	box-shadow: none;
}

.reveal .roll {
	overflow: visible;
	line-height: 1em;
}

/* Slide backgrounds are placed inside of their slide when exporting to PDF */
.reveal section .slide-background {
	display: block !important;
	position: absolute;
	top: 0;
	left: 0;
	width: 100%;
	z-index: -1;
}

/* All elements should be above the slide-background */
.reveal section>* {
	position: relative;
	z-index: 1;
}

/* Display slide speaker notes when 'showNotes' is enabled */
.reveal .speaker-notes-pdf {
	display: block;
	width: 100%;
	max-height: none;
	left: auto;
	top: auto;
	z-index: 100;
}

/* Display slide numbers when 'slideNumber' is enabled */
.reveal .slide-number-pdf {
	display: block;
	position: absolute;
	font-size: 14px;
}

</script>


<script>
var style = document.createElement( 'style' );
style.type = 'text/css';
var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
var style_script = document.getElementById(style_script_id).text;
style.innerHTML = style_script;
document.getElementsByTagName('head')[0].appendChild(style);
</script>

    <link href="libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
</head>
<body>
<style type="text/css">
p { 
  text-align: left; 
  }
.reveal pre code { 
  color: #000000; 
  background-color: rgb(240,240,240);
  font-size: 1.15em;
  border:none; 
  }
.reveal section img { 
  background:none; 
  border:none; 
  box-shadow:none;
  height: 500px;
  }
}
</style>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">QCB 508 – Week 9</h1>
    <h2 class="author">John D. Storey</h2>
    <h3 class="date">Spring 2017</h3>
</section>

<section><section id="section" class="titleslide slide level1"><h1><img src="images/howto.jpg"></img></h1></section></section>
<section><section id="statistical-models" class="titleslide slide level1"><h1>Statistical Models</h1></section><section id="probabilistic-models" class="slide level2">
<h2>Probabilistic Models</h2>
<p>So far we have covered inference of paramters that quantify a population of interest.</p>
<p>This is called inference of probabilistic models.</p>
</section><section id="multivariate-models" class="slide level2">
<h2>Multivariate Models</h2>
<p>Some of the probabilistic models we considered involve calculating conditional probabilities such as <span class="math inline">\(\Pr({\boldsymbol{Z}}| {\boldsymbol{X}}; {\boldsymbol{\theta}})\)</span> or <span class="math inline">\(\Pr({\boldsymbol{\theta}}| {\boldsymbol{X}})\)</span>.</p>
<p>It is often the case that we would like to build a model that <em>explains the variation of one variable in terms of other variables</em>. <strong>Statistical modeling</strong> typically refers to this goal.</p>
</section><section id="variables" class="slide level2">
<h2>Variables</h2>
<p>Let’s suppose our does comes in the form <span class="math inline">\(({\boldsymbol{X}}_1, Y_1), ({\boldsymbol{X}}_2, Y_2), \ldots, ({\boldsymbol{X}}_n, Y_n) \sim F\)</span>.</p>
<p>We will call <span class="math inline">\({\boldsymbol{X}}_i = (X_{i1}, X_{i2}, \ldots, X_{ip}) \in \mathbb{R}_{1 \times p}\)</span> the <strong>explanatory variables</strong> and <span class="math inline">\(Y_i \in \mathbb{R}\)</span> the <strong>dependent variable</strong> or <strong>response variable</strong>.</p>
<p>We can collect all variables as matrices</p>
<p><span class="math display">\[ {\boldsymbol{Y}}_{n \times 1} \ \mbox{ and } \ {\boldsymbol{X}}_{n \times p}\]</span></p>
<p>where each row is a unique observation.</p>
</section><section id="statistical-model" class="slide level2">
<h2>Statistical Model</h2>
<p>Statistical models are concerned with <em>how</em> variables are dependent. The most general model would be to infer</p>
<p><span class="math display">\[
\Pr(Y | {\boldsymbol{X}}) = h({\boldsymbol{X}})
\]</span></p>
<p>where we would specifically study the form of <span class="math inline">\(h(\cdot)\)</span> to understand how <span class="math inline">\(Y\)</span> is dependent on <span class="math inline">\({\boldsymbol{X}}\)</span>.</p>
<p>A more modest goal is to infer the transformed conditional expecation</p>
<p><span class="math display">\[
g\left({\operatorname{E}}[Y | {\boldsymbol{X}}]\right) = h({\boldsymbol{X}})
\]</span></p>
<p>which sometimes leads us back to an estimate of <span class="math inline">\(\Pr(Y | {\boldsymbol{X}})\)</span>.</p>
</section><section id="parametric-vs-nonparametric" class="slide level2">
<h2>Parametric vs Nonparametric</h2>
<p>A <strong>parametric</strong> model is a pre-specified form of <span class="math inline">\(h(X)\)</span> whose terms can be characterized by a formula and interpreted. This usually involves parameters on which inference can be performed, such as coefficients in a linear model.</p>
<p>A <strong>nonparametric</strong> model is a data-driven form of <span class="math inline">\(h(X)\)</span> that is often very flexible and is not easily expressed or intepreted. A nonparametric model often does not include parameters on which we can do inference.</p>
</section><section id="simple-linear-regression" class="slide level2">
<h2>Simple Linear Regression</h2>
<p>For random variables <span class="math inline">\((X_1, Y_1), (X_2, Y_2), \ldots, (X_n, Y_n)\)</span>, <strong>simple linear regression</strong> estimates the model</p>
<p><span class="math display">\[
Y_i  = \beta_1 + \beta_2 X_i + E_i
\]</span></p>
<p>where <span class="math inline">\({\operatorname{E}}[E_i] = 0\)</span>, <span class="math inline">\({\operatorname{Var}}(E_i) = \sigma^2\)</span>, and <span class="math inline">\({\operatorname{Cov}}(E_i, E_j) = 0\)</span> for all <span class="math inline">\(1 \leq i, j \leq n\)</span> and <span class="math inline">\(i \not= j\)</span>.</p>
<p>Note that in this model <span class="math inline">\({\operatorname{E}}[Y | X] = \beta_1 + \beta_2 X.\)</span></p>
</section><section id="ordinary-least-squares" class="slide level2">
<h2>Ordinary Least Squares</h2>
<p><strong>Ordinary least squares</strong> (OLS) estimates the model</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp; = \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_p X_{ip} + E_i \\
 &amp; = {\boldsymbol{X}}_i {\boldsymbol{\beta}}+ E_i
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\({\rm E}[E_i] = 0\)</span>, <span class="math inline">\({\rm Var}(E_i) = \sigma^2\)</span>, and <span class="math inline">\({\operatorname{Cov}}(E_i, E_j) = 0\)</span> for all <span class="math inline">\(1 \leq i, j \leq n\)</span> and <span class="math inline">\(i \not= j\)</span>.</p>
<p>Note that typically <span class="math inline">\(X_{i1} = 1\)</span> for all <span class="math inline">\(i\)</span> so that <span class="math inline">\(\beta_1 X_{i1} = \beta_1\)</span> serves as the intercept.</p>
</section><section id="generalized-least-squares" class="slide level2">
<h2>Generalized Least Squares</h2>
<p><strong>Generalized least squares</strong> (GLS) assumes the same model as OLS, except it allows for <strong>heteroskedasticity</strong> and <strong>covariance</strong> among the <span class="math inline">\(E_i\)</span>. Specifically, it is assumed that <span class="math inline">\({\boldsymbol{E}}= (E_1, \ldots, E_n)^T\)</span> is distributed as</p>
<p><span class="math display">\[
{\boldsymbol{E}}_{n \times 1} \sim (\boldsymbol{0}, {\boldsymbol{\Sigma}})
\]</span> where <span class="math inline">\(\boldsymbol{0}\)</span> is the expected value <span class="math inline">\({\boldsymbol{\Sigma}}= (\sigma_{ij})\)</span> is the <span class="math inline">\(n \times n\)</span> symmetric covariance matrix.</p>
</section><section id="matrix-form-of-linear-models" class="slide level2">
<h2>Matrix Form of Linear Models</h2>
<p>We can write the models as</p>
<p><span class="math display">\[
{\boldsymbol{Y}}_{n \times 1} = {\boldsymbol{X}}_{n \times p} {\boldsymbol{\beta}}_{p \times 1} + {\boldsymbol{E}}_{n \times 1}
\]</span></p>
<p>where simple linear regression, OLS, and GLS differ in the value of <span class="math inline">\(p\)</span> or the distribution of the <span class="math inline">\(E_i\)</span>. We can also write the conditional expecation and covariance as</p>
<p><span class="math display">\[
{\operatorname{E}}[{\boldsymbol{Y}}| {\boldsymbol{X}}] = {\boldsymbol{X}}{\boldsymbol{\beta}}, \ {\operatorname{Cov}}({\boldsymbol{Y}}| {\boldsymbol{X}}) = {\boldsymbol{\Sigma}}.
\]</span></p>
</section><section id="least-squares-regression" class="slide level2">
<h2>Least Squares Regression</h2>
<p>In simple linear regression, OLS, and GLS, the <span class="math inline">\({\boldsymbol{\beta}}\)</span> parameters are fit by minimizing the sum of squares between <span class="math inline">\({\boldsymbol{Y}}\)</span> and <span class="math inline">\({\boldsymbol{X}}{\boldsymbol{\beta}}\)</span>.</p>
<p>Fitting these models by “least squares” satisfies two types of optimality:</p>
<ol type="1">
<li><a href="https://en.wikipedia.org/wiki/Gauss–Markov_theorem">Gauss-Markov Theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Maximum_likelihood">Maximum likelihood estimate</a> when in addition <span class="math inline">\({\boldsymbol{E}}\sim \mbox{MVN}_n(\boldsymbol{0}, {\boldsymbol{\Sigma}})\)</span></li>
</ol>
<p>Details will follow on these.</p>
</section><section id="generalized-linear-models" class="slide level2">
<h2>Generalized Linear Models</h2>
<p>The generalized linear model (GLM) builds from OLS and GLS to allow the response variable to be distributed according to an exponential family distribution. Suppose that <span class="math inline">\(\eta(\theta)\)</span> is function of the expected value into the natural parameter. The estimated model is</p>
<p><span class="math display">\[
\eta\left({\operatorname{E}}[Y | {\boldsymbol{X}}]\right) = {\boldsymbol{X}}{\boldsymbol{\beta}}\]</span></p>
<p>which is fit by maximized likelihood estimation.</p>
</section><section id="generalized-additive-models" class="slide level2">
<h2>Generalized Additive Models</h2>
<p>Next week, we will finally arrive at inferring semiparametric models where <span class="math inline">\(Y | {\boldsymbol{X}}\)</span> is distributed according to an exponential family distribution. The models, which are called <strong>generalized additive models</strong> (GAMs), will be of the form</p>
<p><span class="math display">\[
\eta\left({\operatorname{E}}[Y | {\boldsymbol{X}}]\right) = \sum_{j=1}^p \sum_{k=1}^d h_k(X_{j})
\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is the canonical link function and the <span class="math inline">\(h_k(\cdot)\)</span> functions are very flexible.</p>
</section><section id="some-trade-offs" class="slide level2">
<h2>Some Trade-offs</h2>
<p>There are several important trade-offs encountered in statistical modeling:</p>
<ul>
<li>Bias vs variance</li>
<li>Accuracy vs computational time</li>
<li>Flexibility vs intepretability</li>
</ul>
<p>These are not mutually exclusive phenomena.</p>
</section><section id="bias-and-variance" class="slide level2">
<h2>Bias and Variance</h2>
<p>Suppose we estimate <span class="math inline">\(Y = h({\boldsymbol{X}}) + E\)</span> by some <span class="math inline">\(\hat{Y} = \hat{h}({\boldsymbol{X}})\)</span>. The following bias-variance trade-off exists:</p>
<p><span class="math display">\[
\begin{aligned}
{\operatorname{E}}\left[\left(Y - \hat{Y}\right)^2\right] &amp; = {\rm E}\left[\left(h({\boldsymbol{X}}) + E - \hat{h}({\boldsymbol{X}})\right)^2\right] \\
\ &amp; = {\rm E}\left[\left(h({\boldsymbol{X}}) - \hat{h}({\boldsymbol{X}})\right)^2\right] + {\rm Var}(E) \\
\ &amp; = \left(h({\boldsymbol{X}}) - {\rm E}[\hat{h}({\boldsymbol{X}})]\right)^2 + {\rm Var}\left(\hat{h}({\boldsymbol{X}})\right)^2 + {\rm Var}(E) \\ 
\ &amp; = \mbox{bias}^2 + \mbox{variance} + {\rm Var}(E)
\end{aligned}
\]</span></p>
</section></section>
<section><section id="motivating-examples" class="titleslide slide level1"><h1>Motivating Examples</h1></section><section id="sample-correlation" class="slide level2">
<h2>Sample Correlation</h2>
<p>Least squares regression “modelizes” correlation. Suppose we observe <span class="math inline">\(n\)</span> pairs of data <span class="math inline">\((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)</span>. Their sample correlation is</p>
<span class="math display">\[\begin{eqnarray}
r_{xy} &amp; = &amp; \frac{\sum_{i=1}^n (x_i - \overline{x}) (y_i - \overline{y})}{\sqrt{\sum_{i=1}^n (x_i - \overline{x})^2 \sum_{i=1}^n (y_i - \overline{y})^2}} \\
\ &amp; = &amp; \frac{\sum_{i=1}^n (x_i - \overline{x}) (y_i - \overline{y})}{(n-1) s_x s_y}
\end{eqnarray}\]</span>
<p>where <span class="math inline">\(s_x\)</span> and <span class="math inline">\(s_y\)</span> are the sample standard deviations of each measured variable.</p>
</section><section id="example-hand-size-vs.-height" class="slide level2">
<h2>Example: Hand Size Vs. Height</h2>
<pre class="r"><code>&gt; library(&quot;MASS&quot;)
&gt; data(&quot;survey&quot;, package=&quot;MASS&quot;)
&gt; head(survey)
     Sex Wr.Hnd NW.Hnd W.Hnd    Fold Pulse    Clap Exer Smoke
1 Female   18.5   18.0 Right  R on L    92    Left Some Never
2   Male   19.5   20.5  Left  R on L   104    Left None Regul
3   Male   18.0   13.3 Right  L on R    87 Neither None Occas
4   Male   18.8   18.9 Right  R on L    NA Neither None Never
5   Male   20.0   20.0 Right Neither    35   Right Some Never
6 Female   18.0   17.7 Right  L on R    64   Right Some Never
  Height      M.I    Age
1 173.00   Metric 18.250
2 177.80 Imperial 17.583
3     NA     &lt;NA&gt; 16.917
4 160.00   Metric 20.333
5 165.00   Metric 23.667
6 172.72 Imperial 21.000</code></pre>
</section><section class="slide level2">

<pre class="r"><code>&gt; ggplot(data = survey, mapping=aes(x=Wr.Hnd, y=Height)) +
+   geom_point() + geom_vline(xintercept=mean(survey$Wr.Hnd, na.rm=TRUE)) +
+   geom_hline(yintercept=mean(survey$Height, na.rm=TRUE))</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-3-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="cor.-of-hand-size-and-height" class="slide level2">
<h2>Cor. of Hand Size and Height</h2>
<pre class="r"><code>&gt; cor.test(x=survey$Wr.Hnd, y=survey$Height)

    Pearson&#39;s product-moment correlation

data:  survey$Wr.Hnd and survey$Height
t = 10.792, df = 206, p-value &lt; 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.5063486 0.6813271
sample estimates:
      cor 
0.6009909 </code></pre>
</section><section id="lr-hand-sizes" class="slide level2">
<h2>L/R Hand Sizes</h2>
<pre class="r"><code>&gt; ggplot(data = survey) +
+   geom_point(aes(x=Wr.Hnd, y=NW.Hnd))</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="correlation-of-hand-sizes" class="slide level2">
<h2>Correlation of Hand Sizes</h2>
<pre class="r"><code>&gt; cor.test(x=survey$Wr.Hnd, y=survey$NW.Hnd)

    Pearson&#39;s product-moment correlation

data:  survey$Wr.Hnd and survey$NW.Hnd
t = 45.712, df = 234, p-value &lt; 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.9336780 0.9597816
sample estimates:
      cor 
0.9483103 </code></pre>
</section><section id="davis-data" class="slide level2">
<h2>Davis Data</h2>
<pre class="r"><code>&gt; library(&quot;car&quot;)
&gt; data(&quot;Davis&quot;, package=&quot;car&quot;)</code></pre>
<pre class="r"><code>&gt; htwt &lt;- tbl_df(Davis)
&gt; htwt[12,c(2,3)] &lt;- htwt[12,c(3,2)]
&gt; head(htwt)
# A tibble: 6 × 5
     sex weight height repwt repht
  &lt;fctr&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;
1      M     77    182    77   180
2      F     58    161    51   159
3      F     53    161    54   158
4      M     68    177    70   175
5      F     59    157    59   155
6      M     76    170    76   165</code></pre>
</section><section id="height-and-weight" class="slide level2">
<h2>Height and Weight</h2>
<pre class="r"><code>&gt; ggplot(htwt) + 
+   geom_point(aes(x=height, y=weight, color=sex), size=2, alpha=0.5) +
+   scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;))</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-9-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="correlation-of-height-and-weight" class="slide level2">
<h2>Correlation of Height and Weight</h2>
<pre class="r"><code>&gt; cor.test(x=htwt$height, y=htwt$weight)

    Pearson&#39;s product-moment correlation

data:  htwt$height and htwt$weight
t = 17.04, df = 198, p-value &lt; 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.7080838 0.8218898
sample estimates:
      cor 
0.7710743 </code></pre>
</section><section id="correlation-among-females" class="slide level2">
<h2>Correlation Among Females</h2>
<pre class="r"><code>&gt; htwt %&gt;% filter(sex==&quot;F&quot;) %&gt;%  
+   cor.test(~ height + weight, data = .)

    Pearson&#39;s product-moment correlation

data:  height and weight
t = 6.2801, df = 110, p-value = 6.922e-09
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.3627531 0.6384268
sample estimates:
      cor 
0.5137293 </code></pre>
</section><section id="correlation-among-males" class="slide level2">
<h2>Correlation Among Males</h2>
<pre class="r"><code>&gt; htwt %&gt;% filter(sex==&quot;M&quot;) %&gt;%  
+   cor.test(~ height + weight, data = .)

    Pearson&#39;s product-moment correlation

data:  height and weight
t = 5.9388, df = 86, p-value = 5.922e-08
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.3718488 0.6727460
sample estimates:
      cor 
0.5392906 </code></pre>
<p>Why are the stratified correlations lower?</p>
</section></section>
<section><section id="simple-linear-regression-1" class="titleslide slide level1"><h1>Simple Linear Regression</h1></section><section id="definition" class="slide level2">
<h2>Definition</h2>
<p>For random variables <span class="math inline">\((X_1, Y_1), (X_2, Y_2), \ldots, (X_n, Y_n)\)</span>, <strong>simple linear regression</strong> estimates the model</p>
<p><span class="math display">\[
Y_i  = \beta_1 + \beta_2 X_i + E_i
\]</span></p>
<p>where <span class="math inline">\({\operatorname{E}}[E_i] = 0\)</span>, <span class="math inline">\({\operatorname{Var}}(E_i) = \sigma^2\)</span>, and <span class="math inline">\({\operatorname{Cov}}(E_i, E_j) = 0\)</span> for all <span class="math inline">\(1 \leq i, j \leq n\)</span> and <span class="math inline">\(i \not= j\)</span>.</p>
</section><section id="rationale" class="slide level2">
<h2>Rationale</h2>
<ul>
<li><p><strong>Least squares linear regression</strong> is one of the simplest and most useful modeling systems for building a model that explains the variation of one variable in terms of other variables.</p></li>
<li><p>It is simple to fit, it satisfies some optimality criteria, and it is straightforward to check assumptions on the data so that statistical inference can be performed.</p></li>
</ul>
</section><section id="setup" class="slide level2">
<h2>Setup</h2>
<ul>
<li><p>Suppose that we have observed <span class="math inline">\(n\)</span> pairs of data <span class="math inline">\((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)</span>.</p></li>
<li><p><strong>Least squares linear regression</strong> models variation of the <strong>response variable</strong> <span class="math inline">\(y\)</span> in terms of the <strong>explanatory variable</strong> <span class="math inline">\(x\)</span> in the form of <span class="math inline">\(\beta_1 + \beta_2 x\)</span>, where <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are chosen to satisfy a least squares optimization.</p></li>
</ul>
</section><section id="line-minimizing-squared-error" class="slide level2">
<h2>Line Minimizing Squared Error</h2>
<p>The least squares regression line is formed from the value of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> that minimize:</p>
<p><span class="math display">\[\sum_{i=1}^n \left( y_i - \beta_1 - \beta_2 x_i \right)^2.\]</span></p>
<p>For a given set of data, there is a unique solution to this minimization as long as there are at least two unique values among <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>.</p>
<p>Let <span class="math inline">\(\hat{\beta_1}\)</span> and <span class="math inline">\(\hat{\beta_2}\)</span> be the values that minimize this sum of squares.</p>
</section><section id="least-squares-solution" class="slide level2">
<h2>Least Squares Solution</h2>
<p>These values are:</p>
<p><span class="math display">\[\hat{\beta}_2 = r_{xy} \frac{s_y}{s_x}\]</span></p>
<p><span class="math display">\[\hat{\beta}_1 = \overline{y} - \hat{\beta}_2 \overline{x}\]</span></p>
<p>These values have a useful interpretation.</p>
</section><section id="visualizing-least-squares-line" class="slide level2">
<h2>Visualizing Least Squares Line</h2>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-13-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="example-height-and-weight" class="slide level2">
<h2>Example: Height and Weight</h2>
<pre class="r"><code>&gt; ggplot(data=htwt, mapping=aes(x=height, y=weight)) + 
+   geom_point(size=2, alpha=0.5) +
+   geom_smooth(method=&quot;lm&quot;, se=FALSE, formula=y~x)</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-14-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="calculate-the-line-directly" class="slide level2">
<h2>Calculate the Line Directly</h2>
<pre class="r"><code>&gt; beta2 &lt;- cor(htwt$height, htwt$weight) * 
+                sd(htwt$weight) / sd(htwt$height)
&gt; beta2
[1] 1.150092
&gt; 
&gt; beta1 &lt;- mean(htwt$weight) - beta2 * mean(htwt$height)
&gt; beta1
[1] -130.9104
&gt; 
&gt; yhat &lt;- beta1 + beta2 * htwt$height</code></pre>
</section><section id="plot-the-line" class="slide level2">
<h2>Plot the Line</h2>
<pre class="r"><code>&gt; df &lt;- data.frame(htwt, yhat=yhat)
&gt; ggplot(data=df) + geom_point(aes(x=height, y=weight), size=2, alpha=0.5) +
+   geom_line(aes(x=height, y=yhat), color=&quot;blue&quot;, size=1.2)</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-16-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="observed-data-fits-and-residuals" class="slide level2">
<h2>Observed Data, Fits, and Residuals</h2>
<p>We observe data <span class="math inline">\((x_1, y_1), \ldots, (x_n, y_n)\)</span>. Note that we only observe <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> from the generative model <span class="math inline">\(Y_i = \beta_1 + \beta_2 X_i + E_i\)</span>.</p>
<p>We calculate fitted values and observed residuals:</p>
<p><span class="math display">\[\hat{y}_i = \hat{\beta}_1 + \hat{\beta}_2 x_i\]</span></p>
<p><span class="math display">\[\hat{e}_i = y_i - \hat{y}_i\]</span></p>
<p>By construction, it is the case that <span class="math inline">\(\sum_{i=1}^n \hat{e}_i = 0\)</span>.</p>
</section><section id="proportion-of-variation-explained" class="slide level2">
<h2>Proportion of Variation Explained</h2>
<p>The proportion of variance explained by the fitted model is called <span class="math inline">\(R^2\)</span> or <span class="math inline">\(r^2\)</span>. It is calculated by:</p>
<p><span class="math display">\[r^2 = \frac{s^2_{\hat{y}}}{s^2_{y}}\]</span></p>
</section></section>
<section><section id="lm-function-in-r" class="titleslide slide level1"><h1><code>lm()</code> Function in R</h1></section><section id="calculate-the-line-in-r" class="slide level2">
<h2>Calculate the Line in R</h2>
<p>The syntax for a model in R is</p>
<p><code>response variable ~ explanatory variables</code></p>
<p>where the <code>explanatory variables</code> component can involve several types of terms.</p>
<pre class="r"><code>&gt; myfit &lt;- lm(weight ~ height, data=htwt)
&gt; myfit

Call:
lm(formula = weight ~ height, data = htwt)

Coefficients:
(Intercept)       height  
    -130.91         1.15  </code></pre>
</section><section id="an-lm-object-is-a-list" class="slide level2">
<h2>An <code>lm</code> Object is a List</h2>
<pre class="r"><code>&gt; class(myfit)
[1] &quot;lm&quot;
&gt; is.list(myfit)
[1] TRUE
&gt; names(myfit)
 [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;      
 [4] &quot;rank&quot;          &quot;fitted.values&quot; &quot;assign&quot;       
 [7] &quot;qr&quot;            &quot;df.residual&quot;   &quot;xlevels&quot;      
[10] &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;        </code></pre>
</section><section id="from-the-r-help" class="slide level2">
<h2>From the R Help</h2>
<blockquote>
<p><code>lm</code> returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).</p>
</blockquote>
<blockquote>
<p>The functions <code>summary</code> and <code>anova</code> are used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by <code>lm</code>.</p>
</blockquote>
</section><section id="some-of-the-list-items" class="slide level2">
<h2>Some of the List Items</h2>
<p>These are some useful items to access from the <code>lm</code> object:</p>
<ul>
<li><code>coefficients</code>: a named vector of coefficients</li>
<li><code>residuals</code>: the residuals, that is response minus fitted values.</li>
<li><code>fitted.values</code>: the fitted mean values.</li>
<li><code>df.residual</code>: the residual degrees of freedom.</li>
<li><code>call</code>: the matched call.</li>
<li><code>model</code>: if requested (the default), the model frame used.</li>
</ul>
</section><section id="summary" class="slide level2">
<h2><code>summary()</code></h2>
<pre class="r"><code>&gt; summary(myfit)

Call:
lm(formula = weight ~ height, data = htwt)

Residuals:
    Min      1Q  Median      3Q     Max 
-19.658  -5.381  -0.555   4.807  42.894 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -130.91040   11.52792  -11.36   &lt;2e-16 ***
height         1.15009    0.06749   17.04   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 8.505 on 198 degrees of freedom
Multiple R-squared:  0.5946,    Adjusted R-squared:  0.5925 
F-statistic: 290.4 on 1 and 198 DF,  p-value: &lt; 2.2e-16</code></pre>
</section><section id="summary-list-elements" class="slide level2">
<h2><code>summary()</code> List Elements</h2>
<pre class="r"><code>&gt; mysummary &lt;- summary(myfit)
&gt; names(mysummary)
 [1] &quot;call&quot;          &quot;terms&quot;         &quot;residuals&quot;    
 [4] &quot;coefficients&quot;  &quot;aliased&quot;       &quot;sigma&quot;        
 [7] &quot;df&quot;            &quot;r.squared&quot;     &quot;adj.r.squared&quot;
[10] &quot;fstatistic&quot;    &quot;cov.unscaled&quot; </code></pre>
</section><section id="using-tidy" class="slide level2">
<h2>Using <code>tidy()</code></h2>
<pre class="r"><code>&gt; library(broom)
&gt; tidy(myfit)
         term    estimate   std.error statistic      p.value
1 (Intercept) -130.910400 11.52792138 -11.35594 2.438012e-23
2      height    1.150092  0.06749465  17.03975 1.121241e-40</code></pre>
</section><section id="proportion-of-variation-explained-1" class="slide level2">
<h2>Proportion of Variation Explained</h2>
<p>The proportion of variance explained by the fitted model is called <span class="math inline">\(R^2\)</span> or <span class="math inline">\(r^2\)</span>. It is calculated by:</p>
<p><span class="math display">\[r^2 = \frac{s^2_{\hat{y}}}{s^2_{y}}\]</span></p>
<pre class="r"><code>&gt; summary(myfit)$r.squared
[1] 0.5945555
&gt; 
&gt; var(myfit$fitted.values)/var(htwt$weight)
[1] 0.5945555</code></pre>
</section><section id="assumptions-to-verify" class="slide level2">
<h2>Assumptions to Verify</h2>
<p>The assumptions on the above linear model are really about the joint distribution of the residuals, which are not directly observed. On data, we try to verify:</p>
<ol type="1">
<li>The fitted values and the residuals show no trends with respect to each other</li>
<li>The residuals are distributed approximately Normal<span class="math inline">\((0, \sigma^2)\)</span>
<ul>
<li>A constant variance is called <a href="https://en.wikipedia.org/wiki/Homoscedasticity"><strong>homoscedasticity</strong></a></li>
<li>A non-constant variance is called <a href="https://en.wikipedia.org/wiki/Heteroscedasticity"><strong>heteroscedascity</strong></a></li>
</ul></li>
<li>There are no lurking variables</li>
</ol>
<p>There are two plots we will use in this course to investigate the first two.</p>
</section><section id="residual-distribution" class="slide level2">
<h2>Residual Distribution</h2>
<pre class="r"><code>&gt; plot(myfit, which=1)</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-23-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="normal-residuals-check" class="slide level2">
<h2>Normal Residuals Check</h2>
<pre class="r"><code>&gt; plot(myfit, which=2)</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-24-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="fitted-values-vs.-obs.-residuals" class="slide level2">
<h2>Fitted Values Vs. Obs. Residuals</h2>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-25-1.png" width="960" style="display: block; margin: auto;" /></p>
</section></section>
<section><section id="ordinary-least-squares-1" class="titleslide slide level1"><h1>Ordinary Least Squares</h1></section><section class="slide level2">

<p><strong>Ordinary least squares</strong> (OLS) estimates the model</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp; = \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_p X_{ip} + E_i \\
 &amp; = {\boldsymbol{X}}_i {\boldsymbol{\beta}}+ E_i
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\({\rm E}[E_i] = 0\)</span>, <span class="math inline">\({\rm Var}(E_i) = \sigma^2\)</span>, and <span class="math inline">\({\operatorname{Cov}}(E_i, E_j) = 0\)</span> for all <span class="math inline">\(1 \leq i, j \leq n\)</span> and <span class="math inline">\(i \not= j\)</span>.</p>
<p>Note that typically <span class="math inline">\(X_{i1} = 1\)</span> for all <span class="math inline">\(i\)</span> so that <span class="math inline">\(\beta_1 X_{i1} = \beta_1\)</span> serves as the intercept.</p>
</section><section id="ols-solution" class="slide level2">
<h2>OLS Solution</h2>
<p>The estimates of <span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_p\)</span> are found by identifying the values that minimize:</p>
<p><span class="math display">\[ 
\begin{aligned}
\sum_{i=1}^n \left[ Y_i - (\beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_p X_{ip}) \right]^2 \\
= ({\boldsymbol{Y}}- {\boldsymbol{X}}{\boldsymbol{\beta}})^T ({\boldsymbol{Y}}- {\boldsymbol{X}}{\boldsymbol{\beta}})
\end{aligned}
\]</span></p>
<p>The solution is expressed in terms of matrix algebra computations:</p>
<p><span class="math display">\[
\hat{{\boldsymbol{\beta}}} = ({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} {\boldsymbol{X}}^T {\boldsymbol{Y}}.
\]</span></p>
</section><section id="sample-variance" class="slide level2">
<h2>Sample Variance</h2>
<p>Let the predicted values of the model be</p>
<p><span class="math display">\[
\hat{{\boldsymbol{Y}}} = {\boldsymbol{X}}\hat{{\boldsymbol{\beta}}} = {\boldsymbol{X}}({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} {\boldsymbol{X}}^T {\boldsymbol{Y}}.
\]</span></p>
<p>We estimate <span class="math inline">\(\sigma^2\)</span> by the OLS sample variance</p>
<p><span class="math display">\[
S^2 = \frac{\sum_{i=1}^n (Y_i - \hat{Y}_i)^2}{n-p}.
\]</span></p>
</section><section id="sample-covariance" class="slide level2">
<h2>Sample Covariance</h2>
<p>The <span class="math inline">\(p\)</span>-vector <span class="math inline">\(\hat{{\boldsymbol{\beta}}}\)</span> has covariance matrix</p>
<p><span class="math display">\[
{\operatorname{Cov}}(\hat{{\boldsymbol{\beta}}} | {\boldsymbol{X}}) = ({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} \sigma^2.
\]</span></p>
<p>Its estimated covariance matrix is</p>
<p><span class="math display">\[
\widehat{{\operatorname{Cov}}}(\hat{{\boldsymbol{\beta}}}) = ({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} S^2.
\]</span></p>
</section><section id="expected-values" class="slide level2">
<h2>Expected Values</h2>
<p>Under the assumption that <span class="math inline">\({\rm E}[E_i] = 0\)</span>, <span class="math inline">\({\rm Var}(E_i) = \sigma^2\)</span>, and <span class="math inline">\({\operatorname{Cov}}(E_i, E_j) = 0\)</span> for all <span class="math inline">\(1 \leq i, j \leq n\)</span> and <span class="math inline">\(i \not= j\)</span>, we have the following:</p>
<p><span class="math display">\[
{\operatorname{E}}\left[ \left. \hat{{\boldsymbol{\beta}}} \right| {\boldsymbol{X}}\right] = {\boldsymbol{\beta}}\]</span></p>
<p><span class="math display">\[
{\operatorname{E}}\left[ \left. S^2 \right| {\boldsymbol{X}}\right] = \sigma^2
\]</span></p>
<p><span class="math display">\[
{\operatorname{E}}\left[\left. ({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} S^2 \right| {\boldsymbol{X}}\right] = {\operatorname{Cov}}\left(\hat{{\boldsymbol{\beta}}}\right)
\]</span></p>
<p><span class="math display">\[
{\operatorname{Cov}}\left(\hat{\beta}_j, Y_i - \hat{Y}_i\right) = \boldsymbol{0}.
\]</span></p>
</section><section id="standard-error" class="slide level2">
<h2>Standard Error</h2>
<p>The standard error of <span class="math inline">\(\hat{\beta}_j\)</span> is the square root of the <span class="math inline">\((j, j)\)</span> diagonal entry of <span class="math inline">\(({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} \sigma^2\)</span></p>
<p><span class="math display">\[
{\operatorname{se}}(\hat{\beta}_j) = \sqrt{\left[({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} \sigma^2\right]_{jj}}
\]</span></p>
<p>and estimated standard error is</p>
<p><span class="math display">\[
\hat{{\operatorname{se}}}(\hat{\beta}_j) = \sqrt{\left[({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} S^2\right]_{jj}}
\]</span></p>
</section><section id="proportion-of-variance-explained" class="slide level2">
<h2>Proportion of Variance Explained</h2>
<p>The proportion of variance explained is defined equivalently to the simple linear regression scneario:</p>
<p><span class="math display">\[
R^2 = \frac{\sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2}{\sum_{i=1}^n (Y_i - \bar{Y})^2}.
\]</span></p>
</section><section id="normal-errors" class="slide level2">
<h2>Normal Errors</h2>
<p>Suppose we assume <span class="math inline">\(E_1, E_2, \ldots, E_n {\; \stackrel{\text{iid}}{\sim}\;}\mbox{Normal}(0, \sigma^2)\)</span>. Then</p>
<p><span class="math display">\[
\ell\left({\boldsymbol{\beta}}, \sigma^2 ; {\boldsymbol{Y}}, {\boldsymbol{X}}\right) \propto -n\log(\sigma^2) -\frac{1}{\sigma^2} ({\boldsymbol{Y}}- {\boldsymbol{X}}{\boldsymbol{\beta}})^T ({\boldsymbol{Y}}- {\boldsymbol{X}}{\boldsymbol{\beta}}).
\]</span></p>
<p>Since minimizing <span class="math inline">\(({\boldsymbol{Y}}- {\boldsymbol{X}}{\boldsymbol{\beta}})^T ({\boldsymbol{Y}}- {\boldsymbol{X}}{\boldsymbol{\beta}})\)</span> maximizes the likelihood with respect to <span class="math inline">\({\boldsymbol{\beta}}\)</span>, this implies <span class="math inline">\(\hat{{\boldsymbol{\beta}}}\)</span> is the MLE for <span class="math inline">\({\boldsymbol{\beta}}\)</span>.</p>
<p>It can also be calculated that <span class="math inline">\(\frac{n-p}{n} S^2\)</span> is the MLE for <span class="math inline">\(\sigma^2\)</span>.</p>
</section><section id="sampling-distribution" class="slide level2">
<h2>Sampling Distribution</h2>
<p>When <span class="math inline">\(E_1, E_2, \ldots, E_n {\; \stackrel{\text{iid}}{\sim}\;}\mbox{Normal}(0, \sigma^2)\)</span>, it follows that, conditional on <span class="math inline">\({\boldsymbol{X}}\)</span>:</p>
<p><span class="math display">\[
\hat{{\boldsymbol{\beta}}} \sim \mbox{MVN}_p\left({\boldsymbol{\beta}}, ({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} \sigma^2 \right)
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
S^2 \frac{n-p}{\sigma^2} &amp; \sim \chi^2_{n-p} \\
\frac{\hat{\beta}_j - \beta_j}{\hat{{\operatorname{se}}}(\hat{\beta}_j)} &amp; \sim t_{n-p}
\end{aligned}
\]</span></p>
</section><section id="clt" class="slide level2">
<h2>CLT</h2>
<p>Under the assumption that <span class="math inline">\({\rm E}[E_i] = 0\)</span>, <span class="math inline">\({\rm Var}(E_i) = \sigma^2\)</span>, and <span class="math inline">\({\operatorname{Cov}}(E_i, E_j) = 0\)</span> for <span class="math inline">\(i \not= j\)</span>, it follows that as <span class="math inline">\(n \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[
\sqrt{n} \left(\hat{{\boldsymbol{\beta}}} - {\boldsymbol{\beta}}\right) \stackrel{D}{\longrightarrow} \mbox{MVN}_p\left( \boldsymbol{0}, ({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} \sigma^2 \right).
\]</span></p>
</section><section id="gauss-markov-theorem" class="slide level2">
<h2>Gauss-Markov Theorem</h2>
<p>Under the assumption that <span class="math inline">\({\rm E}[E_i] = 0\)</span>, <span class="math inline">\({\rm Var}(E_i) = \sigma^2\)</span>, and <span class="math inline">\({\operatorname{Cov}}(E_i, E_j) = 0\)</span> for <span class="math inline">\(i \not= j\)</span>, the Gauss-Markov theorem shows that among all BLUEs, <strong>best linear unbiased estimators</strong>, the least squares estimate has the smallest mean-squared error.</p>
<p>Specifically, suppose that <span class="math inline">\(\tilde{{\boldsymbol{\beta}}}\)</span> is a linear estimator (calculated from a linear operator on <span class="math inline">\({\boldsymbol{Y}}\)</span>) where <span class="math inline">\({\operatorname{E}}[\tilde{{\boldsymbol{\beta}}} | {\boldsymbol{X}}] = {\boldsymbol{\beta}}\)</span>. Then</p>
<p><span class="math display">\[
{\operatorname{E}}\left[ \left. ({\boldsymbol{Y}}- {\boldsymbol{X}}\hat{{\boldsymbol{\beta}}})^T ({\boldsymbol{Y}}- {\boldsymbol{X}}\hat{{\boldsymbol{\beta}}}) \right| {\boldsymbol{X}}\right] \leq 
{\operatorname{E}}\left[ \left. ({\boldsymbol{Y}}- {\boldsymbol{X}}\tilde{{\boldsymbol{\beta}}})^T ({\boldsymbol{Y}}- {\boldsymbol{X}}\tilde{{\boldsymbol{\beta}}}) \right| {\boldsymbol{X}}\right].
\]</span></p>
</section></section>
<section><section id="generalized-least-squares-1" class="titleslide slide level1"><h1>Generalized Least Squares</h1></section><section class="slide level2">

<p><strong>Generalized least squares</strong> (GLS) assumes the same model as OLS, except it allows for <strong>heteroskedasticity</strong> and <strong>covariance</strong> among the <span class="math inline">\(E_i\)</span>. Specifically, it is assumed that <span class="math inline">\({\boldsymbol{E}}= (E_1, \ldots, E_n)^T\)</span> is distributed as</p>
<p><span class="math display">\[
{\boldsymbol{E}}_{n \times 1} \sim (\boldsymbol{0}, {\boldsymbol{\Sigma}})
\]</span> where <span class="math inline">\(\boldsymbol{0}\)</span> is the expected value <span class="math inline">\({\boldsymbol{\Sigma}}= (\sigma_{ij})\)</span> is the <span class="math inline">\(n \times n\)</span> covariance matrix.</p>
</section><section class="slide level2">

<p>The most straightforward way to navigate GLS results is to recognize that</p>
<p><span class="math display">\[
{\boldsymbol{\Sigma}}^{-1/2} {\boldsymbol{Y}}= {\boldsymbol{\Sigma}}^{-1/2}{\boldsymbol{X}}{\boldsymbol{\beta}}+ {\boldsymbol{\Sigma}}^{-1/2}{\boldsymbol{E}}\]</span></p>
<p>satisfies the assumptions of the OLS model.</p>
</section><section id="gls-solution" class="slide level2">
<h2>GLS Solution</h2>
<p>The solution to minimizing</p>
<p><span class="math display">\[
({\boldsymbol{Y}}- {\boldsymbol{X}}{\boldsymbol{\beta}})^T {\boldsymbol{\Sigma}}^{-1} ({\boldsymbol{Y}}- {\boldsymbol{X}}{\boldsymbol{\beta}})
\]</span></p>
<p>is</p>
<p><span class="math display">\[
\hat{{\boldsymbol{\beta}}} = \left( {\boldsymbol{X}}^T {\boldsymbol{\Sigma}}^{-1} {\boldsymbol{X}}\right)^{-1} {\boldsymbol{X}}^T {\boldsymbol{\Sigma}}^{-1} {\boldsymbol{Y}}.
\]</span></p>
</section><section id="other-results" class="slide level2">
<h2>Other Results</h2>
<p>The issue of estimating <span class="math inline">\({\boldsymbol{\Sigma}}\)</span> if it is unknown is complicated. Other than estimates of <span class="math inline">\(\sigma^2\)</span>, the results from the OLS section recapitulate by replacing <span class="math inline">\({\boldsymbol{Y}}= {\boldsymbol{X}}{\boldsymbol{\beta}}+ {\boldsymbol{E}}\)</span> with</p>
<p><span class="math display">\[
{\boldsymbol{\Sigma}}^{-1/2} {\boldsymbol{Y}}= {\boldsymbol{\Sigma}}^{-1/2}{\boldsymbol{X}}{\boldsymbol{\beta}}+ {\boldsymbol{\Sigma}}^{-1/2}{\boldsymbol{E}}.
\]</span></p>
</section><section class="slide level2">

<p>For example, as <span class="math inline">\(n \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[
\sqrt{n} \left(\hat{{\boldsymbol{\beta}}} - {\boldsymbol{\beta}}\right) \stackrel{D}{\longrightarrow} \mbox{MNV}_p\left( \boldsymbol{0}, ({\boldsymbol{X}}^T {\boldsymbol{\Sigma}}^{-1} {\boldsymbol{X}})^{-1} \right).
\]</span></p>
<p> </p>
<p>We also still have that</p>
<p><span class="math display">\[
{\operatorname{E}}\left[ \left. \hat{{\boldsymbol{\beta}}} \right| {\boldsymbol{X}}\right] = {\boldsymbol{\beta}}.
\]</span></p>
<p> </p>
<p>And when <span class="math inline">\({\boldsymbol{E}}\sim \mbox{MVN}_n(\boldsymbol{0}, {\boldsymbol{\Sigma}})\)</span>, <span class="math inline">\(\hat{{\boldsymbol{\beta}}}\)</span> is the MLE.</p>
</section></section>
<section><section id="ols-in-r" class="titleslide slide level1"><h1>OLS in R</h1></section><section class="slide level2">

<p>R implements OLS of multiple explanatory variables exactly the same as with a single explanatory variable, except we need to show the sum of all explanatory variables that we want to use.</p>
<pre class="r"><code>&gt; lm(weight ~ height + sex, data=htwt)

Call:
lm(formula = weight ~ height + sex, data = htwt)

Coefficients:
(Intercept)       height         sexM  
   -76.6167       0.8106       8.2269  </code></pre>
</section><section id="weight-regressed-on-height-sex" class="slide level2">
<h2>Weight Regressed on Height + Sex</h2>
<pre class="r"><code>&gt; summary(lm(weight ~ height + sex, data=htwt))

Call:
lm(formula = weight ~ height + sex, data = htwt)

Residuals:
    Min      1Q  Median      3Q     Max 
-20.131  -4.884  -0.640   5.160  41.490 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -76.6167    15.7150  -4.875 2.23e-06 ***
height        0.8105     0.0953   8.506 4.50e-15 ***
sexM          8.2269     1.7105   4.810 3.00e-06 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 8.066 on 197 degrees of freedom
Multiple R-squared:  0.6372,    Adjusted R-squared:  0.6335 
F-statistic:   173 on 2 and 197 DF,  p-value: &lt; 2.2e-16</code></pre>
</section><section id="one-variable-two-scales" class="slide level2">
<h2>One Variable, Two Scales</h2>
<p>We can include a single variable but on two different scales:</p>
<pre class="r"><code>&gt; htwt &lt;- htwt %&gt;% mutate(height2 = height^2)
&gt; summary(lm(weight ~ height + height2, data=htwt))

Call:
lm(formula = weight ~ height + height2, data = htwt)

Residuals:
    Min      1Q  Median      3Q     Max 
-24.265  -5.159  -0.499   4.549  42.965 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 107.117140 175.246872   0.611    0.542
height       -1.632719   2.045524  -0.798    0.426
height2       0.008111   0.005959   1.361    0.175

Residual standard error: 8.486 on 197 degrees of freedom
Multiple R-squared:  0.5983,    Adjusted R-squared:  0.5943 
F-statistic: 146.7 on 2 and 197 DF,  p-value: &lt; 2.2e-16</code></pre>
</section><section id="interactions" class="slide level2">
<h2>Interactions</h2>
<p>It is possible to include products of explanatory variables, which is called an <em>interaction</em>.</p>
<pre class="r"><code>&gt; summary(lm(weight ~ height + sex + height:sex, data=htwt))

Call:
lm(formula = weight ~ height + sex + height:sex, data = htwt)

Residuals:
    Min      1Q  Median      3Q     Max 
-20.869  -4.835  -0.897   4.429  41.122 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -45.6730    22.1342  -2.063   0.0404 *  
height        0.6227     0.1343   4.637 6.46e-06 ***
sexM        -55.6571    32.4597  -1.715   0.0880 .  
height:sexM   0.3729     0.1892   1.971   0.0502 .  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 8.007 on 196 degrees of freedom
Multiple R-squared:  0.6442,    Adjusted R-squared:  0.6388 
F-statistic: 118.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16</code></pre>
</section><section id="more-on-interactions" class="slide level2">
<h2>More on Interactions</h2>
<p>What happens when there is an interaction between a quantitative explanatory variable and a factor explanatory variable? In the next plot, we show three models:</p>
<ul>
<li>Grey solid: <code>lm(weight ~ height, data=htwt)</code></li>
<li>Color dashed: <code>lm(weight ~ height + sex, data=htwt)</code></li>
<li>Color solid: <code>lm(weight ~ height + sex + height:sex, data=htwt)</code></li>
</ul>
</section><section id="visualizing-three-different-models" class="slide level2">
<h2>Visualizing Three Different Models</h2>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-30-1.png" width="576" style="display: block; margin: auto;" /></p>
</section></section>
<section><section id="categorical-explanatory-variables" class="titleslide slide level1"><h1>Categorical Explanatory Variables</h1></section><section id="example-chicken-weights" class="slide level2">
<h2>Example: Chicken Weights</h2>
<pre class="r"><code>&gt; data(&quot;chickwts&quot;, package=&quot;datasets&quot;)
&gt; head(chickwts)
  weight      feed
1    179 horsebean
2    160 horsebean
3    136 horsebean
4    227 horsebean
5    217 horsebean
6    168 horsebean
&gt; summary(chickwts$feed)
   casein horsebean   linseed  meatmeal   soybean sunflower 
       12        10        12        11        14        12 </code></pre>
</section><section id="factor-variables-in-lm" class="slide level2">
<h2>Factor Variables in <code>lm()</code></h2>
<pre class="r"><code>&gt; chick_fit &lt;- lm(weight ~ feed, data=chickwts)
&gt; summary(chick_fit)

Call:
lm(formula = weight ~ feed, data = chickwts)

Residuals:
     Min       1Q   Median       3Q      Max 
-123.909  -34.413    1.571   38.170  103.091 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    323.583     15.834  20.436  &lt; 2e-16 ***
feedhorsebean -163.383     23.485  -6.957 2.07e-09 ***
feedlinseed   -104.833     22.393  -4.682 1.49e-05 ***
feedmeatmeal   -46.674     22.896  -2.039 0.045567 *  
feedsoybean    -77.155     21.578  -3.576 0.000665 ***
feedsunflower    5.333     22.393   0.238 0.812495    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 54.85 on 65 degrees of freedom
Multiple R-squared:  0.5417,    Adjusted R-squared:  0.5064 
F-statistic: 15.36 on 5 and 65 DF,  p-value: 5.936e-10</code></pre>
</section><section id="plot-the-fit" class="slide level2">
<h2>Plot the Fit</h2>
<pre class="r"><code>&gt; plot(chickwts$feed, chickwts$weight, xlab=&quot;Feed&quot;, ylab=&quot;Weight&quot;, las=2)
&gt; points(chickwts$feed, chick_fit$fitted.values, col=&quot;blue&quot;, pch=20, cex=2)</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-33-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="anova-version-1" class="slide level2">
<h2>ANOVA (Version 1)</h2>
<p>ANOVA (<em>analysis of variance</em>) was originally developed as a statistical model and method for comparing differences in mean values between various groups.</p>
<p>ANOVA quantifies and tests for differences in response variables with respect to factor variables.</p>
<p>In doing so, it also partitions the total variance to that due to within and between groups, where groups are defined by the factor variables.</p>
</section><section id="anova" class="slide level2">
<h2><code>anova()</code></h2>
<p>The classic ANOVA table:</p>
<pre class="r"><code>&gt; anova(chick_fit)
Analysis of Variance Table

Response: weight
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
feed       5 231129   46226  15.365 5.936e-10 ***
Residuals 65 195556    3009                      
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>&gt; n &lt;- length(chick_fit$residuals) # n &lt;- 71
&gt; (n-1)*var(chick_fit$fitted.values)
[1] 231129.2
&gt; (n-1)*var(chick_fit$residuals)
[1] 195556
&gt; (n-1)*var(chickwts$weight) # sum of above two quantities
[1] 426685.2
&gt; (231129/5)/(195556/65) # F-statistic
[1] 15.36479</code></pre>
</section><section id="how-it-works" class="slide level2">
<h2>How It Works</h2>
<pre class="r"><code>&gt; levels(chickwts$feed)
[1] &quot;casein&quot;    &quot;horsebean&quot; &quot;linseed&quot;   &quot;meatmeal&quot;  &quot;soybean&quot;  
[6] &quot;sunflower&quot;
&gt; head(chickwts, n=3)
  weight      feed
1    179 horsebean
2    160 horsebean
3    136 horsebean
&gt; tail(chickwts, n=3)
   weight   feed
69    222 casein
70    283 casein
71    332 casein
&gt; x &lt;- model.matrix(weight ~ feed, data=chickwts)
&gt; dim(x)
[1] 71  6</code></pre>
</section><section id="top-of-design-matrix" class="slide level2">
<h2>Top of Design Matrix</h2>
<pre class="r"><code>&gt; head(x)
  (Intercept) feedhorsebean feedlinseed feedmeatmeal
1           1             1           0            0
2           1             1           0            0
3           1             1           0            0
4           1             1           0            0
5           1             1           0            0
6           1             1           0            0
  feedsoybean feedsunflower
1           0             0
2           0             0
3           0             0
4           0             0
5           0             0
6           0             0</code></pre>
</section><section id="bottom-of-design-matrix" class="slide level2">
<h2>Bottom of Design Matrix</h2>
<pre class="r"><code>&gt; tail(x)
   (Intercept) feedhorsebean feedlinseed feedmeatmeal
66           1             0           0            0
67           1             0           0            0
68           1             0           0            0
69           1             0           0            0
70           1             0           0            0
71           1             0           0            0
   feedsoybean feedsunflower
66           0             0
67           0             0
68           0             0
69           0             0
70           0             0
71           0             0</code></pre>
</section><section id="model-fits" class="slide level2">
<h2>Model Fits</h2>
<pre class="r"><code>&gt; chick_fit$fitted.values %&gt;% round(digits=4) %&gt;% unique()
[1] 160.2000 218.7500 246.4286 328.9167 276.9091 323.5833</code></pre>
<pre class="r"><code>&gt; chickwts %&gt;% group_by(feed) %&gt;% summarize(mean(weight))
# A tibble: 6 × 2
       feed `mean(weight)`
     &lt;fctr&gt;          &lt;dbl&gt;
1    casein       323.5833
2 horsebean       160.2000
3   linseed       218.7500
4  meatmeal       276.9091
5   soybean       246.4286
6 sunflower       328.9167</code></pre>
</section></section>
<section><section id="variable-transformations" class="titleslide slide level1"><h1>Variable Transformations</h1></section><section id="rationale-1" class="slide level2">
<h2>Rationale</h2>
<p>In order to obtain reliable model fits and inference on linear models, the model assumptions described earlier must be satisfied.</p>
<p>Sometimes it is necessary to <em>transform</em> the response variable and/or some of the explanatory variables.</p>
<p>This process should involve data visualization and exploration.</p>
</section><section id="power-and-log-transformations" class="slide level2">
<h2>Power and Log Transformations</h2>
<p>It is often useful to explore power and log transforms of the variables, e.g., <span class="math inline">\(\log(y)\)</span> or <span class="math inline">\(y^\lambda\)</span> for some <span class="math inline">\(\lambda\)</span> (and likewise <span class="math inline">\(\log(x)\)</span> or <span class="math inline">\(x^\lambda\)</span>).</p>
<p>You can read more about the <a href="https://en.wikipedia.org/wiki/Power_transform">Box-Cox family of power transformations</a>.</p>
</section><section id="diamonds-data" class="slide level2">
<h2><code>Diamonds</code> Data</h2>
<pre class="r"><code>&gt; data(&quot;diamonds&quot;, package=&quot;ggplot2&quot;)
&gt; head(diamonds)
# A tibble: 6 × 10
  carat       cut color clarity depth table price     x     y
  &lt;dbl&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;
1  0.23     Ideal     E     SI2  61.5    55   326  3.95  3.98
2  0.21   Premium     E     SI1  59.8    61   326  3.89  3.84
3  0.23      Good     E     VS1  56.9    65   327  4.05  4.07
4  0.29   Premium     I     VS2  62.4    58   334  4.20  4.23
5  0.31      Good     J     SI2  63.3    58   335  4.34  4.35
6  0.24 Very Good     J    VVS2  62.8    57   336  3.94  3.96
# ... with 1 more variables: z &lt;dbl&gt;</code></pre>
</section><section id="nonlinear-relationship" class="slide level2">
<h2>Nonlinear Relationship</h2>
<pre class="r"><code>&gt; ggplot(data = diamonds) +
+   geom_point(mapping=aes(x=carat, y=price, color=clarity), alpha=0.3)</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-42-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="regression-with-nonlinear-relationship" class="slide level2">
<h2>Regression with Nonlinear Relationship</h2>
<pre class="r"><code>&gt; diam_fit &lt;- lm(price ~ carat + clarity, data=diamonds)
&gt; anova(diam_fit)
Analysis of Variance Table

Response: price
             Df     Sum Sq    Mean Sq  F value    Pr(&gt;F)    
carat         1 7.2913e+11 7.2913e+11 435639.9 &lt; 2.2e-16 ***
clarity       7 3.9082e+10 5.5831e+09   3335.8 &lt; 2.2e-16 ***
Residuals 53931 9.0264e+10 1.6737e+06                       
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</section><section id="residual-distribution-1" class="slide level2">
<h2>Residual Distribution</h2>
<pre class="r"><code>&gt; plot(diam_fit, which=1)</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-44-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="normal-residuals-check-1" class="slide level2">
<h2>Normal Residuals Check</h2>
<pre class="r"><code>&gt; plot(diam_fit, which=2)</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-45-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="log-transformation" class="slide level2">
<h2>Log-Transformation</h2>
<pre class="r"><code>&gt; ggplot(data = diamonds) +
+   geom_point(aes(x=carat, y=price, color=clarity), alpha=0.3) +
+   scale_y_log10(breaks=c(1000,5000,10000)) + 
+   scale_x_log10(breaks=1:5)</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-46-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="ols-on-log-transformed-data" class="slide level2">
<h2>OLS on Log-Transformed Data</h2>
<pre class="r"><code>&gt; diamonds &lt;- mutate(diamonds, log_price = log(price, base=10), 
+                    log_carat = log(carat, base=10))
&gt; ldiam_fit &lt;- lm(log_price ~ log_carat + clarity, data=diamonds)
&gt; anova(ldiam_fit)
Analysis of Variance Table

Response: log_price
             Df Sum Sq Mean Sq   F value    Pr(&gt;F)    
log_carat     1 9771.9  9771.9 1452922.6 &lt; 2.2e-16 ***
clarity       7  339.1    48.4    7203.3 &lt; 2.2e-16 ***
Residuals 53931  362.7     0.0                        
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</section><section id="residual-distribution-2" class="slide level2">
<h2>Residual Distribution</h2>
<pre class="r"><code>&gt; plot(ldiam_fit, which=1)</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-48-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="normal-residuals-check-2" class="slide level2">
<h2>Normal Residuals Check</h2>
<pre class="r"><code>&gt; plot(ldiam_fit, which=2)</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-49-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="tree-pollen-study" class="slide level2">
<h2>Tree Pollen Study</h2>
<p>Suppose that we have a study where tree pollen measurements are averaged every week, and these data are recorded for 10 years. These data are simulated:</p>
<pre class="r"><code>&gt; pollen_study
# A tibble: 520 × 3
    week  year   pollen
   &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;
1      1  2001 1841.751
2      2  2001 1965.503
3      3  2001 2380.972
4      4  2001 2141.025
5      5  2001 2210.473
6      6  2001 2585.321
7      7  2001 2392.183
8      8  2001 2104.680
9      9  2001 2278.014
10    10  2001 2383.945
# ... with 510 more rows</code></pre>
</section><section id="tree-pollen-count-by-week" class="slide level2">
<h2>Tree Pollen Count by Week</h2>
<pre class="r"><code>&gt; ggplot(pollen_study) + geom_point(aes(x=week, y=pollen))</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-52-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="a-clever-transformation" class="slide level2">
<h2>A Clever Transformation</h2>
<p>We can see there is a linear relationship between <code>pollen</code> and <code>week</code> if we transform <code>week</code> to be number of weeks from the peak week.</p>
<pre class="r"><code>&gt; pollen_study &lt;- pollen_study %&gt;%  
+                        mutate(week_new = abs(week-20))</code></pre>
<p>Note that this is a very different transformation from taking a log or power transformation.</p>
</section><section id="week-transformed" class="slide level2">
<h2><code>week</code> Transformed</h2>
<pre class="r"><code>&gt; ggplot(pollen_study) + geom_point(aes(x=week_new, y=pollen))</code></pre>
<p><img src="week09_files/figure-revealjs/unnamed-chunk-54-1.png" width="576" style="display: block; margin: auto;" /></p>
</section></section>
<section><section id="ols-goodness-of-fit" class="titleslide slide level1"><h1>OLS Goodness of Fit</h1></section><section id="pythagorean-theorem" class="slide level2">
<h2>Pythagorean Theorem</h2>
<div id="left">
<p><img src="images/right_triangle_model_fits.png" alt="PythMod" /></p>
</div>
<div id="right">
<p>Least squares model fitting can be understood through the Pythagorean theorem: <span class="math inline">\(a^2 + b^2 = c^2\)</span>. However, here we have:</p>
<p><span class="math display">\[
\sum_{i=1}^n Y_i^2 = \sum_{i=1}^n \hat{Y}_i^2 + \sum_{i=1}^n (Y_i - \hat{Y}_i)^2
\]</span></p>
<p>where the <span class="math inline">\(\hat{Y}_i\)</span> are the result of a <strong>linear projection</strong> of the <span class="math inline">\(Y_i\)</span>.</p>
</div>
</section><section id="ols-normal-model" class="slide level2">
<h2>OLS Normal Model</h2>
<p>In this section, let’s assume that <span class="math inline">\(({\boldsymbol{X}}_1, Y_1), \ldots, ({\boldsymbol{X}}_n, Y_n)\)</span> are distributed so that</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp; = \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_p X_{ip} + E_i \\
 &amp; = {\boldsymbol{X}}_i {\boldsymbol{\beta}}+ E_i
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\({\boldsymbol{E}}| {\boldsymbol{X}}\sim \mbox{MVN}_n({\boldsymbol{0}}, \sigma^2 {\boldsymbol{I}})\)</span>. Note that we haven’t specified the distribution of the <span class="math inline">\({\boldsymbol{X}}_i\)</span> rv’s.</p>
</section><section id="projection-matrices" class="slide level2">
<h2>Projection Matrices</h2>
<p>In the OLS framework we have:</p>
<p><span class="math display">\[
\hat{{\boldsymbol{Y}}} = {\boldsymbol{X}}({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} {\boldsymbol{X}}^T {\boldsymbol{Y}}.
\]</span></p>
<p>The matrix <span class="math inline">\({\boldsymbol{P}}_{n \times n} = {\boldsymbol{X}}({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} {\boldsymbol{X}}^T\)</span> is a projection matrix. The vector <span class="math inline">\({\boldsymbol{Y}}\)</span> is projected into the space spanned by the column space of <span class="math inline">\({\boldsymbol{X}}\)</span>.</p>
</section><section class="slide level2">

<p>Project matrices have the following properties:</p>
<ul>
<li><span class="math inline">\({\boldsymbol{P}}\)</span> is symmetric</li>
<li><span class="math inline">\({\boldsymbol{P}}\)</span> is idempotent so that <span class="math inline">\({\boldsymbol{P}}{\boldsymbol{P}}= {\boldsymbol{P}}\)</span></li>
<li>If <span class="math inline">\({\boldsymbol{X}}\)</span> has column rank <span class="math inline">\(p\)</span>, then <span class="math inline">\({\boldsymbol{P}}\)</span> has rank <span class="math inline">\(p\)</span></li>
<li>The eigenvalues of <span class="math inline">\({\boldsymbol{P}}\)</span> are <span class="math inline">\(p\)</span> 1’s and <span class="math inline">\(n-p\)</span> 0’s</li>
<li>The trace (sum of diagonal entries) is <span class="math inline">\(\operatorname{tr}({\boldsymbol{P}}) = p\)</span></li>
<li><span class="math inline">\({\boldsymbol{I}}- {\boldsymbol{P}}\)</span> is also a projection matrix with rank <span class="math inline">\(n-p\)</span></li>
</ul>
</section><section id="decomposition" class="slide level2">
<h2>Decomposition</h2>
<p>Note that <span class="math inline">\({\boldsymbol{P}}({\boldsymbol{I}}- {\boldsymbol{P}}) = {\boldsymbol{P}}- {\boldsymbol{P}}{\boldsymbol{P}}= {\boldsymbol{P}}- {\boldsymbol{P}}= {\boldsymbol{0}}\)</span>.</p>
<p>We have</p>
<p><span class="math display">\[
\begin{aligned}
\| {\boldsymbol{Y}}\|_{2}^{2} = {\boldsymbol{Y}}^T {\boldsymbol{Y}}&amp; = ({\boldsymbol{P}}{\boldsymbol{Y}}+ ({\boldsymbol{I}}- {\boldsymbol{P}}) {\boldsymbol{Y}})^T ({\boldsymbol{P}}{\boldsymbol{Y}}+ ({\boldsymbol{I}}- {\boldsymbol{P}}) {\boldsymbol{Y}}) \\
 &amp; = ({\boldsymbol{P}}{\boldsymbol{Y}})^T ({\boldsymbol{P}}{\boldsymbol{Y}}) + (({\boldsymbol{I}}- {\boldsymbol{P}}) {\boldsymbol{Y}})^T (({\boldsymbol{I}}- {\boldsymbol{P}}) {\boldsymbol{Y}}) \\
 &amp; = \| {\boldsymbol{P}}{\boldsymbol{Y}}\|_{2}^{2} + \| ({\boldsymbol{I}}- {\boldsymbol{P}}) {\boldsymbol{Y}}\|_{2}^{2}
\end{aligned}
\]</span></p>
<p>where the cross terms disappear because <span class="math inline">\({\boldsymbol{P}}({\boldsymbol{I}}- {\boldsymbol{P}}) = {\boldsymbol{0}}\)</span>.</p>
</section><section class="slide level2">

<p>Note: The <span class="math inline">\(\ell_p\)</span> norm of an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\boldsymbol{w}\)</span> is defined as</p>
<p><span class="math display">\[
\| \boldsymbol{w} \|_p = \left(\sum_{i=1}^n |w_i|^p\right)^{1/p}.
\]</span></p>
<p>Above we calculated</p>
<p><span class="math display">\[
\| \boldsymbol{w} \|_2^2 = \sum_{i=1}^n w_i^2.
\]</span></p>
</section><section id="distribution-of-projection" class="slide level2">
<h2>Distribution of Projection</h2>
<p>Suppose that <span class="math inline">\(Y_1, Y_2, \ldots, Y_n {\; \stackrel{\text{iid}}{\sim}\;}\mbox{Normal}(0,\sigma^2)\)</span>. This can also be written as <span class="math inline">\({\boldsymbol{Y}}\sim \mbox{MVN}_n({\boldsymbol{0}}, \sigma^2 {\boldsymbol{I}})\)</span>. It follows that</p>
<p><span class="math display">\[
{\boldsymbol{P}}{\boldsymbol{Y}}\sim \mbox{MVN}_{n}({\boldsymbol{0}}, \sigma^2 {\boldsymbol{P}}{\boldsymbol{I}}{\boldsymbol{P}}^T).
\]</span></p>
<p>where <span class="math inline">\({\boldsymbol{P}}{\boldsymbol{I}}{\boldsymbol{P}}^T = {\boldsymbol{P}}{\boldsymbol{P}}^T = {\boldsymbol{P}}{\boldsymbol{P}}= {\boldsymbol{P}}\)</span>.</p>
<p>Also, <span class="math inline">\(({\boldsymbol{P}}{\boldsymbol{Y}})^T ({\boldsymbol{P}}{\boldsymbol{Y}}) = {\boldsymbol{Y}}^T {\boldsymbol{P}}^T {\boldsymbol{P}}{\boldsymbol{Y}}= {\boldsymbol{Y}}^T {\boldsymbol{P}}{\boldsymbol{Y}}\)</span>, a <strong>quadratic form</strong>. Given the eigenvalues of <span class="math inline">\({\boldsymbol{P}}\)</span>, <span class="math inline">\({\boldsymbol{Y}}^T {\boldsymbol{P}}{\boldsymbol{Y}}\)</span> is equivalent in distribution to <span class="math inline">\(p\)</span> squared iid Normal(0,1) rv’s, so</p>
<p><span class="math display">\[
\frac{{\boldsymbol{Y}}^T {\boldsymbol{P}}{\boldsymbol{Y}}}{\sigma^2} \sim \chi^2_{p}.
\]</span></p>
</section><section id="distribution-of-residuals" class="slide level2">
<h2>Distribution of Residuals</h2>
<p>If <span class="math inline">\({\boldsymbol{P}}{\boldsymbol{Y}}= \hat{{\boldsymbol{Y}}}\)</span> are the fitted OLS values, then <span class="math inline">\(({\boldsymbol{I}}-{\boldsymbol{P}}) {\boldsymbol{Y}}= {\boldsymbol{Y}}- \hat{{\boldsymbol{Y}}}\)</span> are the residuals.</p>
<p>It follows by the same argument as above that</p>
<p><span class="math display">\[
\frac{{\boldsymbol{Y}}^T ({\boldsymbol{I}}-{\boldsymbol{P}}) {\boldsymbol{Y}}}{\sigma^2} \sim \chi^2_{n-p}.
\]</span></p>
<p>It’s also straightforward to show that <span class="math inline">\(({\boldsymbol{I}}-{\boldsymbol{P}}){\boldsymbol{Y}}\sim \mbox{MVN}_{n}({\boldsymbol{0}}, \sigma^2({\boldsymbol{I}}-{\boldsymbol{P}}))\)</span> and <span class="math inline">\({\operatorname{Cov}}({\boldsymbol{P}}{\boldsymbol{Y}}, ({\boldsymbol{I}}-{\boldsymbol{P}}){\boldsymbol{Y}}) = {\boldsymbol{0}}\)</span>.</p>
</section><section id="degrees-of-freedom" class="slide level2">
<h2>Degrees of Freedom</h2>
<p>The degrees of freedom, <span class="math inline">\(p\)</span>, of a linear projection model fit is equal to</p>
<ul>
<li>The number of linearly dependent columns of <span class="math inline">\({\boldsymbol{X}}\)</span></li>
<li>The number of nonzero eigenvalues of <span class="math inline">\({\boldsymbol{P}}\)</span> (where nonzero eigenvalues are equal to 1)</li>
<li>The trace of the projection matrix, <span class="math inline">\(\operatorname{tr}({\boldsymbol{P}})\)</span>.</li>
</ul>
<p>The reason why we divide estimates of variance by <span class="math inline">\(n-p\)</span> is because this is the number of effective independent sources of variation remaining after the model is fit by projecting the <span class="math inline">\(n\)</span> observations into a <span class="math inline">\(p\)</span> dimensional linear space.</p>
</section><section id="submodels" class="slide level2">
<h2>Submodels</h2>
<p>Consider the OLS model <span class="math inline">\({\boldsymbol{Y}}= {\boldsymbol{X}}{\boldsymbol{\beta}}+ {\boldsymbol{E}}\)</span> where there are <span class="math inline">\(p\)</span> columns of <span class="math inline">\({\boldsymbol{X}}\)</span> and <span class="math inline">\({\boldsymbol{\beta}}\)</span> is a <span class="math inline">\(p\)</span>-vector.</p>
<p>Let <span class="math inline">\({\boldsymbol{X}}_0\)</span> be a subset of <span class="math inline">\(p_0\)</span> columns of <span class="math inline">\({\boldsymbol{X}}\)</span> and let <span class="math inline">\({\boldsymbol{X}}_1\)</span> be a subset of <span class="math inline">\(p_1\)</span> columns, where <span class="math inline">\(1 \leq p_0 &lt; p_1 \leq p\)</span>. Also, assume that the columns of <span class="math inline">\({\boldsymbol{X}}_0\)</span> are a subset of <span class="math inline">\({\boldsymbol{X}}_1\)</span>.</p>
<p>We can form <span class="math inline">\(\hat{{\boldsymbol{Y}}}_0 = {\boldsymbol{P}}_0 {\boldsymbol{Y}}\)</span> where <span class="math inline">\({\boldsymbol{P}}_0\)</span> is the projection matrix built from <span class="math inline">\({\boldsymbol{X}}_0\)</span>. We can analogously form <span class="math inline">\(\hat{{\boldsymbol{Y}}}_1 = {\boldsymbol{P}}_1 {\boldsymbol{Y}}\)</span>.</p>
</section><section id="hypothesis-testing" class="slide level2">
<h2>Hypothesis Testing</h2>
<p>Without loss of generality, suppose that <span class="math inline">\({\boldsymbol{\beta}}_0 = (\beta_1, \beta_2, \ldots, \beta_{p_0})^T\)</span> and <span class="math inline">\({\boldsymbol{\beta}}_1 = (\beta_1, \beta_2, \ldots, \beta_{p_1})^T\)</span>.</p>
<p>How do we compare these models, specifically to test <span class="math inline">\(H_0: (\beta_{p_0+1}, \beta_{p_0 + 2}, \ldots, \beta_{p_1}) = {\boldsymbol{0}}\)</span> vs <span class="math inline">\(H_1: (\beta_{p_0+1}, \beta_{p_0 + 2}, \ldots, \beta_{p_1}) \not= {\boldsymbol{0}}\)</span>?</p>
<p>The basic idea to perform this test is to compare the goodness of fits of each model via a pivotal statistic. We will discuss the generalized LRT and ANOVA approaches.</p>
</section><section id="generalized-lrt" class="slide level2">
<h2>Generalized LRT</h2>
<p>Under the OLS Normal model, it follows that <span class="math inline">\(\hat{{\boldsymbol{\beta}}}_0 = ({\boldsymbol{X}}^T_0 {\boldsymbol{X}}_0)^{-1} {\boldsymbol{X}}_0^T {\boldsymbol{Y}}\)</span> is the MLE under the null hypothesis and <span class="math inline">\(\hat{{\boldsymbol{\beta}}}_1 = ({\boldsymbol{X}}^T_1 {\boldsymbol{X}}_1)^{-1} {\boldsymbol{X}}_1^T {\boldsymbol{Y}}\)</span> is the unconstrained MLE. Also, the respective MLEs of <span class="math inline">\(\sigma^2\)</span> are</p>
<p><span class="math display">\[
\hat{\sigma}^2_0 = \frac{\sum_{i=1}^n (Y_i - \hat{Y}_{0,i})^2}{n}
\]</span></p>
<p><span class="math display">\[
\hat{\sigma}^2_1 = \frac{\sum_{i=1}^n (Y_i - \hat{Y}_{1,i})^2}{n}
\]</span></p>
<p>where <span class="math inline">\(\hat{{\boldsymbol{Y}}}_{0} = {\boldsymbol{X}}_0 \hat{{\boldsymbol{\beta}}}_0\)</span> and <span class="math inline">\(\hat{{\boldsymbol{Y}}}_{1} = {\boldsymbol{X}}_1 \hat{{\boldsymbol{\beta}}}_1\)</span>.</p>
</section><section class="slide level2">

<p>The generalized LRT statistic is</p>
<p><span class="math display">\[
\lambda({\boldsymbol{X}}, {\boldsymbol{Y}}) = \frac{L\left(\hat{{\boldsymbol{\beta}}}_1, \hat{\sigma}^2_1; {\boldsymbol{X}}, {\boldsymbol{Y}}\right)}{L\left(\hat{{\boldsymbol{\beta}}}_0, \hat{\sigma}^2_0; {\boldsymbol{X}}, {\boldsymbol{Y}}\right)}
\]</span></p>
<p>where <span class="math inline">\(2\log\lambda({\boldsymbol{X}}, {\boldsymbol{Y}})\)</span> has a <span class="math inline">\(\chi^2_{p_1 - p_0}\)</span> null distribution.</p>
</section><section id="nested-projections" class="slide level2">
<h2>Nested Projections</h2>
<p>We can apply the Pythagorean theorem we saw earlier to linear subspaces to get:</p>
<p><span class="math display">\[
\begin{aligned}
\| {\boldsymbol{Y}}\|^2_2 &amp; = \| ({\boldsymbol{I}}- {\boldsymbol{P}}_1) {\boldsymbol{Y}}\|_{2}^{2} + \| {\boldsymbol{P}}_1 {\boldsymbol{Y}}\|_{2}^{2} \\
&amp; = \| ({\boldsymbol{I}}- {\boldsymbol{P}}_1) {\boldsymbol{Y}}\|_{2}^{2} + \| ({\boldsymbol{P}}_1 - {\boldsymbol{P}}_0) {\boldsymbol{Y}}\|_{2}^{2} + \| {\boldsymbol{P}}_0 {\boldsymbol{Y}}\|_{2}^{2}
\end{aligned}
\]</span></p>
<p>We can also use the Pythagorean theorem to decompose the residuals from the smaller projection <span class="math inline">\({\boldsymbol{P}}_0\)</span>:</p>
<p><span class="math display">\[
\| ({\boldsymbol{I}}- {\boldsymbol{P}}_0) {\boldsymbol{Y}}\|^2_2 = \| ({\boldsymbol{I}}- {\boldsymbol{P}}_1) {\boldsymbol{Y}}\|^2_2 + \| ({\boldsymbol{P}}_1 - {\boldsymbol{P}}_0) {\boldsymbol{Y}}\|^2_2
\]</span></p>
</section><section id="f-statistic" class="slide level2">
<h2><em>F</em> Statistic</h2>
<p>The <span class="math inline">\(F\)</span> statistic compares the improvement of goodness in fit of the larger model to that of the smaller model in terms of sums of squared residuals, and it scales this improvement by an estimate of <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
F &amp; = \frac{\left[\| ({\boldsymbol{I}}- {\boldsymbol{P}}_0) {\boldsymbol{Y}}\|^2_2 - \| ({\boldsymbol{I}}- {\boldsymbol{P}}_1) {\boldsymbol{Y}}\|^2_2\right]/(p_1 - p_0)}{\| ({\boldsymbol{I}}- {\boldsymbol{P}}_1) {\boldsymbol{Y}}\|^2_2/(n-p_1)} \\
&amp; = \frac{\left[\sum_{i=1}^n (Y_i - \hat{Y}_{0,i})^2 - \sum_{i=1}^n (Y_i - \hat{Y}_{1,i})^2 \right]/(p_1 - p_0)}{\sum_{i=1}^n (Y_i - \hat{Y}_{1,i})^2 / (n - p_1)} \\
\end{aligned}
\]</span></p>
</section><section class="slide level2">

<p>Since <span class="math inline">\(\| ({\boldsymbol{I}}- {\boldsymbol{P}}_0) {\boldsymbol{Y}}\|^2_2 - \| ({\boldsymbol{I}}- {\boldsymbol{P}}_1) {\boldsymbol{Y}}\|^2_2 = \| ({\boldsymbol{P}}_1 - {\boldsymbol{P}}_0) {\boldsymbol{Y}}\|^2_2\)</span>, we can equivalently write the <span class="math inline">\(F\)</span> statistic as:</p>
<p><span class="math display">\[
\begin{aligned}
F &amp; = \frac{\| ({\boldsymbol{P}}_1 - {\boldsymbol{P}}_0) {\boldsymbol{Y}}\|^2_2 / (p_1 - p_0)}{\| ({\boldsymbol{I}}- {\boldsymbol{P}}_1) {\boldsymbol{Y}}\|^2_2/(n-p_1)} \\
&amp; = \frac{\sum_{i=1}^n (\hat{Y}_{1,i} - \hat{Y}_{0,i})^2 / (p_1 - p_0)}{\sum_{i=1}^n (Y_i - \hat{Y}_{1,i})^2 / (n - p_1)}
\end{aligned} 
\]</span></p>
</section><section id="f-distribution" class="slide level2">
<h2><em>F</em> Distribution</h2>
<p>Suppose we have independent random variables <span class="math inline">\(V \sim \chi^2_a\)</span> and <span class="math inline">\(W \sim \chi^2_b\)</span>. It follows that</p>
<p><span class="math display">\[
\frac{V/a}{W/b} \sim F_{a,b}
\]</span></p>
<p>where <span class="math inline">\(F_{a,b}\)</span> is the <span class="math inline">\(F\)</span> distribution with <span class="math inline">\((a, b)\)</span> degrees of freedom.</p>
</section><section class="slide level2">

<p>By arguments similar to those given above, we have</p>
<p><span class="math display">\[
\frac{\| ({\boldsymbol{P}}_1 - {\boldsymbol{P}}_0) {\boldsymbol{Y}}\|^2_2}{\sigma^2} \sim \chi^2_{p_1 - p_0}
\]</span></p>
<p><span class="math display">\[
\frac{\| ({\boldsymbol{I}}- {\boldsymbol{P}}_1) {\boldsymbol{Y}}\|^2_2}{\sigma^2} \sim \chi^2_{n-p_1}
\]</span></p>
<p>and these two rv’s are independent.</p>
</section><section id="f-test" class="slide level2">
<h2><em>F</em> Test</h2>
<p>Suppose that the OLS model holds where <span class="math inline">\({\boldsymbol{E}}| {\boldsymbol{X}}\sim \mbox{MVN}_n({\boldsymbol{0}}, \sigma^2 {\boldsymbol{I}})\)</span>.</p>
<p>In order to test <span class="math inline">\(H_0: (\beta_{p_0+1}, \beta_{p_0 + 2}, \ldots, \beta_{p_1}) = {\boldsymbol{0}}\)</span> vs <span class="math inline">\(H_1: (\beta_{p_0+1}, \beta_{p_0 + 2}, \ldots, \beta_{p_1}) \not= {\boldsymbol{0}}\)</span>, we can form the <span class="math inline">\(F\)</span> statistic as given above, which has null distribution <span class="math inline">\(F_{p_1 - p_0, n - p_1}\)</span>. The p-value is calculated as <span class="math inline">\(\Pr(F^* \geq F)\)</span> where <span class="math inline">\(F\)</span> is the observed <span class="math inline">\(F\)</span> statistic and <span class="math inline">\(F^* \sim F_{p_1 - p_0, n - p_1}\)</span>.</p>
<p>If the above assumption on the distribution of <span class="math inline">\({\boldsymbol{E}}| {\boldsymbol{X}}\)</span> only approximately holds, then the <span class="math inline">\(F\)</span> test p-value is also an approximation.</p>
</section><section id="example-davis-data" class="slide level2">
<h2>Example: Davis Data</h2>
<pre class="r"><code>&gt; library(&quot;car&quot;)
&gt; data(&quot;Davis&quot;, package=&quot;car&quot;)</code></pre>
<pre class="r"><code>&gt; htwt &lt;- tbl_df(Davis)
&gt; htwt[12,c(2,3)] &lt;- htwt[12,c(3,2)]
&gt; head(htwt)
# A tibble: 6 × 5
     sex weight height repwt repht
  &lt;fctr&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;
1      M     77    182    77   180
2      F     58    161    51   159
3      F     53    161    54   158
4      M     68    177    70   175
5      F     59    157    59   155
6      M     76    170    76   165</code></pre>
</section><section id="comparing-linear-models-in-r" class="slide level2">
<h2>Comparing Linear Models in R</h2>
<p>Example: Davis Data</p>
<p>Suppose we are considering the three following models:</p>
<pre class="r"><code>&gt; f1 &lt;- lm(weight ~ height, data=htwt)
&gt; f2 &lt;- lm(weight ~ height + sex, data=htwt)
&gt; f3 &lt;- lm(weight ~ height + sex + height:sex, data=htwt)</code></pre>
<p>How do we determine if the additional terms in models <code>f2</code> and <code>f3</code> are needed?</p>
</section><section id="anova-version-2" class="slide level2">
<h2>ANOVA (Version 2)</h2>
<p>A generalization of ANOVA exists that allows us to compare two nested models, quantifying their differences in terms of goodness of fit and performing a hypothesis test of whether this difference is statistically significant.</p>
<p>A model is <em>nested</em> within another model if their difference is simply the absence of certain terms in the smaller model.</p>
<p>The null hypothesis is that the additional terms have coefficients equal to zero, and the alternative hypothesis is that at least one coefficient is nonzero.</p>
<p>Both versions of ANOVA can be described in a single, elegant mathematical framework.</p>
</section><section id="comparing-two-models-with-anova" class="slide level2">
<h2>Comparing Two Models <br> with <code>anova()</code></h2>
<p>This provides a comparison of the improvement in fit from model <code>f2</code> compared to model <code>f1</code>:</p>
<pre class="r"><code>&gt; anova(f1, f2)
Analysis of Variance Table

Model 1: weight ~ height
Model 2: weight ~ height + sex
  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
1    198 14321                                  
2    197 12816  1    1504.9 23.133 2.999e-06 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</section><section id="when-theres-a-single-variable-difference" class="slide level2">
<h2>When There’s a Single Variable Difference</h2>
<p>Compare above <code>anova(f1, f2)</code> p-value to that for the <code>sex</code> term from the <code>f2</code> model:</p>
<pre class="r"><code>&gt; library(broom)
&gt; tidy(f2)
         term    estimate   std.error statistic      p.value
1 (Intercept) -76.6167326 15.71504644 -4.875374 2.231334e-06
2      height   0.8105526  0.09529565  8.505662 4.499241e-15
3        sexM   8.2268893  1.71050385  4.809629 2.998988e-06</code></pre>
</section><section id="calculating-the-f-statistic" class="slide level2">
<h2>Calculating the F-statistic</h2>
<pre class="r"><code>&gt; anova(f1, f2)
Analysis of Variance Table

Model 1: weight ~ height
Model 2: weight ~ height + sex
  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
1    198 14321                                  
2    197 12816  1    1504.9 23.133 2.999e-06 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>How the F-statistic is calculated:</p>
<pre class="r"><code>&gt; n &lt;- nrow(htwt)
&gt; ss1 &lt;- (n-1)*var(f1$residuals)
&gt; ss1
[1] 14321.11
&gt; ss2 &lt;- (n-1)*var(f2$residuals)
&gt; ss2
[1] 12816.18
&gt; ((ss1 - ss2)/anova(f1, f2)$Df[2])/(ss2/f2$df.residual)
[1] 23.13253</code></pre>
</section><section id="calculating-the-generalized-lrt" class="slide level2">
<h2>Calculating the Generalized LRT</h2>
<pre class="r"><code>&gt; anova(f1, f2, test=&quot;LRT&quot;)
Analysis of Variance Table

Model 1: weight ~ height
Model 2: weight ~ height + sex
  Res.Df   RSS Df Sum of Sq  Pr(&gt;Chi)    
1    198 14321                           
2    197 12816  1    1504.9 1.512e-06 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>&gt; library(lmtest)
&gt; lrtest(f1, f2)
Likelihood ratio test

Model 1: weight ~ height
Model 2: weight ~ height + sex
  #Df LogLik Df  Chisq Pr(&gt;Chisq)    
1   3 -710.9                         
2   4 -699.8  1 22.205   2.45e-06 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</section><section class="slide level2">

<p>These tests produce slightly different answers because <code>anova()</code> adjusts for degrees of freedom when estimating the variance, whereas <code>lrtest()</code> is the strict generalized LRT. See <a href="https://stats.stackexchange.com/questions/155474/r-why-does-lrtest-not-match-anovatest-lrt">here</a>.</p>
</section><section id="anova-on-more-distant-models" class="slide level2">
<h2>ANOVA on More Distant Models</h2>
<p>We can compare models with multiple differences in terms:</p>
<pre class="r"><code>&gt; anova(f1, f3)
Analysis of Variance Table

Model 1: weight ~ height
Model 2: weight ~ height + sex + height:sex
  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
1    198 14321                                  
2    196 12567  2      1754 13.678 2.751e-06 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</section><section id="compare-multiple-models-at-once" class="slide level2">
<h2>Compare Multiple Models at Once</h2>
<p>We can compare multiple models at once:</p>
<pre class="r"><code>&gt; anova(f1, f2, f3)
Analysis of Variance Table

Model 1: weight ~ height
Model 2: weight ~ height + sex
Model 3: weight ~ height + sex + height:sex
  Res.Df   RSS Df Sum of Sq       F    Pr(&gt;F)    
1    198 14321                                   
2    197 12816  1   1504.93 23.4712 2.571e-06 ***
3    196 12567  1    249.04  3.8841   0.05015 .  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</section></section>
<section><section id="extras" class="titleslide slide level1"><h1>Extras</h1></section><section id="source" class="slide level2">
<h2>Source</h2>
<p><a href="https://github.com/jdstorey/asdslectures/blob/master/LICENSE.md">License</a></p>
<p><a href="https://github.com/jdstorey/asdslectures/">Source Code</a></p>
</section><section id="session-information" class="slide level2">
<h2>Session Information</h2>
<section style="font-size: 0.75em;">
<pre class="r"><code>&gt; sessionInfo()
R version 3.3.2 (2016-10-31)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: macOS Sierra 10.12.4

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods  
[7] base     

other attached packages:
 [1] lmtest_0.9-35   zoo_1.8-0       broom_0.4.2    
 [4] car_2.1-4       MASS_7.3-47     dplyr_0.5.0    
 [7] purrr_0.2.2     readr_1.1.0     tidyr_0.6.2    
[10] tibble_1.3.0    ggplot2_2.2.1   tidyverse_1.1.1
[13] knitr_1.15.1    magrittr_1.5    devtools_1.12.0

loaded via a namespace (and not attached):
 [1] revealjs_0.9       reshape2_1.4.2     splines_3.3.2     
 [4] haven_1.0.0        lattice_0.20-35    colorspace_1.3-2  
 [7] htmltools_0.3.6    mgcv_1.8-17        yaml_2.1.14       
[10] nloptr_1.0.4       foreign_0.8-68     withr_1.0.2       
[13] DBI_0.6-1          modelr_0.1.0       readxl_1.0.0      
[16] plyr_1.8.4         stringr_1.2.0      MatrixModels_0.4-1
[19] munsell_0.4.3      gtable_0.2.0       cellranger_1.1.0  
[22] rvest_0.3.2        codetools_0.2-15   psych_1.7.5       
[25] memoise_1.1.0      evaluate_0.10      labeling_0.3      
[28] forcats_0.2.0      SparseM_1.77       quantreg_5.33     
[31] pbkrtest_0.4-7     parallel_3.3.2     Rcpp_0.12.10      
[34] scales_0.4.1       backports_1.0.5    jsonlite_1.4      
[37] lme4_1.1-13        mnormt_1.5-5       hms_0.3           
[40] digest_0.6.12      stringi_1.1.5      grid_3.3.2        
[43] rprojroot_1.2      tools_3.3.2        lazyeval_0.2.0    
[46] Matrix_1.2-10      xml2_1.1.1         lubridate_1.6.0   
[49] assertthat_0.2.0   minqa_1.2.4        rmarkdown_1.5     
[52] httr_1.2.1         R6_2.2.0           nnet_7.3-12       
[55] nlme_3.1-131      </code></pre>
</section>
</section></section>
    </div>
  </div>

  <script src="libs/reveal.js-3.3.0.1/lib/js/head.min.js"></script>
  <script src="libs/reveal.js-3.3.0.1/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display a presentation progress bar
        progress: true,
        // Display the page number of the current slide
        slideNumber: false,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: true,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom



        chalkboard: {
        },

        keyboard: {
          67: function() { RevealChalkboard.toggleNotesCanvas() },    // toggle notes canvas when 'c' is pressed
          66: function() { RevealChalkboard.toggleChalkboard() }, // toggle chalkboard when 'b' is pressed
          46: function() { RevealChalkboard.clear() },    // clear chalkboard when 'DEL' is pressed
           8: function() { RevealChalkboard.reset() },    // reset chalkboard data on current slide when 'BACKSPACE' is pressed
          68: function() { RevealChalkboard.download() }, // downlad recorded chalkboard drawing when 'd' is pressed
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'libs/reveal.js-3.3.0.1/plugin/zoom-js/zoom.js', async: true },
          { src: 'libs/reveal.js-3.3.0.1/plugin/chalkboard/chalkboard.js', async: true },
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>


  </body>
</html>
