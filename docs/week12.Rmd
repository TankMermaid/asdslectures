---
title: QCB 508 -- Week 12
author: John D. Storey
date: Spring 2017
output: 
  revealjs::revealjs_presentation:
    theme: simple
    transition: slide
    center: true
    highlight: null
    self_contained: false
    lib_dir: libs
    reveal_plugins: ["chalkboard", "zoom"]
    reveal_options:
      slideNumber: false
      progress: true
    includes:
      before_body: customization/doc_prefix.html
    css: customization/custom.css
---

\providecommand{\E}{\operatorname{E}}
\providecommand{\V}{\operatorname{Var}}
\providecommand{\Cov}{\operatorname{Cov}}
\providecommand{\se}{\operatorname{se}}
\providecommand{\logit}{\operatorname{logit}}
\providecommand{\iid}{\; \stackrel{\text{iid}}{\sim}\;}
\providecommand{\asim}{\; \stackrel{.}{\sim}\;}
\providecommand{\xs}{x_1, x_2, \ldots, x_n}
\providecommand{\Xs}{X_1, X_2, \ldots, X_n}
\providecommand{\bB}{\boldsymbol{B}}
\providecommand{\bb}{\boldsymbol{\beta}}
\providecommand{\bx}{\boldsymbol{x}}
\providecommand{\bX}{\boldsymbol{X}}
\providecommand{\by}{\boldsymbol{y}}
\providecommand{\bY}{\boldsymbol{Y}}
\providecommand{\bz}{\boldsymbol{z}}
\providecommand{\bZ}{\boldsymbol{Z}}
\providecommand{\be}{\boldsymbol{e}}
\providecommand{\bE}{\boldsymbol{E}}
\providecommand{\bs}{\boldsymbol{s}}
\providecommand{\bS}{\boldsymbol{S}}
\providecommand{\bP}{\boldsymbol{P}}
\providecommand{\bI}{\boldsymbol{I}}
\providecommand{\bD}{\boldsymbol{D}}
\providecommand{\bd}{\boldsymbol{d}}
\providecommand{\bW}{\boldsymbol{W}}
\providecommand{\bw}{\boldsymbol{w}}
\providecommand{\bM}{\boldsymbol{M}}
\providecommand{\bPhi}{\boldsymbol{\Phi}}
\providecommand{\bphi}{\boldsymbol{\phi}}
\providecommand{\bN}{\boldsymbol{N}}
\providecommand{\bR}{\boldsymbol{R}}
\providecommand{\bu}{\boldsymbol{u}}
\providecommand{\bU}{\boldsymbol{U}}
\providecommand{\bv}{\boldsymbol{v}}
\providecommand{\bV}{\boldsymbol{V}}
\providecommand{\bO}{\boldsymbol{0}}
\providecommand{\bOmega}{\boldsymbol{\Omega}}
\providecommand{\bLambda}{\boldsymbol{\Lambda}}
\providecommand{\bSig}{\boldsymbol{\Sigma}}
\providecommand{\bSigma}{\boldsymbol{\Sigma}}
\providecommand{\bt}{\boldsymbol{\theta}}
\providecommand{\bT}{\boldsymbol{\Theta}}
\providecommand{\bpi}{\boldsymbol{\pi}}
\providecommand{\argmax}{\text{argmax}}
\providecommand{\KL}{\text{KL}}
\providecommand{\fdr}{{\rm FDR}}
\providecommand{\pfdr}{{\rm pFDR}}
\providecommand{\mfdr}{{\rm mFDR}}
\providecommand{\bh}{\hat}
\providecommand{\dd}{\lambda}
\providecommand{\q}{\operatorname{q}}

```{r, message=FALSE, echo=FALSE, cache=FALSE}
source("./customization/knitr_options.R")
```

```{r, message=FALSE, echo=FALSE, cache=FALSE}
library(broom)
library(MASS)
library(qvalue)
```

# <img src="./images/howto.jpg"></img>

# EDA of HD Data

## Rationale

Exploratory data analysis (EDA) of high-dimensional data adds the additional challenge that many variables must be examined simultaneously.  Therefore, in addition to the EDA methods we discussed earlier, methods are often employed to organize, visualize, or numerically capture high-dimensional data into lower dimensions.

Examples of EDA approaches applied to HD data include:

- Traditional EDA methods covered earlier
- Cluster analysis
- Dimensionality reduction

## Cluster Analysis

An overview of common **cluster analysis** methods can be found here:

<http://sml201.github.io/lectures/week12/week12.html>

These slides include:

- Distance measures
- Hierarchical clustering
- $K$-means clustering

## Example: Cancer Subtypes

<center>![cancer_clustering](images/cancer_clustering.jpg)</center>

<font size=3em>
Figure from [Alizadeh et al. (2000) *Nature*](http://www.nature.com/nature/journal/v403/n6769/abs/403503a0.html).
</font>

## Dimensionality Reduction

The goal of **dimensionality reduction** is to extract low dimensional representations of high dimensional data that are useful for visualization, exploration, inference, or prediction.

The low dimensional representations should capture key sources of variation in the data.

## Some Methods

- Principal component analysis
- Singular value decomposition 
- Latent variable modeling
- Vector quantization
- Self-organizing maps
- Multidimensional scaling

## Example: Weather Data

These daily temperature data (in tenths of degrees C) come from meteorogical observations for weather stations in the US for the year 2012 provided by NOAA (National Oceanic and Atmospheric Administration).:

```{r}
load("./data/weather_data.RData")
dim(weather_data)

weather_data[1:5, 1:7]
```

This matrix contains temperature data on `r ncol(weather_data)` days and `r nrow(weather_data)` stations that were randomly selected.

----

Convert temperatures to Fahrenheit:

```{r}
weather_data <- 0.18*weather_data + 32
weather_data[1:5, 1:6]

apply(weather_data, 1, median) %>% 
  quantile(probs=seq(0,1,0.1))
```

----

Here are the `r nrow(weather_data)` rows converted to a single row that captures the most variation among the rows:

```{r, echo=FALSE}
m <- rowMeans(weather_data)
tm <- mean(weather_data)
x <- weather_data - m
x <- x/sqrt(nrow(x)-1)
s <- svd(x)
#y <- colMeans(s$d[1] * (s$u[,1] %*% t(s$v[,1]))) + tm
y <- -s$d[1] * s$v[,1] + tm
day_of_the_year <- as.numeric(colnames(weather_data))
data.frame(day=day_of_the_year, max_temp=y) %>%
  ggplot() + geom_point(aes(x=day, y=max_temp))
```


# Principal Component Analysis

## Goal

For a given set of variables, **principal component analysis** (PCA) finds (constrained) weighted sums of the variables to produce variables (called principal components) that capture consectuive maximum levels of variation in the data.

Specifically, the first principal component is the weighted sum of the variables that results in a component with the highest variation.  

This component is then "removed" from the data, and the second principal component is obtained on the resulting residuals.  

This process is repeated until there is no variation left in the data.

## Population PCA

Suppose we have $m$ random variables $X_1, X_2, \ldots, X_m$.  We wish to identify a set of weights $w_1, w_2, \ldots, w_m$ that maximizes

$$
\V\left(w_1 X_1 + w_2 X_2 + \cdots + w_m X_m \right).
$$

However, this is unbounded, so we need to constrain the weights.  It turns out that constraining the weights so that 

$$
\| \bw \|_2^2 = \sum_{i=1}^m w_i^2 = 1
$$

is both interpretable and mathematically tractable.

----

Therefore we wish to maximize

$$
\V\left(w_1 X_1 + w_2 X_2 + \cdots + w_m X_m \right)
$$

subject to $\| \bw \|_2^2 = 1$.  Let $\bSig$ be the $m \times m$ population covariance matrix of the random variables $X_1, X_2, \ldots, X_m$.  It follows that

$$
\V\left(w_1 X_1 + w_2 X_2 + \cdots + w_m X_m \right) = \bw^T \bSig \bw.
$$

Using a Lagrange multiplier, we wish to maximize

$$
\bw^T \bSig \bw + \lambda(\bw^T \bw - 1).
$$

----

Differentiating with respect to $\bw$ and setting to $\bO$, we get $\bSig \bw - \lambda \bw = 0$ or

$$
\bSig \bw = \lambda \bw.
$$

For any such $\bw$ and $\lambda$ where this holds, note that

$$
\V\left(w_1 X_1 + w_2 X_2 + \cdots + w_m X_m \right) = \bw^T \bSig \bw = \lambda
$$

so the variance is $\lambda$.

----

The eigendecompositon of a matrix identifies all such solutions to $\bSig \bw = \lambda \bw$.  Specifically, it calculates the decompositon

$$
\bSig = \bW \bLambda \bW^T
$$

where $\bW$ is an $m \times m$ orthogonal matrix and $\bLambda$ is a diagonal matrix with entries $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_m \geq 0$.  

The fact that $\bW$ is orthogonal means $\bW \bW^T = \bW^T \bW = \bI$.  

----

The following therefore hold:

- For each column $j$ of $\bW$, say $\bw_j$, it follows that $\bSig \bw_j = \lambda_j \bw_j$
- $\| \bw_j \|^2_2 = 1$ and $\bw_j^T \bw_k = \bO$ for $\lambda_j \not= \lambda_k$
- $\V(\bw_j^T \bX) = \lambda_j$
- $\V(\bw_1^T \bX) \geq \V(\bw_2^T \bX) \geq \cdots \geq \V(\bw_m^T \bX)$
- $\bSig = \sum_{j=1}^m \lambda_j \bw^j \bw_j^T$
- For $\lambda_j \not= \lambda_k$, 
$$\Cov(\bw_j^T \bX, \bw_k^T \bX) = \bw_j^T \bSig \bw_k = \lambda_k \bw_j^T \bw_k =  \bO$$ 

## Population PCs

The $j$th **population principal component** (PC) of $X_1, X_2, \ldots, X_m$ is 

$$
\bw_j^T \bX = w_{1j} X_1 + w_{2j} X_2 + \cdots + w_{mj} X_m
$$

where $\bw_j = (w_{1j}, w_{2j}, \ldots, w_{mj})^T$ is column $j$ of $\bW$ from the eigendecomposition

$$
\bSig = \bW \bLambda \bW^T.
$$

The column $\bw_j$ are called the **loadings** of the $j$th principal component.  The **variance explained** by the $j$th PC is $\lambda_j$, which is diagonal element $j$ of $\bLambda$.

## Sample PCA

Suppose we have $m$ variables, each with $n$ observations:


$$
\begin{aligned}
\bx_1 & = (x_{11}, x_{12}, \ldots, x_{1n}) \\
\bx_2 & = (x_{21}, x_{22}, \ldots, x_{2n}) \\
\  & \vdots \ \\
\bx_m & = (x_{m1}, x_{m2}, \ldots, x_{mn})
\end{aligned}
$$

We can organize these variables into an $m \times n$ matrix $\bX$ where row $i$ is $\bx_i$.

PCA can be extended from the population scenario applied to rv's to the sample scenario applied to the observed data $\bX$.

----

Consider all possible weighted sums of these variables

$$\tilde{\pmb{x}} = \sum_{i=1}^{m} u_i \pmb{x_i}$$

where we constrain $\sum_{i=1}^{m} u_i^2 = 1$.

The first principal component of $\bX$ is the results $\tilde{\bx}$ with maximum sample variance

$$
s^2_{\tilde{\bx}} = \frac{\sum_{j=1}^n \left(\tilde{x}_j - \frac{1}{n} \sum_{k=1}^n \tilde{x}_k \right)^2}{n-1}.
$$

This first sample principal component (PC) is then "removed" from the data, and the procedure is repeated until $\min(m, n-1)$ sample PCs are constructed.

----

The sample PCs are found in a manner analogous to the population PCs. First, we construct the $m \times m$ sample covariance matrix $\bS$ with $(i,j)$ entry

$$
s_{ij} = \frac{\sum_{k=1}^n (x_{ik} - \bar{x}_{i\cdot})(x_{jk} - \bar{x}_{j\cdot})}{n-1}.
$$

Identifying $\bu$ that maximizes $s^2_{\tilde{\bx}}$ also maximizes

$$
\bu^T \bS \bu.
$$

----

Following the steps from before, we want to identify $\bu$ and $\lambda$ where

$$
\bS \bu = \lambda \bu.
$$

which is accomplished with the eigendecomposition

$$
\bS = \bU \bLambda \bU^T
$$

where again $\bU^T \bU = \bU \bU^T = \bI$ and $\bLambda$ is a diagonal matrix so that $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_m \geq 0$.

## Sample PCs

Let $x^*_{ij} = x_{ij} - \bar{x}_{i\cdot}$ be the row-wise mean-centered values of $\bX$, and let $\bX^*$ be the matrix composed of these values.  Also, let $\bu_j$ be column $j$ of $\bU$ from $\bS = \bU \bLambda \bU^T$.

Sample PC $j$ is then 

$$
\tilde{\bx}_j = \bu_j^T \bX^* = \sum_{i=1}^m u_{ij} \bx^*_i 
$$

for $j = 1, 2, \ldots, \min(m, n-1)$.

----

The loadings corresponding to PC $j$ are $\bu_j$.

Note that the mean of PC $j$ is zero, i.e., that

$$
\frac{1}{n} \sum_{k=1}^n \tilde{x}_{jk} = 0.
$$

It can be calculated that the variance of PC $j$ is

$$
s^2_{\tilde{\bx}_j} = \frac{\sum_{k=1}^n \tilde{x}_{jk}^2}{n-1} = \lambda_j.
$$

## Proportion of Variance Explained

The proportion of variance explained by PC $j$ is

$$
\operatorname{PVE}_j = \frac{\lambda_j}{\sum_{k=1}^m \lambda_k}.
$$

## Singular Value Decomposition

One way in which PCA is performed is to carry out a **singular value decomposition** (SVD) of the data matrix $\bX$.  Let $q = \min(m, n)$. Recalling that $\bX^*$ is the row-wise mean centered $\bX$, we can take the SVD of $\bX^*/\sqrt{n-1}$ to obtain

$$
\frac{1}{\sqrt{n-1}} \bX^* = \bU \bD \bV^T
$$

where $\bU_{m \times q}$, $\bV_{n \times q}$, and diagonal $\bD_{q \times q}$.  Also, we have the orthogonality properties $\bV^T \bV = \bU^T \bU = \bI_{q}$.  Finally, $\bD$ is composed of diagonal elements $d_1 \geq d_2 \geq \cdots \geq d_q \geq 0$ where $d_q = 0$ if $q = n$.

----

Note that

$$
\bS = \frac{1}{n-1} \bX^{*} \bX^{*T} = \bU \bD \bV^T \left(\bU \bD \bV^T\right)^T = \bU \bD^2 \bU^T.
$$

Therefore:

- The variance of PC $j$ is $\lambda_j = d_j^2$
- The loadings of PC $j$ are contained in the columns of the left-hand matrix from the decomposition of $\bS$ or $\bX^*$
- PC $j$ is row $j$ of $\bD \bV^T$

## My PCA Function

```{r}
pca <- function(x, space=c("rows", "columns"), 
                center=TRUE, scale=FALSE) {
  space <- match.arg(space)
  if(space=="columns") {x <- t(x)}
  x <- t(scale(t(x), center=center, scale=scale))
  x <- x/sqrt(nrow(x)-1)
  s <- svd(x)
  loading <- s$u
  colnames(loading) <- paste0("Loading", 1:ncol(loading))
  rownames(loading) <- rownames(x)
  pc <- diag(s$d) %*% t(s$v)
  rownames(pc) <- paste0("PC", 1:nrow(pc))
  colnames(pc) <- colnames(x)
  pve <- s$d^2 / sum(s$d^2)
  if(space=="columns") {pc <- t(pc); loading <- t(loading)}
  return(list(pc=pc, loading=loading, pve=pve))
}
```

## How It Works

Input is as follows:

- `x`: a matrix of numerical values
- `space`: either `"rows"` or `"columns"`, denoting which dimension contains the variables
- `center`: if `TRUE` then the variables are mean centered before calculating PCs
- `scale`: if `TRUE` then the variables are std dev scaled before calculating PCs

----

Output is a list with the following items:

- `pc`: a matrix of all possible PCs
- `loading`:  the weights or "loadings" that determined each PC
- `pve`: the proportion of variation explained by each PC

Note that the rows or columns of `pc` and `loading` have names to let you know on which dimension the values are organized.

## The Ubiquitous Example

Here's an example very frequently encountered to explain PCA, but it's slightly complicated.

```{r}
set.seed(508)
n <- 70
z <- sqrt(0.8) * rnorm(n)
x1 <- z + sqrt(0.2) * rnorm(n)
x2 <- z + sqrt(0.2) * rnorm(n)
X <- rbind(x1, x2)
p <- pca(x=X, space="rows")
``` 

----

"The first PC finds the direction of maximal variance in the data..."

```{r, echo=FALSE, fig.width=7}
df <- data.frame(x1=c(x1, lm(x1 ~ p$pc[1,])$fit), x2=c(x2, lm(x2 ~ p$pc[1,])$fit), legend=c(rep("data",n),rep("pc1_projection",n)))
ggplot(df) + geom_point(aes(x=x1,y=x2,color=legend)) + 
  scale_color_manual(values=c("blue", "red"))
```


----

The above figure was made with the following code:

```{r, eval=FALSE}
df <- data.frame(x1=c(x1, lm(x1 ~ p$pc[1,])$fit), 
                 x2=c(x2, lm(x2 ~ p$pc[1,])$fit), 
                 legend=c(rep("data",n),rep("pc1_projection",n)))
ggplot(df) + geom_point(aes(x=x1,y=x2,color=legend)) + 
  scale_color_manual(values=c("blue", "red"))
```

The red dots are therefore the projection of `x1` and `x2` onto the first PC, so they are neither the loadings nor the PC.

Note that 

```
outer(p$loading[,1], p$pc[1,])[1,] + mean(x1) 
# yields the same as  
lm(x1 ~ p$pc[1,])$fit # and
outer(p$loading[,1], p$pc[1,])[2,] + mean(x2) 
# yields the same as 
lm(x2 ~ p$pc[1,])$fit
```

----

Here is PC1 vs PC2:

```{r}
data.frame(pc1=p$pc[1,], pc2=p$pc[2,]) %>% 
  ggplot() + geom_point(aes(x=pc1,y=pc2)) + 
  theme(aspect.ratio=1)
```


----

Here is PC1 vs `x1`:

```{r}
data.frame(pc1=p$pc[1,], x1=x1) %>% 
  ggplot() + geom_point(aes(x=pc1,y=x1)) + 
  theme(aspect.ratio=1)
```

----

Here is PC1 vs `x2`:

```{r}
data.frame(pc1=p$pc[1,], x2=x2) %>% 
  ggplot() + geom_point(aes(x=pc1,y=x2)) + 
  theme(aspect.ratio=1)
```

----

Here is PC1 vs `z`:

```{r}
data.frame(pc1=p$pc[1,], z=z) %>% 
  ggplot() + geom_point(aes(x=pc1,y=z)) + 
  theme(aspect.ratio=1)
```

# PCA Examples

## Weather Data

```{r}
mypca <- pca(weather_data, space="rows")

names(mypca)
dim(mypca$pc)
dim(mypca$loading)
```

```{r}
mypca$pc[1:3, 1:3]
mypca$loading[1:3, 1:3]
```

----

### PC1 vs Time

```{r}
day_of_the_year <- as.numeric(colnames(weather_data))
data.frame(day=day_of_the_year, PC1=mypca$pc[1,]) %>%
  ggplot() + geom_point(aes(x=day, y=PC1), size=2)
```


----

### PC2 vs Time

```{r}
data.frame(day=day_of_the_year, PC2=mypca$pc[2,]) %>%
  ggplot() + geom_point(aes(x=day, y=PC2), size=2)
```

----

### PC Biplots

Sometimes it is informative to plot a PC versus another PC.  This is called a **PC biplot**.

It is possible that interesting subgroups or clusters of *observations* will emerge.

This does not appear to be the case in the weather data set, however, due to what we observe in the next two plots.

----

### PC1 vs PC2 Biplot

```{r}
data.frame(PC1=mypca$pc[1,], PC2=mypca$pc[2,]) %>%
  ggplot() + geom_point(aes(x=PC1, y=PC2), size=2)
```

----

### Proportion of Variance Explained

```{r}
data.frame(Component=1:length(mypca$pve), PVE=mypca$pve) %>%
  ggplot() + geom_point(aes(x=Component, y=PVE), size=2)
```

----

### PCs Reproduce the Data

We can multiple the loadings matrix by the PCs matrix to reproduce the data:
```{r}
# mean centered weather data
weather_data_mc <- weather_data - rowMeans(weather_data)

# difference between the PC projections and the data
# the small sum is just machine imprecision
sum(abs(weather_data_mc/sqrt(nrow(weather_data_mc)-1) - 
          mypca$loading %*% mypca$pc))
```

----

### Loadings 

The sum of squared weights -- i.e., loadings -- equals one for each component:

```{r}
sum(mypca$loading[,1]^2)

apply(mypca$loading, 2, function(x) {sum(x^2)})
```

----

### Pairs of PCs Have Correlaton Zero

PCs by contruction have sample correlation equal to zero:

```{r}
cor(mypca$pc[1,], mypca$pc[2,])
cor(mypca$pc[1,], mypca$pc[3,])
cor(mypca$pc[1,], mypca$pc[12,])
cor(mypca$pc[5,], mypca$pc[27,])
# etc...
```

----

```{r}
day_of_the_year <- as.numeric(colnames(weather_data))
y <- -mypca$pc[1,] + mean(weather_data)
data.frame(day=day_of_the_year, max_temp=y) %>%
  ggplot() + geom_point(aes(x=day, y=max_temp))
```

## Yeast Gene Expression

Yeast cells were synchronized so that they were on the same approximate cell cycle timing.  The goal was to understand how gene expression varies over the cell cycle from a genome-wide perspective.

```{r}
load("./data/spellman.RData")
time
dim(gene_expression)
gene_expression[1:6,1:5]
```

----

### Proportion Variance Explained

```{r}
p <- pca(gene_expression, space="rows")
ggplot(data.frame(pc=1:13, pve=p$pve)) + 
  geom_point(aes(x=pc,y=pve), size=2)
```

----

### PCs vs Time (with Smoothers)

```{r, echo=FALSE}
plot(rep(time,2), c(p$pc[1,], p$pc[2,]), pch=" ",xlab="time",ylab="principal component")
points(time, p$pc[1,], pch=19, col="blue")
points(time, p$pc[2,], pch=19, col="red")
lines(smooth.spline(x=time,y=p$pc[1,],df=5), lwd=2, col="blue")
lines(smooth.spline(x=time,y=p$pc[2,],df=5), lwd=2, col="red")
legend(x="topright", legend=c("PC1", "PC2"), pch=c(19,19), col=c("blue", "red"))
```

## HapMap Genotypes

Recall the example HapMap data used to demonstrate the MCMC algorithm for estimating structure.  We curated a small data set that cleanly separates human subpopulations.

```{r}
hapmap <- read.table("./data/hapmap_sample.txt")
dim(hapmap)
hapmap[1:6,1:6]
```

----

### Proportion Variance Explained

```{r}
p <- pca(hapmap, space="rows")
ggplot(data.frame(pc=(1:ncol(hapmap)), pve=p$pve)) + 
  geom_point(aes(x=pc,y=pve), size=2)
```

----

### PC1 vs PC2 Biplot

```{r, echo=FALSE}
df <- as.data.frame(t(p$pc))
ggplot(df) + 
  geom_point(aes(x=PC1, y=PC2), size=2, alpha=0.7)
```

----

### PC1 vs PC3 Biplot

```{r, echo=FALSE}
ggplot(df) + 
  geom_point(aes(x=PC1, y=PC3), size=2, alpha=0.7)
```


----

### PC2 vs PC3 Biplot

```{r, echo=FALSE}
ggplot(df) + 
  geom_point(aes(x=PC2, y=PC3), size=2, alpha=0.7)
```


# HD Latent Variable Models

## Definition

Latent variables (or hidden variables) are random variables that are present in the underlying probabilistic model of the data, but they are unobserved.

In high-dimensional data, there may be latent variables present that affect many variables simultaneously.

These are latent variables that induce **systematic variation**.  A topic of much interest is how to estimate these and incorporate them into further HD inference procedures.

## Model

Suppose we have observed data $\bY_{m \times n}$ of $m$ variables with $n$ observations each.  Suppose there are $r$ latent variables contained in the $r$ rows of $\bZ_{r \times n}$ where

$$
\E\left[\bY_{m \times n} \left. \right| \bZ_{r \times n} \right] = \bPhi_{m \times r} \bZ_{r \times n}.
$$

Let's also assume that $m \gg n > r$.  The latent variables $\bZ$ induce systematic variation in variable $\by_i$ parameterized by $\bphi_i$ for $i = 1, 2, \ldots, m$.

## Estimation

There exist methods for estimating the row space of $\bZ$ with probability 1 as $m \rightarrow \infty$ for a fixed $n$ in two scenarios.

[Leek (2011)](http://onlinelibrary.wiley.com/doi/10.1111/j.1541-0420.2010.01455.x/abstract) shows how to do this when $\by_i | \bZ \sim \text{MVN}(\bphi_i \bZ, \sigma^2_i \bI)$, and the $\by_i | \bZ$ are jointly independent.   

[Chen and Storey (2015)](https://arxiv.org/abs/1510.03497) show how to do this when the $\by_i | \bZ$ are distributed according to a single parameter exponential family distribution with mean $\bphi_i \bZ$, and the $\by_i | \bZ$ are jointly independent.

## Jackstraw

Suppose we have a reasonable method for estimating $\bZ$ in the model

$$
\E\left[\bY \left. \right| \bZ \right] = \bPhi \bZ.
$$

The **jackstraw** method allows us to perform hypothesis tests of the form

$$
H_0: \bphi_i = \bO \mbox{ vs } H_1: \bphi_i \not= \bO.
$$

We can also perform this hypothesis test on any subset of the columns of $\bPhi$.

This is a challening problem because we have to "double dip" in the data $\bY$, first to estimate $\bZ$, and second to perform significance tests on $\bPhi$.

## Procedure

The first step is to form estimate $\hat{\bZ}$ and then test statistic $t_i$ that performs the hypothesis test for each $\bphi_i$  from $\by_i$ and $\hat{\bZ}$ ($i=1, \ldots, m$).  Assume that the larger $t_i$ is, the more evidence there is against the null hypothesis in favor of the alternative.

Next we randomly select $s$ rows of $\bY$ and permute them to create data set $\bY^{0}$.  Let this set of $s$ variables be indexed by $\mathcal{S}$.  This breaks the relationship between $\by_i$ and $\bZ$, thereby inducing a true $H_0$, for each $i \in \mathcal{S}$.

----

We estimate $\hat{\bZ}^{0}$ from $\bY^{0}$ and again obtain test statistics $t_i^{0}$.  Specifically, the test statistics $t_i^{0}$ for $i \in \mathcal{S}$ are saved as draws from the null distribution.

We repeat permutation procedure $B$ times, and then utilize all saved $sB$ permutation null statistics to calculate empirical p-values:

$$
p_i = \frac{1}{sB} \sum_{b=1}^B \sum_{k \in \mathcal{S}_b} 1\left(t_i \geq  t_k^{0b} \right).
$$

## Example: Yeast Cell Cycle

Recall the yeast cell cycle data from earlier.  We will test which genes have expression significantly associated with PC1 and PC2 since these both capture cell cycle regulation.

```{r, message=FALSE}
library(jackstraw)
load("./data/spellman.RData")
time
dim(gene_expression)
dat <- t(scale(t(gene_expression), center=TRUE, scale=FALSE))
```

----

<p style="font-size: 0.75em;">
Test for associations between PC1 and each gene, conditioning on PC1 and PC2 being relevant sources of systematic variation.
</p>

```{r, message=FALSE}
jsobj <- jackstraw(dat, r1=1, r=2, B=500, s=50, verbose=FALSE)
jsobj$p.value %>% qvalue() %>% hist()
```

----

This is the most significant gene plotted with PC1.

```{r, echo=FALSE}
y <- gene_expression[which.min(jsobj$p.value),]
p <- pca(gene_expression, space="rows")
plot(rep(time,2), c(p$pc[1,], y), pch=" ",xlab="time",ylab="expression")
points(time, p$pc[1,], pch=19, col="blue")
points(time, y, pch=19, col="red")
lines(smooth.spline(x=time,y=p$pc[1,],df=5), lwd=2, col="blue")
lines(smooth.spline(x=time,y=y,df=5), lwd=2, col="red")
legend(x="topright", legend=c("PC1", "top gene"), pch=c(19,19), col=c("blue", "red"))
```

----

<p style="font-size: 0.75em;">
Test for associations between PC2 and each gene, conditioning on PC1 and PC2 being relevant sources of systematic variation.
</p>


```{r, message=FALSE}
jsobj <- jackstraw(dat, r1=2, r=2, B=500, s=50, verbose=FALSE)
jsobj$p.value %>% qvalue() %>% hist()
```

----

This is the most significant gene plotted with PC2.

```{r, echo=FALSE}
y <- gene_expression[which.min(jsobj$p.value),]
p <- pca(gene_expression, space="rows")
plot(rep(time,2), c(p$pc[2,], y), pch=" ",xlab="time",ylab="expression")
points(time, p$pc[2,], pch=19, col="blue")
points(time, y, pch=19, col="red")
lines(smooth.spline(x=time,y=p$pc[2,],df=5), lwd=2, col="blue")
lines(smooth.spline(x=time,y=y,df=5), lwd=2, col="red")
legend(x="topright", legend=c("PC2", "top gene"), pch=c(19,19), col=c("blue", "red"))
```

## Surrogate Variable Analysis

The **surrogate variable analysis** (SVA) model combines the many responses model with the latent variable model introduced above:

$$
\bY_{m \times n} = \bB_{m \times d} \bX_{d \times n} + \bPhi_{m \times r} \bZ_{r \times n} + \bE_{m \times n}
$$

where $m \gg n > d + r$.

Here, only $\bY$ and $\bX$ are observed, so we must combine many regressors model fitting techniques with latent variable estimation.

The variables $\bZ$ are called **surrogate variables** for what would be a complete model of all systematic variation.

## Procedure

The main challenge is that the row spaces of $\bX$ and $\bZ$ may overlap.  Even when $\bX$ is the result of a randomized experiment, there will be a high probability that the row spaces of $\bX$ and $\bZ$ have some overlap.

Therefore, one cannot simply estimate $\bZ$ by applying a latent variable esitmation method on the residuals $\bY - \hat{\bB} \bX$ or on the observed response data $\bY$.  In the former case, we will only estimate $\bZ$ in the space orthogonal to $\hat{\bB} \bX$.  In the latter case, the estimate of $\bZ$ may modify the signal we can estimate in $\bB \bX$.

----

A [recent method](http://dx.doi.org/10.1080/01621459.2011.645777), takes an EM approach to esitmating $\bZ$ in the model 

$$
\bY_{m \times n} = \bB_{m \times d} \bX_{d \times n} + \bPhi_{m \times r} \bZ_{r \times n} + \bE_{m \times n}.
$$

It is shown to be necessary to penalize the likelihood in the estimation of $\bB$ --- i.e., form shrinkage estimates of $\bB$ --- in order to properly balance the row spaces of $\bX$ and $\bZ$.

----

The regularized EM algorithm, called **cross-dimensonal inference** (CDI) iterates between

1. Estimate $\bZ$ from $\bY - \hat{\bB}^{\text{Reg}} \bX$
2. Estimate $\bB$ from $\bY - \hat{\bPhi} \hat{\bZ}$

where $\hat{\bB}^{\text{Reg}}$ is a regularized or shrunken estimate of $\bB$.

It can be shown that when the regularization can be represented by a prior distribution on $\bB$ then this algorithm achieves the MAP.


## Example: Kidney Expr by Age

In [Storey et al. (2005)](http://www.pnas.org/content/102/36/12837.full), we considered a study where kidney samples were obtained on individuals across a range of ages.  The goal was to identify genes with expression associated with age.

```{r kid_load, message=FALSE, cache=TRUE}
library(edge)
library(splines)
load("./data/kidney.RData")
age <- kidcov$age
sex <- kidcov$sex
dim(kidexpr)
cov <- data.frame(sex = sex, age = age)
null_model <- ~sex
full_model <- ~sex + ns(age, df = 3)
```

----

```{r kid_lrt, dependson="kid_load", cache=TRUE}
de_obj <- build_models(data = kidexpr, cov = cov, 
                       null.model = null_model, 
                       full.model = full_model)
de_lrt <- lrt(de_obj, nullDistn = "bootstrap", bs.its = 100, verbose=FALSE)
qobj1 <- qvalueObj(de_lrt)
hist(qobj1)
```

----

Now that we have completed a standard generalized LRT, let's estimate $\bZ$ (the surrogate variables) using the `sva` package as accessed via the `edge` package.

```{r kid_sva, dependson="kid_lrt", cache=TRUE}
dim(nullMatrix(de_obj))
de_sva <- apply_sva(de_obj, n.sv=4, method="irw", B=10)
dim(nullMatrix(de_sva))
de_svalrt <- lrt(de_sva, nullDistn = "bootstrap", bs.its = 100, verbose=FALSE)
```

----

```{r kid_svaq, dependson="kid_sva", cache=TRUE}
qobj2 <- qvalueObj(de_svalrt)
hist(qobj2)
```

----

```{r qvalues1, dependson="kid_svaq"}
summary(qobj1)
```

```{r qvalues2, dependson="kid_svaq"}
summary(qobj2)
```

----

P-values from two analyses are fairly different.

```{r pval_comp, dependson="kid_svaq"}
data.frame(lrt=-log10(qobj1$pval), sva=-log10(qobj2$pval)) %>% 
  ggplot() + geom_point(aes(x=lrt, y=sva), alpha=0.3) + geom_abline()
```

# Extras

## Source

[License](https://github.com/jdstorey/asdslectures/blob/master/LICENSE.md)

[Source Code](https://github.com/jdstorey/asdslectures/)

## Session Information

<section style="font-size: 0.75em;">
```{r}
sessionInfo()
```
</section>

```{r converttonotes, include=FALSE, cache=FALSE}
source("./customization/make_notes.R")
source("./customization/make_bookdown.R")
```
