<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="John D. Storey" />
  <title>QCB 508 – Week 10</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="libs/reveal.js-3.3.0.1/css/reveal.css"/>



<link rel="stylesheet" href="libs/reveal.js-3.3.0.1/css/theme/simple.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }


  </style>

    <style type="text/css">code{white-space: pre;}</style>

    <link rel="stylesheet" href="customization/custom.css"/>

<!-- Printing and PDF exports -->
<script id="paper-css" type="application/dynamic-css">

/* Default Print Stylesheet Template
   by Rob Glazebrook of CSSnewbie.com
   Last Updated: June 4, 2008

   Feel free (nay, compelled) to edit, append, and
   manipulate this file as you see fit. */


@media print {

	/* SECTION 1: Set default width, margin, float, and
	   background. This prevents elements from extending
	   beyond the edge of the printed page, and prevents
	   unnecessary background images from printing */
	html {
		background: #fff;
		width: auto;
		height: auto;
		overflow: visible;
	}
	body {
		background: #fff;
		font-size: 20pt;
		width: auto;
		height: auto;
		border: 0;
		margin: 0 5%;
		padding: 0;
		overflow: visible;
		float: none !important;
	}

	/* SECTION 2: Remove any elements not needed in print.
	   This would include navigation, ads, sidebars, etc. */
	.nestedarrow,
	.controls,
	.fork-reveal,
	.share-reveal,
	.state-background,
	.reveal .progress,
	.reveal .backgrounds {
		display: none !important;
	}

	/* SECTION 3: Set body font face, size, and color.
	   Consider using a serif font for readability. */
	body, p, td, li, div {
		font-size: 20pt!important;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		color: #000;
	}

	/* SECTION 4: Set heading font face, sizes, and color.
	   Differentiate your headings from your body text.
	   Perhaps use a large sans-serif for distinction. */
	h1,h2,h3,h4,h5,h6 {
		color: #000!important;
		height: auto;
		line-height: normal;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		text-shadow: 0 0 0 #000 !important;
		text-align: left;
		letter-spacing: normal;
	}
	/* Need to reduce the size of the fonts for printing */
	h1 { font-size: 28pt !important;  }
	h2 { font-size: 24pt !important; }
	h3 { font-size: 22pt !important; }
	h4 { font-size: 22pt !important; font-variant: small-caps; }
	h5 { font-size: 21pt !important; }
	h6 { font-size: 20pt !important; font-style: italic; }

	/* SECTION 5: Make hyperlinks more usable.
	   Ensure links are underlined, and consider appending
	   the URL to the end of the link for usability. */
	a:link,
	a:visited {
		color: #000 !important;
		font-weight: bold;
		text-decoration: underline;
	}
	/*
	.reveal a:link:after,
	.reveal a:visited:after {
		content: " (" attr(href) ") ";
		color: #222 !important;
		font-size: 90%;
	}
	*/


	/* SECTION 6: more reveal.js specific additions by @skypanther */
	ul, ol, div, p {
		visibility: visible;
		position: static;
		width: auto;
		height: auto;
		display: block;
		overflow: visible;
		margin: 0;
		text-align: left !important;
	}
	.reveal pre,
	.reveal table {
		margin-left: 0;
		margin-right: 0;
	}
	.reveal pre code {
		padding: 20px;
		border: 1px solid #ddd;
	}
	.reveal blockquote {
		margin: 20px 0;
	}
	.reveal .slides {
		position: static !important;
		width: auto !important;
		height: auto !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 0 !important;
		zoom: 1 !important;

		overflow: visible !important;
		display: block !important;

		text-align: left !important;
		-webkit-perspective: none;
		   -moz-perspective: none;
		    -ms-perspective: none;
		        perspective: none;

		-webkit-perspective-origin: 50% 50%;
		   -moz-perspective-origin: 50% 50%;
		    -ms-perspective-origin: 50% 50%;
		        perspective-origin: 50% 50%;
	}
	.reveal .slides section {
		visibility: visible !important;
		position: static !important;
		width: auto !important;
		height: auto !important;
		display: block !important;
		overflow: visible !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 60px 20px !important;
		z-index: auto !important;

		opacity: 1 !important;

		page-break-after: always !important;

		-webkit-transform-style: flat !important;
		   -moz-transform-style: flat !important;
		    -ms-transform-style: flat !important;
		        transform-style: flat !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;

		-webkit-transition: none !important;
		   -moz-transition: none !important;
		    -ms-transition: none !important;
		        transition: none !important;
	}
	.reveal .slides section.stack {
		padding: 0 !important;
	}
	.reveal section:last-of-type {
		page-break-after: avoid !important;
	}
	.reveal section .fragment {
		opacity: 1 !important;
		visibility: visible !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;
	}
	.reveal section img {
		display: block;
		margin: 15px 0px;
		background: rgba(255,255,255,1);
		border: 1px solid #666;
		box-shadow: none;
	}

	.reveal section small {
		font-size: 0.8em;
	}

}  
</script>


<script id="pdf-css" type="application/dynamic-css">
    
/**
 * This stylesheet is used to print reveal.js
 * presentations to PDF.
 *
 * https://github.com/hakimel/reveal.js#pdf-export
 */

* {
	-webkit-print-color-adjust: exact;
}

body {
	margin: 0 auto !important;
	border: 0;
	padding: 0;
	float: none !important;
	overflow: visible;
}

html {
	width: 100%;
	height: 100%;
	overflow: visible;
}

/* Remove any elements not needed in print. */
.nestedarrow,
.reveal .controls,
.reveal .progress,
.reveal .playback,
.reveal.overview,
.fork-reveal,
.share-reveal,
.state-background {
	display: none !important;
}

h1, h2, h3, h4, h5, h6 {
	text-shadow: 0 0 0 #000 !important;
}

.reveal pre code {
	overflow: hidden !important;
	font-family: Courier, 'Courier New', monospace !important;
}

ul, ol, div, p {
	visibility: visible;
	position: static;
	width: auto;
	height: auto;
	display: block;
	overflow: visible;
	margin: auto;
}
.reveal {
	width: auto !important;
	height: auto !important;
	overflow: hidden !important;
}
.reveal .slides {
	position: static;
	width: 100%;
	height: auto;

	left: auto;
	top: auto;
	margin: 0 !important;
	padding: 0 !important;

	overflow: visible;
	display: block;

	-webkit-perspective: none;
	   -moz-perspective: none;
	    -ms-perspective: none;
	        perspective: none;

	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
	   -moz-perspective-origin: 50% 50%;
	    -ms-perspective-origin: 50% 50%;
	        perspective-origin: 50% 50%;
}

.reveal .slides section {
	page-break-after: always !important;

	visibility: visible !important;
	position: relative !important;
	display: block !important;
	position: relative !important;

	margin: 0 !important;
	padding: 0 !important;
	box-sizing: border-box !important;
	min-height: 1px;

	opacity: 1 !important;

	-webkit-transform-style: flat !important;
	   -moz-transform-style: flat !important;
	    -ms-transform-style: flat !important;
	        transform-style: flat !important;

	-webkit-transform: none !important;
	   -moz-transform: none !important;
	    -ms-transform: none !important;
	        transform: none !important;
}

.reveal section.stack {
	margin: 0 !important;
	padding: 0 !important;
	page-break-after: avoid !important;
	height: auto !important;
	min-height: auto !important;
}

.reveal img {
	box-shadow: none;
}

.reveal .roll {
	overflow: visible;
	line-height: 1em;
}

/* Slide backgrounds are placed inside of their slide when exporting to PDF */
.reveal section .slide-background {
	display: block !important;
	position: absolute;
	top: 0;
	left: 0;
	width: 100%;
	z-index: -1;
}

/* All elements should be above the slide-background */
.reveal section>* {
	position: relative;
	z-index: 1;
}

/* Display slide speaker notes when 'showNotes' is enabled */
.reveal .speaker-notes-pdf {
	display: block;
	width: 100%;
	max-height: none;
	left: auto;
	top: auto;
	z-index: 100;
}

/* Display slide numbers when 'slideNumber' is enabled */
.reveal .slide-number-pdf {
	display: block;
	position: absolute;
	font-size: 14px;
}

</script>


<script>
var style = document.createElement( 'style' );
style.type = 'text/css';
var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
var style_script = document.getElementById(style_script_id).text;
style.innerHTML = style_script;
document.getElementsByTagName('head')[0].appendChild(style);
</script>

    <link href="libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
</head>
<body>
<style type="text/css">
p { 
  text-align: left; 
  }
.reveal pre code { 
  color: #000000; 
  background-color: rgb(240,240,240);
  font-size: 1.15em;
  border:none; 
  }
.reveal section img { 
  background:none; 
  border:none; 
  box-shadow:none;
  height: 500px;
  }
}
</style>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">QCB 508 – Week 10</h1>
    <h2 class="author">John D. Storey</h2>
    <h3 class="date">Spring 2017</h3>
</section>

<section><section id="section" class="titleslide slide level1"><h1><img src="images/howto.jpg"></img></h1></section></section>
<section><section id="logistic-regression" class="titleslide slide level1"><h1>Logistic Regression</h1></section><section id="goal" class="slide level2">
<h2>Goal</h2>
<p>Logistic regression models a Bernoulli distributed response variable in terms of linear combinations of explanatory variables.</p>
<p>This extends least squares regression to the case where the response variable captures a “success” or “failure” type outcome.</p>
</section><section id="bernoulli-as-efd" class="slide level2">
<h2>Bernoulli as EFD</h2>
<p>If <span class="math inline">\(Y \sim \mbox{Bernoulli}(p)\)</span>, then its pmf is:</p>
<p><span class="math display">\[
\begin{aligned}
f(y; p) &amp; = p^{y} (1-p)^{1-y} \\
 &amp; = \exp\left\{ \log\left(\frac{p}{1-p}\right)y + \log(1-p) \right\}
\end{aligned}
\]</span></p>
<p>In exponential family distribution (EFD) notation,</p>
<p><span class="math display">\[
\eta(p) = \log\left(\frac{p}{1-p}\right) \equiv {\operatorname{logit}}(p),
\]</span></p>
<p><span class="math inline">\(A(\eta(p)) = \log(1 + \exp(\eta)) = \log(1-p)\)</span>, and <span class="math inline">\(y\)</span> is the sufficient statistic.</p>
</section><section id="model" class="slide level2">
<h2>Model</h2>
<p><span class="math inline">\(({\boldsymbol{X}}_1, Y_1), ({\boldsymbol{X}}_2, Y_2), \ldots, ({\boldsymbol{X}}_n, Y_n)\)</span> are distributed so that <span class="math inline">\(Y_i | {\boldsymbol{X}}_i \sim \mbox{Bernoulli}(p_i)\)</span>, where <span class="math inline">\(\{Y_i | {\boldsymbol{X}}_i\}_{i=1}^n\)</span> are jointly independent and</p>
<p><span class="math display">\[
{\operatorname{logit}}\left({\operatorname{E}}[Y_i | {\boldsymbol{X}}_i]\right) = \log\left( \frac{\Pr(Y_i = 1 | {\boldsymbol{X}}_i)}{\Pr(Y_i = 0 | {\boldsymbol{X}}_i)} \right)  = {\boldsymbol{X}}_i {\boldsymbol{\beta}}.
\]</span></p>
<p>From this it follows that</p>
<p><span class="math display">\[
p_i = \frac{\exp\left({\boldsymbol{X}}_i {\boldsymbol{\beta}}\right)}{1 + \exp\left({\boldsymbol{X}}_i {\boldsymbol{\beta}}\right)}.
\]</span></p>
</section><section id="maximum-likelihood-estimation" class="slide level2">
<h2>Maximum Likelihood Estimation</h2>
<p>The <span class="math inline">\({\boldsymbol{\beta}}\)</span> are estimated from the MLE calculated from:</p>
<p><span class="math display">\[
\begin{aligned}
\ell\left({\boldsymbol{\beta}}; {\boldsymbol{y}}, {\boldsymbol{X}}\right) &amp; = \sum_{i=1}^n \log\left(\frac{p_i}{1-p_i}\right) y_i + \log(1-p_i) \\
 &amp; = \sum_{i=1}^n ({\boldsymbol{x}}_i {\boldsymbol{\beta}}) y_i - \log\left(1 + \exp\left({\boldsymbol{x}}_i {\boldsymbol{\beta}}\right) \right)  
\end{aligned}
\]</span></p>
</section><section id="iteratively-reweighted-least-squares" class="slide level2">
<h2>Iteratively Reweighted Least Squares</h2>
<ol type="1">
<li><p>Initialize <span class="math inline">\({\boldsymbol{\beta}}^{(1)}\)</span>.</p></li>
<li><p>For each iteration <span class="math inline">\(t=1, 2, \ldots\)</span>, set <span class="math display">\[
p_i^{(t)} = {\operatorname{logit}}^{-1}\left( {\boldsymbol{x}}_i {\boldsymbol{\beta}}^{(t)} \right), \ \ \ \ 
z_i^{(t)} = {\operatorname{logit}}\left(p_i^{(t)}\right) + \frac{y_i - p_i^{(t)}}{p_i^{(t)}(1-p_i^{(t)})}
\]</span> and let <span class="math inline">\({\boldsymbol{z}}^{(t)} = \left\{z_i^{(t)}\right\}_{i=1}^n\)</span>.</p></li>
</ol>
</section><section class="slide level2">

<ol start="3" type="1">
<li><p>Form <span class="math inline">\(n \times n\)</span> diagonal matrix <span class="math inline">\(\boldsymbol{W}^{(t)}\)</span> with <span class="math inline">\((i, i)\)</span> entry equal to <span class="math inline">\(p_i^{(t)}(1-p_i^{(t)})\)</span>.</p></li>
<li><p>Obtain <span class="math inline">\({\boldsymbol{\beta}}^{(t+1)}\)</span> by performing the wieghted least squares regression (see <a href="#//generalized-least-squares-1">GLS</a> from earlier)</p></li>
</ol>
<p><span class="math display">\[
{\boldsymbol{\beta}}^{(t+1)} = \left({\boldsymbol{X}}^T \boldsymbol{W}^{(t)} {\boldsymbol{X}}\right)^{-1} {\boldsymbol{X}}^T \boldsymbol{W}^{(t)} {\boldsymbol{z}}^{(t)}.
\]</span></p>
<ol start="5" type="1">
<li>Iterate Steps 2-4 over <span class="math inline">\(t=1, 2, 3, \ldots\)</span> until convergence, setting <span class="math inline">\(\hat{{\boldsymbol{\beta}}} = {\boldsymbol{\beta}}^{(\infty)}\)</span>.</li>
</ol>
</section><section id="glms" class="slide level2">
<h2>GLMs</h2>
<p>For exponential family distribution response variables, the <strong>generalized linear model</strong> is</p>
<p><span class="math display">\[
\eta\left({\operatorname{E}}[Y | {\boldsymbol{X}}]\right) = {\boldsymbol{X}}{\boldsymbol{\beta}}\]</span></p>
<p>where <span class="math inline">\(\eta(\theta)\)</span> is function of the expected value <span class="math inline">\(\theta\)</span> into the natural parameter. This is called the <strong>canonical link function</strong> in the GLM setting.</p>
<p>The <strong>iteratively reweighted least squares</strong> algorithm presented above for calculating (local) maximum likelihood estimates of <span class="math inline">\({\boldsymbol{\beta}}\)</span> has a generalization to a large class of exponential family distribution response vairables.</p>
</section></section>
<section><section id="glm-function-in-r" class="titleslide slide level1"><h1><code>glm()</code> Function in R</h1></section><section id="example-grad-school-admissions" class="slide level2">
<h2>Example: Grad School Admissions</h2>
<pre class="r"><code>&gt; mydata &lt;- 
+   read.csv(&quot;http://www.ats.ucla.edu/stat/data/binary.csv&quot;)
&gt; dim(mydata)
[1] 400   4
&gt; head(mydata)
  admit gre  gpa rank
1     0 380 3.61    3
2     1 660 3.67    3
3     1 800 4.00    1
4     1 640 3.19    4
5     0 520 2.93    4
6     1 760 3.00    2</code></pre>
<p>Data and analysis courtesy of <a href="http://www.ats.ucla.edu/stat/r/dae/logit.htm" class="uri">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>.</p>
</section><section id="explore-the-data" class="slide level2">
<h2>Explore the Data</h2>
<pre class="r"><code>&gt; apply(mydata, 2, mean)
   admit      gre      gpa     rank 
  0.3175 587.7000   3.3899   2.4850 
&gt; apply(mydata, 2, sd)
      admit         gre         gpa        rank 
  0.4660867 115.5165364   0.3805668   0.9444602 
&gt; 
&gt; table(mydata$admit, mydata$rank)
   
     1  2  3  4
  0 28 97 93 55
  1 33 54 28 12</code></pre>
</section><section class="slide level2">

<pre class="r"><code>&gt; ggplot(data=mydata) +
+   geom_boxplot(aes(x=as.factor(admit), y=gre))</code></pre>
<p><img src="week10_files/figure-revealjs/unnamed-chunk-4-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section class="slide level2">

<pre class="r"><code>&gt; ggplot(data=mydata) +
+   geom_boxplot(aes(x=as.factor(admit), y=gpa))</code></pre>
<p><img src="week10_files/figure-revealjs/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="logistic-regression-in-r" class="slide level2">
<h2>Logistic Regression in R</h2>
<pre class="r"><code>&gt; mydata$rank &lt;- factor(mydata$rank, levels=c(1, 2, 3, 4))
&gt; myfit &lt;- glm(admit ~ gre + gpa + rank, 
+              data = mydata, family = &quot;binomial&quot;)
&gt; myfit

Call:  glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, 
    data = mydata)

Coefficients:
(Intercept)          gre          gpa        rank2  
  -3.989979     0.002264     0.804038    -0.675443  
      rank3        rank4  
  -1.340204    -1.551464  

Degrees of Freedom: 399 Total (i.e. Null);  394 Residual
Null Deviance:      500 
Residual Deviance: 458.5    AIC: 470.5</code></pre>
</section><section id="summary-of-fit" class="slide level2">
<h2>Summary of Fit</h2>
<pre class="r"><code>&gt; summary(myfit)

Call:
glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6268  -0.8662  -0.6388   1.1490   2.0790  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *  
gpa          0.804038   0.331819   2.423 0.015388 *  
rank2       -0.675443   0.316490  -2.134 0.032829 *  
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4</code></pre>
</section><section id="anova-of-fit" class="slide level2">
<h2>ANOVA of Fit</h2>
<pre class="r"><code>&gt; anova(myfit, test=&quot;Chisq&quot;)
Analysis of Deviance Table

Model: binomial, link: logit

Response: admit

Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                   399     499.98              
gre   1  13.9204       398     486.06 0.0001907 ***
gpa   1   5.7122       397     480.34 0.0168478 *  
rank  3  21.8265       394     458.52 7.088e-05 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&gt; anova(myfit, test=&quot;LRT&quot;)
Analysis of Deviance Table

Model: binomial, link: logit

Response: admit

Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                   399     499.98              
gre   1  13.9204       398     486.06 0.0001907 ***
gpa   1   5.7122       397     480.34 0.0168478 *  
rank  3  21.8265       394     458.52 7.088e-05 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</section><section id="example-contraceptive-use" class="slide level2">
<h2>Example: Contraceptive Use</h2>
<pre class="r"><code>&gt; cuse &lt;- 
+   read.table(&quot;http://data.princeton.edu/wws509/datasets/cuse.dat&quot;, 
+              header=TRUE)
&gt; dim(cuse)
[1] 16  5
&gt; head(cuse)
    age education wantsMore notUsing using
1   &lt;25       low       yes       53     6
2   &lt;25       low        no       10     4
3   &lt;25      high       yes      212    52
4   &lt;25      high        no       50    10
5 25-29       low       yes       60    14
6 25-29       low        no       19    10</code></pre>
<p>Data and analysis courtesy of <a href="http://data.princeton.edu/R/glms.html" class="uri">http://data.princeton.edu/R/glms.html</a>.</p>
</section><section id="a-different-format" class="slide level2">
<h2>A Different Format</h2>
<p>Note that in this data set there are multiple observations per explanatory variable configuration.</p>
<p>The last two columns of the data frame count the successes and failures per configuration.</p>
<pre class="r"><code>&gt; head(cuse)
    age education wantsMore notUsing using
1   &lt;25       low       yes       53     6
2   &lt;25       low        no       10     4
3   &lt;25      high       yes      212    52
4   &lt;25      high        no       50    10
5 25-29       low       yes       60    14
6 25-29       low        no       19    10</code></pre>
</section><section id="fitting-the-model" class="slide level2">
<h2>Fitting the Model</h2>
<p>When this is the case, we call the <code>glm()</code> function slighlty differently.</p>
<pre class="r"><code>&gt; myfit &lt;- glm(cbind(using, notUsing) ~ age + education + wantsMore, 
+              data=cuse, family = binomial)
&gt; myfit

Call:  glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, 
    family = binomial, data = cuse)

Coefficients:
 (Intercept)      age25-29      age30-39      age40-49  
     -0.8082        0.3894        0.9086        1.1892  
educationlow  wantsMoreyes  
     -0.3250       -0.8330  

Degrees of Freedom: 15 Total (i.e. Null);  10 Residual
Null Deviance:      165.8 
Residual Deviance: 29.92    AIC: 113.4</code></pre>
</section><section id="summary-of-fit-1" class="slide level2">
<h2>Summary of Fit</h2>
<pre class="r"><code>&gt; summary(myfit)

Call:
glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, 
    family = binomial, data = cuse)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.5148  -0.9376   0.2408   0.9822   1.7333  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.8082     0.1590  -5.083 3.71e-07 ***
age25-29       0.3894     0.1759   2.214  0.02681 *  
age30-39       0.9086     0.1646   5.519 3.40e-08 ***
age40-49       1.1892     0.2144   5.546 2.92e-08 ***
educationlow  -0.3250     0.1240  -2.620  0.00879 ** 
wantsMoreyes  -0.8330     0.1175  -7.091 1.33e-12 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 165.772  on 15  degrees of freedom
Residual deviance:  29.917  on 10  degrees of freedom
AIC: 113.43

Number of Fisher Scoring iterations: 4</code></pre>
</section><section id="anova-of-fit-1" class="slide level2">
<h2>ANOVA of Fit</h2>
<pre class="r"><code>&gt; anova(myfit)
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(using, notUsing)

Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev
NULL                         15    165.772
age        3   79.192        12     86.581
education  1    6.162        11     80.418
wantsMore  1   50.501        10     29.917</code></pre>
</section><section id="more-on-this-data-set" class="slide level2">
<h2>More on this Data Set</h2>
<p>See <a href="http://data.princeton.edu/R/glms.html" class="uri">http://data.princeton.edu/R/glms.html</a> for more on fitting logistic regression to this data set.</p>
<p>A number of interesting choices are made that reveal more about the data.</p>
</section></section>
<section><section id="generalized-linear-models" class="titleslide slide level1"><h1>Generalized Linear Models</h1></section><section id="definition" class="slide level2">
<h2>Definition</h2>
<p>The <strong>generalized linear model</strong> (GLM) builds from OLS and GLS to allow for the case where <span class="math inline">\(Y | {\boldsymbol{X}}\)</span> is distributed according to an exponential family distribution. The estimated model is</p>
<p><span class="math display">\[
g\left({\operatorname{E}}[Y | {\boldsymbol{X}}]\right) = {\boldsymbol{X}}{\boldsymbol{\beta}}\]</span></p>
<p>where <span class="math inline">\(g(\cdot)\)</span> is called the <strong>link function</strong>. This model is typically fit by numerical methods to calculate the maximum likelihood estimate of <span class="math inline">\({\boldsymbol{\beta}}\)</span>.</p>
</section><section id="exponential-family-distributions" class="slide level2">
<h2>Exponential Family Distributions</h2>
<p>Recall that if <span class="math inline">\(Y\)</span> follows an EFD then it has pdf of the form</p>
<p><span class="math display">\[f(y ; \boldsymbol{\theta}) =
h(y) \exp \left\{ \sum_{k=1}^d \eta_k(\boldsymbol{\theta}) T_k(y) - A(\boldsymbol{\eta}) \right\}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\theta}\)</span> is a vector of parameters, <span class="math inline">\(\{T_k(y)\}\)</span> are sufficient statistics, <span class="math inline">\(A(\boldsymbol{\eta})\)</span> is the cumulant generating function.</p>
</section><section class="slide level2">

<p>The functions <span class="math inline">\(\eta_k(\boldsymbol{\theta})\)</span> for <span class="math inline">\(k=1, \ldots, d\)</span> map the usual parameters <span class="math inline">\({\boldsymbol{\theta}}\)</span> (often moments of the rv <span class="math inline">\(Y\)</span>) to the <em>natural parameters</em> or <em>canonical parameters</em>.</p>
<p><span class="math inline">\(\{T_k(y)\}\)</span> are sufficient statistics for <span class="math inline">\(\{\eta_k\}\)</span> due to the factorization theorem.</p>
<p><span class="math inline">\(A(\boldsymbol{\eta})\)</span> is sometimes called the <em>log normalizer</em> because</p>
<p><span class="math display">\[A(\boldsymbol{\eta}) = \log \int h(y) \exp \left\{ \sum_{k=1}^d \eta_k(\boldsymbol{\theta}) T_k(y) \right\}.\]</span></p>
</section><section id="natural-single-parameter-efd" class="slide level2">
<h2>Natural Single Parameter EFD</h2>
<p>A natural single parameter EFD simplifies to the scenario where <span class="math inline">\(d=1\)</span> and <span class="math inline">\(T(y) = y\)</span></p>
<p><span class="math display">\[f(y ; \eta) =
h(y) \exp \left\{ \eta(\theta) y - A(\eta(\theta)) \right\}
\]</span></p>
<p>where without loss of generality we can write <span class="math inline">\({\operatorname{E}}[Y] = \theta\)</span>.</p>
</section><section id="dispersion-efds" class="slide level2">
<h2>Dispersion EFDs</h2>
<p>The family of distributions for which GLMs are most typically developed are dispersion EFDs. An example of a dispersion EFD that extends the natural single parameter EFD is</p>
<p><span class="math display">\[f(y ; \eta) =
h(y, \phi) \exp \left\{ \frac{\eta(\theta) y - A(\eta(\theta))}{\phi} \right\}
\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> is the dispersion parameter.</p>
</section><section id="example-normal" class="slide level2">
<h2>Example: Normal</h2>
<p>Let <span class="math inline">\(Y \sim \mbox{Normal}(\mu, \sigma^2)\)</span>. Then:</p>
<p><span class="math display">\[
\theta = \mu, \eta(\mu) = \mu
\]</span></p>
<p><span class="math display">\[
\phi = \sigma^2
\]</span></p>
<p><span class="math display">\[
A(\mu) = \frac{\mu^2}{2}
\]</span></p>
<p><span class="math display">\[
h(y, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2}\frac{y^2}{\sigma^2}}
\]</span></p>
</section><section id="efd-for-glms" class="slide level2">
<h2>EFD for GLMs</h2>
<p>There has been a very broad development of GLMs and extensions. A common setting for introducting GLMs is the dispersion EFD with a general link function <span class="math inline">\(g(\cdot)\)</span>.</p>
<p>See the classic text <em>Generalized Linear Models</em>, by McCullagh and Nelder, for such a development.</p>
</section><section id="components-of-a-glm" class="slide level2">
<h2>Components of a GLM</h2>
<ol type="1">
<li><p><em>Random</em>: The particular exponential family distribution. <span class="math display">\[
Y \sim f(y ; \eta, \phi)
\]</span></p></li>
<li><p><em>Systematic</em>: The determination of each <span class="math inline">\(\eta_i\)</span> from <span class="math inline">\({\boldsymbol{X}}_i\)</span> and <span class="math inline">\({\boldsymbol{\beta}}\)</span>. <span class="math display">\[
\eta_i = {\boldsymbol{X}}_i {\boldsymbol{\beta}}\]</span></p></li>
<li><p><em>Parametric Link</em>: The connection between <span class="math inline">\(E[Y_i|{\boldsymbol{X}}_i]\)</span> and <span class="math inline">\({\boldsymbol{X}}_i {\boldsymbol{\beta}}\)</span>. <span class="math display">\[
g(E[Y_i|{\boldsymbol{X}}_i]) = {\boldsymbol{X}}_i {\boldsymbol{\beta}}\]</span></p></li>
</ol>
</section><section id="link-functions" class="slide level2">
<h2>Link Functions</h2>
<p>Even though the link function <span class="math inline">\(g(\cdot)\)</span> can be considered in a fairly general framework, the <strong>canonical link function</strong> <span class="math inline">\(\eta(\cdot)\)</span> is often utilized.</p>
<p>The canonical link function is the function that maps the expected value into the natural paramater.</p>
<p>In this case, <span class="math inline">\(Y | {\boldsymbol{X}}\)</span> is distributed according to an exponential family distribution with</p>
<p><span class="math display">\[
\eta \left({\operatorname{E}}[Y | {\boldsymbol{X}}]\right) = {\boldsymbol{X}}{\boldsymbol{\beta}}.
\]</span></p>
</section><section id="calculating-mles" class="slide level2">
<h2>Calculating MLEs</h2>
<p>Given the model <span class="math inline">\(g\left({\operatorname{E}}[Y | {\boldsymbol{X}}]\right) = {\boldsymbol{X}}{\boldsymbol{\beta}}\)</span>, the EFD should be fully parameterized. The Newton-Raphson method or Fisher’s scoring method can be utilized to find the MLE of <span class="math inline">\({\boldsymbol{\beta}}\)</span>.</p>
</section><section class="slide level2">

<h3 id="newton-raphson">Newton-Raphson</h3>
<ol type="1">
<li><p>Initialize <span class="math inline">\({\boldsymbol{\beta}}^{(0)}\)</span>. For <span class="math inline">\(t = 1, 2, \ldots\)</span></p></li>
<li><p>Calculate the score <span class="math inline">\(s({\boldsymbol{\beta}}^{(t)}) = \nabla \ell({\boldsymbol{\beta}}; {\boldsymbol{X}}, {\boldsymbol{Y}}) \mid_{{\boldsymbol{\beta}}= {\boldsymbol{\beta}}^{(t)}}\)</span> and observed Fisher information <span class="math display">\[H({\boldsymbol{\beta}}^{(t)}) = - \nabla \nabla^T \ell({\boldsymbol{\beta}}; {\boldsymbol{X}}, {\boldsymbol{Y}}) \mid_{{\boldsymbol{\beta}}= {\boldsymbol{\beta}}^{(t)}}\]</span>. Note that the observed Fisher information is also the negative Hessian matrix.</p></li>
<li><p>Update <span class="math inline">\({\boldsymbol{\beta}}^{(t+1)} = {\boldsymbol{\beta}}^{(t)} + H({\boldsymbol{\beta}}^{(t)})^{-1} s({\boldsymbol{\beta}}^{(t)})\)</span>.</p></li>
<li><p>Iterate until convergence, and set <span class="math inline">\(\hat{{\boldsymbol{\beta}}} = {\boldsymbol{\beta}}^{(\infty)}\)</span>.</p></li>
</ol>
</section><section class="slide level2">

<h3 id="fishers-scoring">Fisher’s scoring</h3>
<ol type="1">
<li><p>Initialize <span class="math inline">\({\boldsymbol{\beta}}^{(0)}\)</span>. For <span class="math inline">\(t = 1, 2, \ldots\)</span></p></li>
<li><p>Calculate the score <span class="math inline">\(s({\boldsymbol{\beta}}^{(t)}) = \nabla \ell({\boldsymbol{\beta}}; {\boldsymbol{X}}, {\boldsymbol{Y}}) \mid_{{\boldsymbol{\beta}}= {\boldsymbol{\beta}}^{(t)}}\)</span> and expected Fisher information <span class="math display">\[I({\boldsymbol{\beta}}^{(t)}) = - {\operatorname{E}}\left[\nabla \nabla^T \ell({\boldsymbol{\beta}}; {\boldsymbol{X}}, {\boldsymbol{Y}}) \mid_{{\boldsymbol{\beta}}= {\boldsymbol{\beta}}^{(t)}} \right].\]</span></p></li>
<li><p>Update <span class="math inline">\({\boldsymbol{\beta}}^{(t+1)} = {\boldsymbol{\beta}}^{(t)} + I({\boldsymbol{\beta}}^{(t)})^{-1} s({\boldsymbol{\beta}}^{(t)})\)</span>.</p></li>
<li><p>Iterate until convergence, and set <span class="math inline">\(\hat{{\boldsymbol{\beta}}} = {\boldsymbol{\beta}}^{(\infty)}\)</span>.</p></li>
</ol>
</section><section class="slide level2">

<p>When the canonical link function is used, the Newton-Raphson algorithm and Fisher’s scoring algorithm are equivalent.</p>
<p>Exercise: Prove this.</p>
</section><section id="iteratively-reweighted-least-squares-1" class="slide level2">
<h2>Iteratively Reweighted Least Squares</h2>
<p>For the canonical link, Fisher’s scoring method can be written as an iteratively reweighted least squares algorithm, as shown earlier for logistic regression. Note that the Fisher information is</p>
<p><span class="math display">\[
I({\boldsymbol{\beta}}^{(t)}) = {\boldsymbol{X}}^T {\boldsymbol{W}}{\boldsymbol{X}}\]</span></p>
<p>where <span class="math inline">\({\boldsymbol{W}}\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix with <span class="math inline">\((i, i)\)</span> entry equal to <span class="math inline">\({\operatorname{Var}}(Y_i | {\boldsymbol{X}}; {\boldsymbol{\beta}}^{(t)})\)</span>.</p>
</section><section class="slide level2">

<p>The score function is</p>
<p><span class="math display">\[
s({\boldsymbol{\beta}}^{(t)}) = {\boldsymbol{X}}^T \left( {\boldsymbol{Y}}- {\boldsymbol{X}}{\boldsymbol{\beta}}^{(t)} \right)
\]</span></p>
<p>and the current coefficient value <span class="math inline">\({\boldsymbol{\beta}}^{(t)}\)</span> can be written as</p>
<p><span class="math display">\[
{\boldsymbol{\beta}}^{(t)} = ({\boldsymbol{X}}^T {\boldsymbol{W}}{\boldsymbol{X}})^{-1} {\boldsymbol{X}}^T {\boldsymbol{W}}{\boldsymbol{X}}{\boldsymbol{\beta}}^{(t)}.
\]</span></p>
</section><section class="slide level2">

<p>Putting this together we get</p>
<p><span class="math display">\[
{\boldsymbol{\beta}}^{(t)} + I({\boldsymbol{\beta}}^{(t)})^{-1} s({\boldsymbol{\beta}}^{(t)}) = ({\boldsymbol{X}}^T {\boldsymbol{W}}{\boldsymbol{X}})^{-1} {\boldsymbol{X}}^T {\boldsymbol{W}}{\boldsymbol{z}}^{(t)}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
{\boldsymbol{z}}^{(t)} = {\boldsymbol{X}}{\boldsymbol{\beta}}^{(t)} + {\boldsymbol{W}}^{-1} \left( {\boldsymbol{Y}}- {\boldsymbol{X}}{\boldsymbol{\beta}}^{(t)} \right).
\]</span></p>
<p>This is a generalization of the iteratively reweighted least squares algorithm we showed earlier for logistic regression.</p>
</section><section id="estimating-dispersion" class="slide level2">
<h2>Estimating Dispersion</h2>
<p>For the simple dispersion model above, it is typically straightforward to calculate the MLE <span class="math inline">\(\hat{\phi}\)</span> once <span class="math inline">\(\hat{{\boldsymbol{\beta}}}\)</span> has been calculated.</p>
</section><section id="clt-applied-to-the-mle" class="slide level2">
<h2>CLT Applied to the MLE</h2>
<p>Given that <span class="math inline">\(\hat{{\boldsymbol{\beta}}}\)</span> is a maximum likelihood estimate, we have the following CLT result on its distribution as <span class="math inline">\(n \rightarrow \infty\)</span>:</p>
<p><span class="math display">\[
\sqrt{n} (\hat{{\boldsymbol{\beta}}} - {\boldsymbol{\beta}}) \stackrel{D}{\longrightarrow} \mbox{MVN}_{p}({\boldsymbol{0}}, \phi ({\boldsymbol{X}}^T {\boldsymbol{W}}{\boldsymbol{X}})^{-1})
\]</span></p>
</section><section id="approximately-pivotal-statistics" class="slide level2">
<h2>Approximately Pivotal Statistics</h2>
<p>The previous CLT gives us the following two approximations for pivtoal statistics. The first statistic facilitates getting overall measures of uncertainty on the estimate <span class="math inline">\(\hat{{\boldsymbol{\beta}}}\)</span>.</p>
<p><span class="math display">\[
\hat{\phi}^{-1} (\hat{{\boldsymbol{\beta}}} - {\boldsymbol{\beta}})^T ({\boldsymbol{X}}^T \hat{{\boldsymbol{W}}} {\boldsymbol{X}}) (\hat{{\boldsymbol{\beta}}} - {\boldsymbol{\beta}}) {\; \stackrel{.}{\sim}\;}\chi^2_1
\]</span></p>
<p>This second pivotal statistic allows for performing a Wald test or forming a confidence interval on each coefficient, <span class="math inline">\(\beta_j\)</span>, for <span class="math inline">\(j=1, \ldots, p\)</span>.</p>
<p><span class="math display">\[
\frac{\hat{\beta}_j - \beta_j}{\sqrt{\hat{\phi} [({\boldsymbol{X}}^T \hat{{\boldsymbol{W}}} {\boldsymbol{X}})^{-1}]_{jj}}} {\; \stackrel{.}{\sim}\;}\mbox{Normal}(0,1)
\]</span></p>
</section><section id="deviance" class="slide level2">
<h2>Deviance</h2>
<p>Let <span class="math inline">\(\hat{\boldsymbol{\eta}}\)</span> be the estimated natural parameters from a GLM. For example, <span class="math inline">\(\hat{\boldsymbol{\eta}} = {\boldsymbol{X}}\hat{{\boldsymbol{\beta}}}\)</span> when the canonical link function is used.</p>
<p>Let <span class="math inline">\(\hat{\boldsymbol{\eta}}_n\)</span> be the <strong>saturated model</strong> wwhere <span class="math inline">\(Y_i\)</span> is directly used to estimate <span class="math inline">\(\eta_i\)</span> without model constraints. For example, in the Bernoulli logistic regression model <span class="math inline">\(\hat{\boldsymbol{\eta}}_n = {\boldsymbol{Y}}\)</span>, the observed outcomes.</p>
<p>The <strong>deviance</strong> for the model is defined to be</p>
<p><span class="math display">\[
D\left(\hat{\boldsymbol{\eta}}\right) = 2 \ell(\hat{\boldsymbol{\eta}}_n; {\boldsymbol{X}}, {\boldsymbol{Y}}) - 2 \ell(\hat{\boldsymbol{\eta}}; {\boldsymbol{X}}, {\boldsymbol{Y}})
\]</span></p>
</section><section id="generalized-lrt" class="slide level2">
<h2>Generalized LRT</h2>
<p>Let <span class="math inline">\({\boldsymbol{X}}_0\)</span> be a subset of <span class="math inline">\(p_0\)</span> columns of <span class="math inline">\({\boldsymbol{X}}\)</span> and let <span class="math inline">\({\boldsymbol{X}}_1\)</span> be a subset of <span class="math inline">\(p_1\)</span> columns, where <span class="math inline">\(1 \leq p_0 &lt; p_1 \leq p\)</span>. Also, assume that the columns of <span class="math inline">\({\boldsymbol{X}}_0\)</span> are a subset of <span class="math inline">\({\boldsymbol{X}}_1\)</span>.</p>
<p>Without loss of generality, suppose that <span class="math inline">\({\boldsymbol{\beta}}_0 = (\beta_1, \beta_2, \ldots, \beta_{p_0})^T\)</span> and <span class="math inline">\({\boldsymbol{\beta}}_1 = (\beta_1, \beta_2, \ldots, \beta_{p_1})^T\)</span>.</p>
<p>Suppose we wish to test <span class="math inline">\(H_0: (\beta_{p_0+1}, \beta_{p_0 + 2}, \ldots, \beta_{p_1}) = {\boldsymbol{0}}\)</span> vs <span class="math inline">\(H_1: (\beta_{p_0+1}, \beta_{p_0 + 2}, \ldots, \beta_{p_1}) \not= {\boldsymbol{0}}\)</span></p>
</section><section class="slide level2">

<p>We can form <span class="math inline">\(\hat{\boldsymbol{\eta}}_0 = {\boldsymbol{X}}\hat{{\boldsymbol{\beta}}}_0\)</span> from the GLM model <span class="math inline">\(g\left({\operatorname{E}}[Y | {\boldsymbol{X}}_0]\right) = {\boldsymbol{X}}_0 {\boldsymbol{\beta}}_0\)</span>. We can analogously form <span class="math inline">\(\hat{\boldsymbol{\eta}}_1 = {\boldsymbol{X}}\hat{{\boldsymbol{\beta}}}_1\)</span> from the GLM model <span class="math inline">\(g\left({\operatorname{E}}[Y | {\boldsymbol{X}}_1]\right) = {\boldsymbol{X}}_1 {\boldsymbol{\beta}}_1\)</span>.</p>
<p>The <span class="math inline">\(2\log\)</span> generalized LRT can then be formed from the two deviance statistics</p>
<p><span class="math display">\[
2 \log \lambda({\boldsymbol{X}}, {\boldsymbol{Y}}) = 2 \log \frac{L(\hat{\boldsymbol{\eta}}_1; {\boldsymbol{X}}, {\boldsymbol{Y}})}{L(\hat{\boldsymbol{\eta}}_0; {\boldsymbol{X}}, {\boldsymbol{Y}})} = D\left(\hat{\boldsymbol{\eta}}_0\right) - D\left(\hat{\boldsymbol{\eta}}_1\right)
\]</span></p>
<p>where the null distribution is <span class="math inline">\(\chi^2_{p_1-p_0}\)</span>.</p>
</section><section id="example-grad-school-admissions-1" class="slide level2">
<h2>Example: Grad School Admissions</h2>
<p>Let’s revisit a logistic regression example now that we know how the various statistics are calculated.</p>
<pre class="r"><code>&gt; mydata &lt;- 
+   read.csv(&quot;http://www.ats.ucla.edu/stat/data/binary.csv&quot;)
&gt; dim(mydata)
&gt; head(mydata)</code></pre>
</section><section class="slide level2">

<p>Fit the model with basic output. Note the argument <code>family = &quot;binomial&quot;</code>.</p>
<pre class="r"><code>&gt; mydata$rank &lt;- factor(mydata$rank, levels=c(1, 2, 3, 4))
&gt; myfit &lt;- glm(admit ~ gre + gpa + rank, 
+              data = mydata, family = &quot;binomial&quot;)
&gt; myfit

Call:  glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, 
    data = mydata)

Coefficients:
(Intercept)          gre          gpa        rank2  
  -3.989979     0.002264     0.804038    -0.675443  
      rank3        rank4  
  -1.340204    -1.551464  

Degrees of Freedom: 399 Total (i.e. Null);  394 Residual
Null Deviance:      500 
Residual Deviance: 458.5    AIC: 470.5</code></pre>
</section><section class="slide level2">

<p>This shows the fitted coefficient values, which is on the link function scale – logit aka log odds here. Also, a Wald test is performed for each coefficient.</p>
<pre class="r"><code>&gt; summary(myfit)

Call:
glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6268  -0.8662  -0.6388   1.1490   2.0790  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *  
gpa          0.804038   0.331819   2.423 0.015388 *  
rank2       -0.675443   0.316490  -2.134 0.032829 *  
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4</code></pre>
</section><section class="slide level2">

<p>Here we perform a generalized LRT on each variable. Note the <code>rank</code> variable is now tested as a single factor variable as opposed to each dummy variable.</p>
<pre class="r"><code>&gt; anova(myfit, test=&quot;LRT&quot;)
Analysis of Deviance Table

Model: binomial, link: logit

Response: admit

Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                   399     499.98              
gre   1  13.9204       398     486.06 0.0001907 ***
gpa   1   5.7122       397     480.34 0.0168478 *  
rank  3  21.8265       394     458.52 7.088e-05 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</section><section class="slide level2">

<pre class="r"><code>&gt; mydata &lt;- data.frame(mydata, probs = myfit$fitted.values)
&gt; ggplot(mydata) + geom_point(aes(x=gpa, y=probs, color=rank)) +
+   geom_jitter(aes(x=gpa, y=admit), width=0, height=0.01, alpha=0.3)</code></pre>
<p><img src="week10_files/figure-revealjs/unnamed-chunk-17-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section class="slide level2">

<pre class="r"><code>&gt; ggplot(mydata) + geom_point(aes(x=gre, y=probs, color=rank)) +
+   geom_jitter(aes(x=gre, y=admit), width=0, height=0.01, alpha=0.3)</code></pre>
<p><img src="week10_files/figure-revealjs/unnamed-chunk-18-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section class="slide level2">

<pre class="r"><code>&gt; ggplot(mydata) + geom_boxplot(aes(x=rank, y=probs)) +
+   geom_jitter(aes(x=rank, y=probs), width=0.1, height=0.01, alpha=0.3)</code></pre>
<p><img src="week10_files/figure-revealjs/unnamed-chunk-19-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="glm-function" class="slide level2">
<h2><code>glm()</code> Function</h2>
<p>The <code>glm()</code> function has many different options available to the user.</p>
<pre><code>glm(formula, family = gaussian, data, weights, subset,
    na.action, start = NULL, etastart, mustart, offset,
    control = list(...), model = TRUE, method = &quot;glm.fit&quot;,
    x = FALSE, y = TRUE, contrasts = NULL, ...)</code></pre>
<p>To see the different link functions available, type:</p>
<pre><code>help(family)</code></pre>
</section></section>
<section><section id="nonparametric-regression" class="titleslide slide level1"><h1>Nonparametric Regression</h1></section><section id="simple-linear-regression" class="slide level2">
<h2>Simple Linear Regression</h2>
<p>Recall the set up for simple linear regression. For random variables <span class="math inline">\((X_1, Y_1), (X_2, Y_2), \ldots, (X_n, Y_n)\)</span>, <strong>simple linear regression</strong> estimates the model</p>
<p><span class="math display">\[
Y_i  = \beta_1 + \beta_2 X_i + E_i
\]</span></p>
<p>where <span class="math inline">\({\operatorname{E}}[E_i | X_i] = 0\)</span>, <span class="math inline">\({\operatorname{Var}}(E_i | X_i) = \sigma^2\)</span>, and <span class="math inline">\({\operatorname{Cov}}(E_i, E_j | X_i, X_j) = 0\)</span> for all <span class="math inline">\(1 \leq i, j \leq n\)</span> and <span class="math inline">\(i \not= j\)</span>.</p>
<p>Note that in this model <span class="math inline">\({\operatorname{E}}[Y | X] = \beta_1 + \beta_2 X.\)</span></p>
</section><section id="simple-nonparametric-regression" class="slide level2">
<h2>Simple Nonparametric Regression</h2>
<p>In <strong>simple nonparametric regression</strong>, we define a similar model while eliminating the linear assumption:</p>
<p><span class="math display">\[
Y_i  = s(X_i) + E_i
\]</span></p>
<p>for some function <span class="math inline">\(s(\cdot)\)</span> with the same assumptions on the distribution of <span class="math inline">\({\boldsymbol{E}}| {\boldsymbol{X}}\)</span>. In this model, we also have</p>
<p><span class="math display">\[
{\operatorname{E}}[Y | X] = s(X).
\]</span></p>
</section><section id="smooth-functions" class="slide level2">
<h2>Smooth Functions</h2>
<p>Suppose we consider fitting the model <span class="math inline">\(Y_i = s(X_i) + E_i\)</span> with the restriction that <span class="math inline">\(s \in C^2\)</span>, the class of functions with continuous second derivatives. We can set up an objective function that regularizes how smooth vs wiggly <span class="math inline">\(s\)</span> is.</p>
<p>Specifically, suppose for a given set of observed data <span class="math inline">\((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)</span> we wish to identify a function <span class="math inline">\(s \in C^2\)</span> that minimizes for some <span class="math inline">\(\lambda\)</span></p>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - s(x_i))^2 + \lambda \int |s&#39;&#39;(x)|^2 dx
\]</span></p>
</section><section id="smoothness-parameter-lambda" class="slide level2">
<h2>Smoothness Parameter <span class="math inline">\(\lambda\)</span></h2>
<p>When minimizing</p>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - s(x_i))^2 + \lambda \int |s^{\prime\prime}(x)|^2 dx
\]</span></p>
<p>it follows that if <span class="math inline">\(\lambda=0\)</span> then any function <span class="math inline">\(s \in C^2\)</span> that interpolates the data is a solution.</p>
<p>As <span class="math inline">\(\lambda \rightarrow \infty\)</span>, then the minimizing function is the simple linear regression solution.</p>
</section><section id="the-solution" class="slide level2">
<h2>The Solution</h2>
<p>For an observed data set <span class="math inline">\((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)</span> where <span class="math inline">\(n \geq 4\)</span> and a fixed value <span class="math inline">\(\lambda\)</span>, there is an exact solution to minimizing</p>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - s(x_i))^2 + \lambda \int |s^{\prime\prime}(x)|^2 dx.
\]</span></p>
<p>The solution is called a <strong>natural cubic spline</strong>, which is constructed to have knots at <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>.</p>
</section><section id="natural-cubic-splines" class="slide level2">
<h2>Natural Cubic Splines</h2>
<p>Suppose without loss of generality that we have ordered <span class="math inline">\(x_1 &lt; x_2 &lt; \cdots &lt; x_n\)</span>. We assume all <span class="math inline">\(x_i\)</span> are unique to simplify the explanation here, but ties can be deal with.</p>
<p>A <strong>natural cubic spline</strong> (NCS) is a function constructed from a set of piecewise cubic functions over the range <span class="math inline">\([x_1, x_n]\)</span> joined at the knots so that the second derivative is continuous at the knots. Outside of the range (<span class="math inline">\(&lt; x_1\)</span> or <span class="math inline">\(&gt; x_n\)</span>), the spline is linear and it has continuous second derivatives at the endpoint knots.</p>
</section><section id="basis-functions" class="slide level2">
<h2>Basis Functions</h2>
<p>Depending on the value <span class="math inline">\(\lambda\)</span>, a different ncs will be constructed, but the entire family of ncs solutions over <span class="math inline">\(0 &lt; \lambda &lt; \infty\)</span> can be constructed from a common set of basis functions.</p>
<p>We construct <span class="math inline">\(n\)</span> basis functions <span class="math inline">\(N_1(x), N_2(x), \ldots, N_n(x)\)</span> with coefficients <span class="math inline">\(\theta_1(\lambda), \theta_2(\lambda), \ldots, \theta_n(\lambda)\)</span>. The NCS takes the form</p>
<p><span class="math display">\[
s(x) = \sum_{i=1}^n \theta_i(\lambda) N_i(x).
\]</span></p>
</section><section class="slide level2">

<p>Define <span class="math inline">\(N_1(x) = 1\)</span> and <span class="math inline">\(N_2(x) = x\)</span>. For <span class="math inline">\(i = 3, \ldots, n\)</span>, define <span class="math inline">\(N_i(x) = d_{i-1}(x) - d_{i-2}(x)\)</span> where</p>
<p><span class="math display">\[
d_{i}(x) = \frac{(x - x_i)^3 - (x - x_n)^3}{x_n - x_i}.
\]</span></p>
<p>Recall that we’ve labeled indices so that <span class="math inline">\(x_1 &lt; x_2 &lt; \cdots &lt; x_n\)</span>.</p>
</section><section id="calculating-the-solution" class="slide level2">
<h2>Calculating the Solution</h2>
<p>Let <span class="math inline">\({\boldsymbol{\theta}}_\lambda = (\theta_1(\lambda), \theta_2(\lambda), \ldots, \theta_n(\lambda))^T\)</span> and let <span class="math inline">\({\boldsymbol{N}}\)</span> be the <span class="math inline">\(n \times n\)</span> matrix with <span class="math inline">\((i, j)\)</span> entry equal to <span class="math inline">\(N_j(x_i)\)</span>. Finally, let <span class="math inline">\({\boldsymbol{\Omega}}\)</span> be the <span class="math inline">\(n \times n\)</span> matrix with <span class="math inline">\((i,j)\)</span> entry equal to <span class="math inline">\(\int N_i^{\prime\prime}(x) N_j^{\prime\prime}(x) dx\)</span>.</p>
<p>The solution to <span class="math inline">\({\boldsymbol{\theta}}_\lambda\)</span> are the values that minimize</p>
<p><span class="math display">\[
({\boldsymbol{y}}- {\boldsymbol{N}}{\boldsymbol{\theta}})^T ({\boldsymbol{y}}- {\boldsymbol{N}}{\boldsymbol{\theta}}) + \lambda {\boldsymbol{\theta}}^T {\boldsymbol{\Omega}}{\boldsymbol{\theta}}.
\]</span></p>
<p>which results in</p>
<p><span class="math display">\[
\hat{{\boldsymbol{\theta}}}_\lambda = ({\boldsymbol{N}}^T {\boldsymbol{N}}+ \lambda {\boldsymbol{\Omega}})^{-1} {\boldsymbol{N}}^T {\boldsymbol{y}}.
\]</span></p>
</section><section id="linear-operator" class="slide level2">
<h2>Linear Operator</h2>
<p>Letting</p>
<p><span class="math display">\[
{\boldsymbol{S}}_\lambda = {\boldsymbol{N}}({\boldsymbol{N}}^T {\boldsymbol{N}}+ \lambda {\boldsymbol{\Omega}})^{-1} {\boldsymbol{N}}^T
\]</span></p>
<p>it folows that the fitted values are</p>
<p><span class="math display">\[
\hat{{\boldsymbol{y}}} = {\boldsymbol{S}}_\lambda {\boldsymbol{y}}.
\]</span></p>
<p>Thus, the fitted values from a NCS are contructed by taking linear combination of the response variable values <span class="math inline">\(y_1, y_2, \ldots, y_n\)</span>.</p>
</section><section id="degrees-of-freedom" class="slide level2">
<h2>Degrees of Freedom</h2>
<p>Recall that in OLS, we formed projection matrix <span class="math inline">\({\boldsymbol{P}}= {\boldsymbol{X}}({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} {\boldsymbol{X}}^T\)</span> and noted that the number of columns <span class="math inline">\(p\)</span> of <span class="math inline">\({\boldsymbol{X}}\)</span> is also found in the trace of <span class="math inline">\({\boldsymbol{P}}\)</span> where <span class="math inline">\(\operatorname{tr}({\boldsymbol{P}}) = p\)</span>.</p>
<p>The effective degrees of freedom for a model fit by a linear operator is calculated as the trace of the operator.</p>
<p>Therefore, for a given <span class="math inline">\(\lambda\)</span>, the effective degrees of freedom is</p>
<p><span class="math display">\[
\operatorname{df}_\lambda = \operatorname{tr}({\boldsymbol{S}}_\lambda).
\]</span></p>
</section><section id="bayesian-intepretation" class="slide level2">
<h2>Bayesian Intepretation</h2>
<p>Minimizing</p>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - s(x_i))^2 + \lambda \int |s^{\prime\prime}(x)|^2 dx
\]</span></p>
<p>is equivalent to maximizing</p>
<p><span class="math display">\[
\exp\left\{ -\frac{\sum_{i=1}^n (y_i - s(x_i))^2}{2\sigma^2} \right\} \exp\left\{-\frac{\lambda}{2\sigma^2} \int |s^{\prime\prime}(x)|^2 dx\right\}.
\]</span></p>
<p>Therefore, the NCS solution can be interpreted as calculting the MAP where <span class="math inline">\(Y | X\)</span> is Normal and there’s an Exponential prior on the smoothness of <span class="math inline">\(s\)</span>.</p>
</section><section id="bias-and-variance-trade-off" class="slide level2">
<h2>Bias and Variance Trade-off</h2>
<p>Typically we will choose some <span class="math inline">\(0 &lt; \lambda &lt; \infty\)</span> in an effort to balance the bias and variance. Let <span class="math inline">\(\hat{Y} = \hat{s}(X; \lambda)\)</span> where <span class="math inline">\(\hat{s}(\cdot; \lambda)\)</span> minimizes the above for some chosen <span class="math inline">\(\lambda\)</span> on an independent data set. Then</p>
<p><span class="math display">\[
\begin{aligned}
{\operatorname{E}}\left[\left(Y - \hat{Y}\right)^2\right] &amp; = {\rm E}\left[\left(s(x) + E - \hat{s}(x; \lambda)\right)^2 \right] \\
\ &amp; = {\rm E}\left[\left( s(x) - \hat{s}(x; \lambda) \right)^2 \right] + {\rm Var}(E) \\
\ &amp; = \left( s(x) - {\rm E}[\hat{s}(x; \lambda)]\right)^2 + {\rm Var}\left(\hat{s}(x; \lambda)\right) + {\rm Var}(E) \\ 
\ &amp; = \mbox{bias}^2_{\lambda} + \mbox{variance}_{\lambda} + {\rm Var}(E)
\end{aligned}
\]</span></p>
<p>where all of the above calculations are conditioned on <span class="math inline">\(X=x\)</span>.</p>
</section><section class="slide level2">

<p>In minimizing</p>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - s(x_i))^2 + \lambda \int |s^{\prime\prime}(x)|^2 dx
\]</span></p>
<p>the relationship is such that:</p>
<p><span class="math display">\[
\uparrow \lambda \Longrightarrow \mbox{bias}^2 \uparrow, \mbox{variance} \downarrow
\]</span></p>
<p><span class="math display">\[
\downarrow \lambda \Longrightarrow \mbox{bias}^2 \downarrow, \mbox{variance} \uparrow
\]</span></p>
</section><section id="choosing-lambda" class="slide level2">
<h2>Choosing <span class="math inline">\(\lambda\)</span></h2>
<p>There are several approaches that are commonly used to identify a value of <span class="math inline">\(\lambda\)</span>, including:</p>
<ul>
<li>Scientific knowledge that guides the acceptable value of <span class="math inline">\(\operatorname{df}_\lambda\)</span></li>
<li>Cross-validation or some other prediction quality measure</li>
<li>Model selection measures, such as Akaike information criterion (AIC) or Mallows <span class="math inline">\(C_p\)</span></li>
</ul>
</section><section id="smoothers-and-spline-models" class="slide level2">
<h2>Smoothers and Spline Models</h2>
<p>We investigated one type of nonparametric regression model here, the NCS. However, in general there are many such “smoother” methods available in the simple nonparametric regression scenario.</p>
<p>Splines are particularly popular since splines are constructed from putting together polynomials and are therefore usually tractable to compute and analyze.</p>
</section><section id="smoothers-in-r" class="slide level2">
<h2>Smoothers in R</h2>
<p>There are several functions and packages available in R for computing smoothers and tuning smoothness parameters. Examples include:</p>
<ul>
<li><code>splines</code> library</li>
<li><code>smooth.spline()</code></li>
<li><code>loess()</code></li>
<li><code>lowess()</code></li>
</ul>
</section><section id="example" class="slide level2">
<h2>Example</h2>
<p><img src="week10_files/figure-revealjs/unnamed-chunk-20-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section class="slide level2">

<pre class="r"><code>&gt; y2 &lt;- smooth.spline(x=x, y=y, df=2)
&gt; y5 &lt;- smooth.spline(x=x, y=y, df=5)
&gt; y25 &lt;- smooth.spline(x=x, y=y, df=25)
&gt; ycv &lt;- smooth.spline(x=x, y=y)
&gt; ycv
Call:
smooth.spline(x = x, y = y)

Smoothing Parameter  spar= 0.5162045  lambda= 0.0002730906 (11 iterations)
Equivalent Degrees of Freedom (Df): 7.293673
Penalized Criterion: 14.80602
GCV: 1.180651</code></pre>
</section><section class="slide level2">

<p><img src="week10_files/figure-revealjs/unnamed-chunk-22-1.png" width="576" style="display: block; margin: auto;" /></p>
</section></section>
<section><section id="generalized-additive-models" class="titleslide slide level1"><h1>Generalized Additive Models</h1></section><section id="ordinary-least-squares" class="slide level2">
<h2>Ordinary Least Squares</h2>
<p>Recall that OLS estimates the model</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp; = \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_p X_{ip} + E_i \\
 &amp; = {\boldsymbol{X}}_i {\boldsymbol{\beta}}+ E_i
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\({\operatorname{E}}[{\boldsymbol{E}}| {\boldsymbol{X}}] = {\boldsymbol{0}}\)</span> and <span class="math inline">\({\operatorname{Cov}}({\boldsymbol{E}}| {\boldsymbol{X}}) = \sigma^2 {\boldsymbol{I}}\)</span>.</p>
</section><section id="additive-models" class="slide level2">
<h2>Additive Models</h2>
<p>The <strong>additive model</strong> (which could also be called “ordinary nonparametric additive regression”) is of the form</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp; = s_1(X_{i1}) + s_2(X_{i2}) + \ldots + s_p(X_{ip}) + E_i \\
 &amp; = \sum_{j=1}^p s_j(X_{ij}) + E_i
\end{aligned}
\]</span></p>
<p>where the <span class="math inline">\(s_j(\cdot)\)</span> for <span class="math inline">\(j = 1, \ldots, p\)</span> are a set of nonparametric (or flexible) functions. Again, we assume that <span class="math inline">\({\operatorname{E}}[{\boldsymbol{E}}| {\boldsymbol{X}}] = {\boldsymbol{0}}\)</span> and <span class="math inline">\({\operatorname{Cov}}({\boldsymbol{E}}| {\boldsymbol{X}}) = \sigma^2 {\boldsymbol{I}}\)</span>.</p>
</section><section id="backfitting" class="slide level2">
<h2>Backfitting</h2>
<p>The additive model can be fit through a technique called <strong>backfitting</strong>.</p>
<ol type="1">
<li>Intialize <span class="math inline">\(s^{(0)}_j(x)\)</span> for <span class="math inline">\(j = 1, \ldots, p\)</span>.</li>
<li>For <span class="math inline">\(t=1, 2, \ldots\)</span>, fit <span class="math inline">\(s_j^{(t)}(x)\)</span> on response variable <span class="math display">\[y_i - \sum_{k \not= j} s_k^{(t-1)}(x_{ij}).\]</span></li>
<li>Repeat until convergence.</li>
</ol>
<p>Note that some extra steps have to be taken to deal with the intercept.</p>
</section><section id="gam-definition" class="slide level2">
<h2>GAM Definition</h2>
<p><span class="math inline">\(Y | {\boldsymbol{X}}\)</span> is distributed according to an exponential family distribution. The extension of additive models to this family of response variable is called <strong>generalized additive models</strong> (GAMs). The model is of the form</p>
<p><span class="math display">\[
g\left({\operatorname{E}}[Y_i | {\boldsymbol{X}}_i]\right) = \sum_{j=1}^p s_j(X_{ij})
\]</span></p>
<p>where <span class="math inline">\(g(\cdot)\)</span> is the link function and the <span class="math inline">\(s_j(\cdot)\)</span> are flexible and/or nonparametric functions.</p>
</section><section id="overview-of-fitting-gams" class="slide level2">
<h2>Overview of Fitting GAMs</h2>
<p>Fitting GAMs involves putting together the following three tools:</p>
<ol type="1">
<li>We know how to fit a GLM via IRLS</li>
<li>We know how to fit a smoother of a single explanatory variable via a least squares solution, as seen for the NCS</li>
<li>We know how to combine additive smoothers by backfitting</li>
</ol>
</section><section id="gams-in-r" class="slide level2">
<h2>GAMs in R</h2>
<p>Three common ways to fit GAMs in R:</p>
<ol type="1">
<li>Utilize <code>glm()</code> on explanatory variables constructed from <code>ns()</code> or <code>bs()</code></li>
<li>The <code>gam</code> library</li>
<li>The <code>mgcv</code> library</li>
</ol>
</section><section id="example-1" class="slide level2">
<h2>Example</h2>
<pre class="r"><code>&gt; set.seed(508)
&gt; x1 &lt;- seq(1, 10, length.out=50)
&gt; n &lt;- length(x1)
&gt; x2 &lt;- rnorm(n)
&gt; f &lt;- 4*log(x1) + sin(x1) - 7 + 0.5*x2
&gt; p &lt;- exp(f)/(1+exp(f))
&gt; summary(p)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
0.001842 0.074170 0.310700 0.436200 0.860400 0.944800 
&gt; y &lt;- rbinom(n, size=1, prob=p)
&gt; mean(y)
[1] 0.42
&gt; df &lt;- data.frame(x1=x1, x2=x2, y=y)</code></pre>
</section><section class="slide level2">

<p>Here, we use the <code>gam()</code> function from the <code>mgcv</code> library. It automates choosing the smoothness of the splines.</p>
<pre class="r"><code>&gt; library(mgcv)
&gt; mygam &lt;- gam(y ~ s(x1) + s(x2), family = binomial(), data=df)
&gt; library(broom)
&gt; tidy(mygam)
   term      edf   ref.df statistic     p.value
1 s(x1) 1.870282 2.374897 12.742930 0.005308795
2 s(x2) 1.000024 1.000048  1.163059 0.280836876</code></pre>
</section><section class="slide level2">

<pre class="r"><code>&gt; summary(mygam)

Family: binomial 
Link function: logit 

Formula:
y ~ s(x1) + s(x2)

Parametric coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.1380     0.6723  -1.693   0.0905 .
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Approximate significance of smooth terms:
       edf Ref.df Chi.sq p-value   
s(x1) 1.87  2.375 12.743 0.00531 **
s(x2) 1.00  1.000  1.163 0.28084   
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

R-sq.(adj) =  0.488   Deviance explained =   47%
UBRE = -0.12392  Scale est. = 1         n = 50</code></pre>
</section><section class="slide level2">

<p>True probabilities vs. estimated probabilities.</p>
<pre class="r"><code>&gt; plot(p, mygam$fitted.values, pch=19); abline(0,1)</code></pre>
<p><img src="week10_files/figure-revealjs/unnamed-chunk-26-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section class="slide level2">

<p>Smoother estimated for <code>x1</code>.</p>
<pre class="r"><code>&gt; plot(mygam, select=1)</code></pre>
<p><img src="week10_files/figure-revealjs/unnamed-chunk-27-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section class="slide level2">

<p>Smoother estimated for <code>x2</code>.</p>
<pre class="r"><code>&gt; plot(mygam, select=2)</code></pre>
<p><img src="week10_files/figure-revealjs/unnamed-chunk-28-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section class="slide level2">

<p>Here, we use the <code>glm()</code> function and include as an explanatory variable a NCS built from the <code>ns()</code> function from the <code>splines</code> library. We include a <code>df</code> argument in the <code>ns()</code> call.</p>
<pre class="r"><code>&gt; library(splines)
&gt; myglm &lt;- glm(y ~ ns(x1, df=2) + x2, family = binomial(), data=df)
&gt; tidy(myglm)
             term    estimate  std.error statistic     p.value
1     (Intercept) -10.9228749  5.3078903 -2.057856 0.039603941
2 ns(x1, df = 2)1  21.3848301 10.1317747  2.110670 0.034800710
3 ns(x1, df = 2)2   6.3266262  2.1103294  2.997933 0.002718174
4              x2   0.7341812  0.6089442  1.205663 0.227947633</code></pre>
</section><section class="slide level2">

<p>The spline basis evaluated at <code>x1</code> values.</p>
<pre class="r"><code>&gt; ns(x1, df=2)
               1           2
 [1,] 0.00000000  0.00000000
 [2,] 0.03114456 -0.02075171
 [3,] 0.06220870 -0.04138180
 [4,] 0.09311200 -0.06176867
 [5,] 0.12377405 -0.08179071
 [6,] 0.15411442 -0.10132630
 [7,] 0.18405270 -0.12025384
 [8,] 0.21350847 -0.13845171
 [9,] 0.24240131 -0.15579831
[10,] 0.27065081 -0.17217201
[11,] 0.29817654 -0.18745121
[12,] 0.32489808 -0.20151430
[13,] 0.35073503 -0.21423967
[14,] 0.37560695 -0.22550571
[15,] 0.39943343 -0.23519080
[16,] 0.42213406 -0.24317334
[17,] 0.44362840 -0.24933170
[18,] 0.46383606 -0.25354429
[19,] 0.48267660 -0.25568949
[20,] 0.50006961 -0.25564569
[21,] 0.51593467 -0.25329128
[22,] 0.53019136 -0.24850464
[23,] 0.54275927 -0.24116417
[24,] 0.55355797 -0.23114825
[25,] 0.56250705 -0.21833528
[26,] 0.56952943 -0.20260871
[27,] 0.57462513 -0.18396854
[28,] 0.57787120 -0.16253131
[29,] 0.57934806 -0.13841863
[30,] 0.57913614 -0.11175212
[31,] 0.57731586 -0.08265339
[32,] 0.57396762 -0.05124405
[33,] 0.56917185 -0.01764570
[34,] 0.56300897  0.01802003
[35,] 0.55555939  0.05563154
[36,] 0.54690354  0.09506722
[37,] 0.53712183  0.13620546
[38,] 0.52629468  0.17892464
[39,] 0.51450251  0.22310315
[40,] 0.50182573  0.26861939
[41,] 0.48834478  0.31535174
[42,] 0.47414005  0.36317859
[43,] 0.45929198  0.41197833
[44,] 0.44388099  0.46162934
[45,] 0.42798748  0.51201003
[46,] 0.41169188  0.56299877
[47,] 0.39507460  0.61447395
[48,] 0.37821607  0.66631397
[49,] 0.36119670  0.71839720
[50,] 0.34409692  0.77060206
attr(,&quot;degree&quot;)
[1] 3
attr(,&quot;knots&quot;)
50% 
5.5 
attr(,&quot;Boundary.knots&quot;)
[1]  1 10
attr(,&quot;intercept&quot;)
[1] FALSE
attr(,&quot;class&quot;)
[1] &quot;ns&quot;     &quot;basis&quot;  &quot;matrix&quot;</code></pre>
</section><section class="slide level2">

<p>Plot of basis function values vs <code>x1</code>.</p>
<p><img src="week10_files/figure-revealjs/unnamed-chunk-31-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section class="slide level2">

<pre class="r"><code>&gt; summary(myglm)

Call:
glm(formula = y ~ ns(x1, df = 2) + x2, family = binomial(), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0214  -0.3730  -0.0162   0.5762   1.7616  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)     -10.9229     5.3079  -2.058  0.03960 * 
ns(x1, df = 2)1  21.3848    10.1318   2.111  0.03480 * 
ns(x1, df = 2)2   6.3266     2.1103   2.998  0.00272 **
x2                0.7342     0.6089   1.206  0.22795   
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 68.029  on 49  degrees of freedom
Residual deviance: 35.682  on 46  degrees of freedom
AIC: 43.682

Number of Fisher Scoring iterations: 7</code></pre>
</section><section class="slide level2">

<pre class="r"><code>&gt; anova(myglm, test=&quot;LRT&quot;)
Analysis of Deviance Table

Model: binomial, link: logit

Response: y

Terms added sequentially (first to last)

               Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                              49     68.029              
ns(x1, df = 2)  2   30.755        47     37.274 2.097e-07 ***
x2              1    1.592        46     35.682     0.207    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</section><section class="slide level2">

<p>True probabilities vs. estimated probabilities.</p>
<pre class="r"><code>&gt; plot(p, myglm$fitted.values, pch=19); abline(0,1)</code></pre>
<p><img src="week10_files/figure-revealjs/unnamed-chunk-34-1.png" width="576" style="display: block; margin: auto;" /></p>
</section></section>
<section><section id="bootstrap-for-statistical-models" class="titleslide slide level1"><h1>Bootstrap for Statistical Models</h1></section><section id="homoskedastic-models" class="slide level2">
<h2>Homoskedastic Models</h2>
<p>Let’s first discuss how one can utilize the bootstrap on any of the three homoskedastic models:</p>
<ul>
<li>Simple linear regression</li>
<li>Ordinary least squares</li>
<li>Additive models</li>
</ul>
</section><section id="residuals" class="slide level2">
<h2>Residuals</h2>
<p>In each of these scenarios we sample data <span class="math inline">\(({\boldsymbol{X}}_1, Y_1), ({\boldsymbol{X}}_2, Y_2), \ldots, ({\boldsymbol{X}}_n, Y_n)\)</span>. Let suppose we calculate fitted values <span class="math inline">\(\hat{Y}_i\)</span> and they are unbiased:</p>
<p><span class="math display">\[
{\operatorname{E}}[\hat{Y}_i | {\boldsymbol{X}}] = {\operatorname{E}}[Y_i | {\boldsymbol{X}}].
\]</span></p>
<p>We can calculate residuals <span class="math inline">\(\hat{E}_i = Y_i - \hat{Y}_i\)</span> for <span class="math inline">\(i=1, 2, \ldots, n\)</span>.</p>
</section><section id="studentized-residuals" class="slide level2">
<h2>Studentized Residuals</h2>
<p>One complication is that the residuals have a covariance. For example, in OLS we showed that</p>
<p><span class="math display">\[
{\operatorname{Cov}}(\hat{{\boldsymbol{E}}}) = \sigma^2 ({\boldsymbol{I}}- {\boldsymbol{P}})
\]</span></p>
<p>where <span class="math inline">\({\boldsymbol{P}}= {\boldsymbol{X}}({\boldsymbol{X}}^T {\boldsymbol{X}})^{-1} {\boldsymbol{X}}^T\)</span>.</p>
<p>To correct for this induced heteroskedasticity, we studentize the residuals by calculating</p>
<p><span class="math display">\[
R_i = \frac{\hat{E}_i}{\sqrt{1-P_{ii}}}
\]</span></p>
<p>which gives <span class="math inline">\({\operatorname{Cov}}({\boldsymbol{R}}) = \sigma^2 {\boldsymbol{I}}\)</span>.</p>
</section><section id="confidence-intervals" class="slide level2">
<h2>Confidence Intervals</h2>
<p>The following is a bootstrap procedure for calculating a confidence interval on some statistic <span class="math inline">\(\hat{\theta}\)</span> calculated from a homoskedastic model fit. An example is <span class="math inline">\(\hat{\beta}_j\)</span> in an OLS.</p>
<ol type="1">
<li>Fit the model to obtain fitted values <span class="math inline">\(\hat{Y}_i\)</span>, studentized residuals <span class="math inline">\(R_i\)</span>, and the statistic of interest <span class="math inline">\(\hat{\theta}\)</span>.<br />
For <span class="math inline">\(b = 1, 2, \ldots, B\)</span>.</li>
<li>Sample <span class="math inline">\(n\)</span> observations with replacement from <span class="math inline">\(\{R_i\}_{i=1}^n\)</span> to obtain bootstrap residuals <span class="math inline">\(R_1^{*}, R_2^{*}, \ldots, R_n^{*}\)</span>.</li>
<li>Form new response variables <span class="math inline">\(Y_i^{*} = \hat{Y}_i + R_i^{*}\)</span>.</li>
<li>Fit the model to obtain <span class="math inline">\(\hat{Y}^{*}_i\)</span> and all other fitted parameters.</li>
<li>Calculate statistic of interest <span class="math inline">\(\hat{\theta}^{*(b)}\)</span>.</li>
</ol>
</section><section class="slide level2">

<p>The bootstrap statistics <span class="math inline">\(\hat{\theta}^{*(1)}, \hat{\theta}^{*(2)}, \ldots, \hat{\theta}^{*(B)}\)</span> are then utilized through one of the techniques discussed earlier (percentile, pivotal, studentized pivotal) to calculate a bootstrap CI.</p>
</section><section id="hypothesis-testing" class="slide level2">
<h2>Hypothesis Testing</h2>
<p>Suppose we are testing the hypothesis <span class="math inline">\(H_0: {\operatorname{E}}[Y | {\boldsymbol{X}}] = f_0({\boldsymbol{X}})\)</span> vs <span class="math inline">\(H_1: {\operatorname{E}}[Y | {\boldsymbol{X}}] = f_1({\boldsymbol{X}})\)</span>. Suppose it is possible to form unbiased estimates <span class="math inline">\(f_0({\boldsymbol{X}})\)</span> and <span class="math inline">\(f_1({\boldsymbol{X}})\)</span> given <span class="math inline">\({\boldsymbol{X}}\)</span>, and <span class="math inline">\(f_0\)</span> is a restricted version of <span class="math inline">\(f_1\)</span>.</p>
<p>Suppose also we have a statistic <span class="math inline">\(T(\hat{f}_0, \hat{f}_1)\)</span> for performing this test so that the larger the statistic, the more evidence there is against the null hypothesis in favor of the alternative.</p>
<p>The big picture strategy is to bootstrap studentized residuals from the unconstrained (alternative hypothesis) fitted model and then add those to the constrained (null hypothesis) fitted model to generate bootstrap null data sets.</p>
</section><section class="slide level2">

<ol type="1">
<li>Fit the models to obtain fitted values <span class="math inline">\(\hat{f}_0({\boldsymbol{X}}_i)\)</span> and <span class="math inline">\(\hat{f}_1({\boldsymbol{X}}_i)\)</span>, studentized residuals <span class="math inline">\(R_i\)</span> from the fit <span class="math inline">\(\hat{f}_1({\boldsymbol{X}}_i)\)</span>, and the observed statistic <span class="math inline">\(T(\hat{f}_0, \hat{f}_1)\)</span>.<br />
For <span class="math inline">\(b = 1, 2, \ldots, B\)</span>.</li>
<li>Sample <span class="math inline">\(n\)</span> observations with replacement from <span class="math inline">\(\{R_i\}_{i=1}^n\)</span> to obtain bootstrap residuals <span class="math inline">\(R_1^{*}, R_2^{*}, \ldots, R_n^{*}\)</span>.</li>
<li>Form new response variables <span class="math inline">\(Y_i^{*} = \hat{f}_0({\boldsymbol{X}}_i) + R_i^{*}\)</span>.</li>
<li>Fit the models on the response variables <span class="math inline">\(Y_i^{*}\)</span> to obtain <span class="math inline">\(\hat{f}^{*}_0\)</span> and <span class="math inline">\(\hat{f}^{*}_1\)</span>.</li>
<li>Calculate statistic <span class="math inline">\(T(\hat{f}^{*(b)}_0, \hat{f}^{*(b)}_1)\)</span>.</li>
</ol>
</section><section class="slide level2">

<p>The p-value is then calculated as</p>
<p><span class="math display">\[
\frac{\sum_{b=1}^B 1\left(T(\hat{f}^{*(b)}_0, \hat{f}^{*(b)}_1) \geq T(\hat{f}_0, \hat{f}_1) \right) }{B}
\]</span></p>
</section><section id="parametric-bootstrap" class="slide level2">
<h2>Parametric Bootstrap</h2>
<p>For more complex scenarios, such as GLMs, GAMs, and heteroskedastic models, it is typically more straightforward to utilize a parametric bootstrap.</p>
</section></section>
<section><section id="extras" class="titleslide slide level1"><h1>Extras</h1></section><section id="source" class="slide level2">
<h2>Source</h2>
<p><a href="https://github.com/jdstorey/asdslectures/blob/master/LICENSE.md">License</a></p>
<p><a href="https://github.com/jdstorey/asdslectures/">Source Code</a></p>
</section><section id="session-information" class="slide level2">
<h2>Session Information</h2>
<section style="font-size: 0.75em;">
<pre class="r"><code>&gt; sessionInfo()
R version 3.3.2 (2016-10-31)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: macOS Sierra 10.12.4

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] splines   stats     graphics  grDevices utils     datasets 
[7] methods   base     

other attached packages:
 [1] mgcv_1.8-17     nlme_3.1-131    broom_0.4.2    
 [4] dplyr_0.5.0     purrr_0.2.2     readr_1.1.0    
 [7] tidyr_0.6.2     tibble_1.3.0    ggplot2_2.2.1  
[10] tidyverse_1.1.1 knitr_1.15.1    magrittr_1.5   
[13] devtools_1.12.0

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.10     highr_0.6        cellranger_1.1.0
 [4] plyr_1.8.4       forcats_0.2.0    tools_3.3.2     
 [7] digest_0.6.12    lubridate_1.6.0  jsonlite_1.4    
[10] evaluate_0.10    memoise_1.1.0    gtable_0.2.0    
[13] lattice_0.20-35  Matrix_1.2-10    psych_1.7.5     
[16] DBI_0.6-1        yaml_2.1.14      parallel_3.3.2  
[19] haven_1.0.0      xml2_1.1.1       withr_1.0.2     
[22] stringr_1.2.0    httr_1.2.1       revealjs_0.9    
[25] hms_0.3          rprojroot_1.2    grid_3.3.2      
[28] R6_2.2.0         readxl_1.0.0     foreign_0.8-68  
[31] rmarkdown_1.5    modelr_0.1.0     reshape2_1.4.2  
[34] backports_1.0.5  scales_0.4.1     htmltools_0.3.6 
[37] rvest_0.3.2      assertthat_0.2.0 mnormt_1.5-5    
[40] colorspace_1.3-2 labeling_0.3     stringi_1.1.5   
[43] lazyeval_0.2.0   munsell_0.4.3   </code></pre>
</section>
</section></section>
    </div>
  </div>

  <script src="libs/reveal.js-3.3.0.1/lib/js/head.min.js"></script>
  <script src="libs/reveal.js-3.3.0.1/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display a presentation progress bar
        progress: true,
        // Display the page number of the current slide
        slideNumber: false,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: true,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom



        chalkboard: {
        },

        keyboard: {
          67: function() { RevealChalkboard.toggleNotesCanvas() },    // toggle notes canvas when 'c' is pressed
          66: function() { RevealChalkboard.toggleChalkboard() }, // toggle chalkboard when 'b' is pressed
          46: function() { RevealChalkboard.clear() },    // clear chalkboard when 'DEL' is pressed
           8: function() { RevealChalkboard.reset() },    // reset chalkboard data on current slide when 'BACKSPACE' is pressed
          68: function() { RevealChalkboard.download() }, // downlad recorded chalkboard drawing when 'd' is pressed
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'libs/reveal.js-3.3.0.1/plugin/zoom-js/zoom.js', async: true },
          { src: 'libs/reveal.js-3.3.0.1/plugin/chalkboard/chalkboard.js', async: true },
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>


  </body>
</html>
