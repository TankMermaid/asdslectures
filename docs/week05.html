<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="John D. Storey" />
  <title>QCB 508 – Week 5</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="libs/reveal.js-3.3.0/css/reveal.css"/>



<link rel="stylesheet" href="libs/reveal.js-3.3.0/css/theme/simple.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }

  </style>

    <style type="text/css">code{white-space: pre;}</style>

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'libs/reveal.js-3.3.0/css/print/pdf.css' : 'libs/reveal.js-3.3.0/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
    <!--[if lt IE 9]>
    <script src="libs/reveal.js-3.3.0/lib/js/html5shiv.js"></script>
    <![endif]-->

    <link href="libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
</head>
<body>
<style type="text/css">
p { 
  text-align: left; 
  }
.reveal pre code { 
  color: #000000; 
  background-color: rgb(240,240,240);
  font-size: 1.15em;
  border:none; 
  }
.reveal section img { 
  background:none; 
  border:none; 
  box-shadow:none;
  height: 500px;
  }
}
</style>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">QCB 508 – Week 5</h1>
    <h2 class="author">John D. Storey</h2>
    <h3 class="date">Spring 2017</h3>
</section>

<section><section id="section" class="titleslide slide level1"><h1><img src="images/howto.jpg"></img></h1></section></section>
<section><section id="statistical-inference" class="titleslide slide level1"><h1>Statistical Inference</h1></section><section id="data-collection-as-a-probability" class="slide level2">
<h1>Data Collection as a Probability</h1>
<ul>
<li>Suppose data are collected in such a way that it is randomly observed according to a probability distribution</li>
<li>If that probability distribution can be parameterized, then it is possible that the <em>parameters describe key characteristics of the population of interest</em></li>
<li><strong>Statistical inference</strong> reverse engineers this process to estimate the unknown values of the parameters and express a measure of uncertainty about these estimates</li>
</ul>
</section><section id="example-simple-random-sample" class="slide level2">
<h1>Example: Simple Random Sample</h1>
<p>Individuals are uniformly and independently randomly sampled from a population.</p>
<p>The measurements taken on these individuals are then modeled as random variables, specifically random realizations from the complete population of values.</p>
<p>Simple random samples form the basis of modern surveys.</p>
</section><section id="example-randomized-controlled-trial" class="slide level2">
<h1>Example: Randomized Controlled Trial</h1>
<p>Individuals under study are randomly assigned to one of two or more available treatments.</p>
<p>This induces randomization directly into the study and breaks the relationship between the treatments and other variables that may be influencing the response of interest.</p>
<p>This is the gold standard study design in clinical trials to assess the evidence that a new drug works on a given disease.</p>
</section><section id="parameters-and-statistics" class="slide level2">
<h1>Parameters and Statistics</h1>
<ul>
<li>A <strong>parameter</strong> is a number that describes a population
<ul>
<li>A parameter is often a fixed number</li>
<li>We usually do not know its value<br />
</li>
</ul></li>
<li>A <strong>statistic</strong> is a number calculated from a sample of data</li>
<li>A statistic is used to estimate a parameter</li>
</ul>
</section><section id="sampling-distribution" class="slide level2">
<h1>Sampling Distribution</h1>
<p>The <strong>sampling distribution</strong> of a statistic is the probability disribution of the statistic under repeated realizations of the data from the assumed data generating probability distribution.</p>
<p><em>The sampling distribution is how we connect an observed statistic to the population.</em></p>
</section><section id="central-dogma-of-inference" class="slide level2">
<h1>Central Dogma of Inference</h1>
<center>
<img src="images/inference_idea.jpg" alt="central_dogma_statistics" />
</center>
</section><section id="example-fair-coin" class="slide level2">
<h1>Example: Fair Coin?</h1>
<p>Suppose I claim that a specific coin is fair, i.e., that it lands on heads or tails with equal probability.</p>
<p>I flip it 20 times and it lands on heads 16 times.</p>
<ol type="1">
<li>My data is <span class="math inline">\(x=16\)</span> heads out of <span class="math inline">\(n=20\)</span> flips.</li>
<li>My data generation model is <span class="math inline">\(X \sim \mbox{Binomial}(20, p)\)</span>.</li>
<li>I form the statistic <span class="math inline">\(\hat{p} = 16/20\)</span> as an estimate of <span class="math inline">\(p\)</span>.</li>
</ol>
</section><section id="example-contd" class="slide level2">
<h1>Example (cont’d)</h1>
<p>Let’s simulate 10,000 times what my estimate would look like if <span class="math inline">\(p=0.5\)</span> and I repeated the 20 coin flips over and over.</p>
<pre class="r"><code>&gt; x &lt;- replicate(n=1e4, expr=rbinom(1, size=20, prob=0.5))
&gt; sim_p_hat &lt;- x/20
&gt; my_p_hat &lt;- 16/20</code></pre>
<p>What can I do with this information?</p>
</section><section id="example-contd-1" class="slide level2">
<h1>Example (cont’d)</h1>
<p><img src="week05_files/figure-revealjs/sampling_plot-1.png" width="576" style="display: block; margin: auto;" /></p>
</section></section>
<section><section id="inference-goals-and-strategies" class="titleslide slide level1"><h1>Inference Goals and Strategies</h1></section><section id="basic-idea" class="slide level2">
<h1>Basic Idea</h1>
<p>Data are collected in such a way that there exists a reasonable probability model for this process that involves parameters informative about the population.</p>
<p>Common Goals:</p>
<ol type="1">
<li>Form point estimates the parameters</li>
<li>Quantify uncertainty on the estimates</li>
<li>Test hypotheses on the parameters</li>
</ol>
</section><section id="normal-example" class="slide level2">
<h1>Normal Example</h1>
<p>Suppose a simple random sample of <span class="math inline">\(n\)</span> data points is collected so that the following model of the data is reasonable: <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> are iid Normal(<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span>).</p>
<p>The goal is to do inference on <span class="math inline">\(\mu\)</span>, the population mean.</p>
<p>For simplicity, assume that <span class="math inline">\(\sigma^2\)</span> is known (e.g., <span class="math inline">\(\sigma^2 = 1\)</span>).</p>
</section><section id="point-estimate-of-mu" class="slide level2">
<h1>Point Estimate of <span class="math inline">\(\mu\)</span></h1>
<p>There are a number of ways to form an estimate of <span class="math inline">\(\mu\)</span>, but one that has several justifications is the sample mean:</p>
<p><span class="math display">\[\hat{\mu} = \overline{x} = \frac{1}{n}\sum_{i=1}^n x_i,\]</span></p>
<p>where <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> are the observed data points.</p>
</section><section id="sampling-distribution-of-hatmu" class="slide level2">
<h1>Sampling Distribution of <span class="math inline">\(\hat{\mu}\)</span></h1>
<p>If we were to repeat this study over and over, how would <span class="math inline">\(\hat{\mu}\)</span> behave?</p>
<p><span class="math display">\[\hat{\mu} = \overline{X} = \frac{1}{n}\sum_{i=1}^n X_i\]</span></p>
<p><span class="math display">\[\overline{X} \sim \mbox{Normal}(\mu, \sigma^2/n)\]</span></p>
<p>How do we use this to quantify uncertainty and test hypotheses?</p>
</section><section id="pivotal-statistic" class="slide level2">
<h1>Pivotal Statistic</h1>
<p>One <em>very useful</em> strategy is to work backwards from a pivotal statistic, which is a statistic that does not depend on any unknown paramaters.</p>
<p>Example:</p>
<p><span class="math display">\[\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim \mbox{Normal}(0,1)\]</span></p>
<p>Note that in general for a rv <span class="math inline">\(Y\)</span> it is the case that <span class="math inline">\((Y - \operatorname{E}[Y])/\sqrt{\operatorname{Var}(Y)}\)</span> has population mean 0 and variance 1.</p>
</section></section>
<section><section id="confidence-intervals" class="titleslide slide level1"><h1>Confidence Intervals</h1></section><section id="goal" class="slide level2">
<h1>Goal</h1>
<p>Once we have a point estimate of a parameter, we would like a measure of its uncertainty.</p>
<p>Given that we are working within a probabilistic framework, the natural language of uncertainty is through probability statements.</p>
<p>We interpret this measure of uncertainty in terms of hypothetical repetitions of the sampling scheme we used to collect the original data set.</p>
</section><section id="formulation" class="slide level2">
<h1>Formulation</h1>
<p>Confidence intervals take the form</p>
<p><span class="math display">\[(\hat{\mu} - C_{\ell}, \hat{\mu} + C_{u})\]</span></p>
<p>where</p>
<p><span class="math display">\[{\rm Pr}(\mu - C_{\ell} \leq \hat{\mu} \leq \mu + C_{u})\]</span></p>
<p>forms the “level” or coverage probability of the interval.</p>
</section><section id="interpretation" class="slide level2">
<h1>Interpretation</h1>
<p>If we repeat the study many times, then the CI <span class="math inline">\((\hat{\mu} - C_{\ell}, \hat{\mu} + C_{u})\)</span> will contain the true value <span class="math inline">\(\mu\)</span> with a long run frequency equal to <span class="math inline">\({\rm Pr}(\mu - C_{\ell} \leq \hat{\mu} \leq \mu + C_{u})\)</span>.</p>
<p>A CI calculated on an observed data set is <em>not</em> intepreted as: “There is probability <span class="math inline">\({\rm Pr}(\mu - C_{\ell} \leq \hat{\mu} \leq \mu + C_{u})\)</span> that <span class="math inline">\(\mu\)</span> is in our calculated <span class="math inline">\((\hat{\mu} - C_{\ell}, \hat{\mu} + C_{u})\)</span>.” Why not?</p>
</section><section id="a-normal-ci" class="slide level2">
<h1>A Normal CI</h1>
<p>If <span class="math inline">\(Z \sim\)</span> Normal(0,1), then <span class="math inline">\({\rm Pr}(-1.96 \leq Z \leq 1.96) = 0.95.\)</span></p>
<span class="math display">\[\begin{eqnarray}
0.95 &amp; = &amp; {\rm Pr} \left(-1.96 \leq \frac{\hat{\mu} - \mu}{\sigma/\sqrt{n}} \leq 1.96 \right) \\
\ &amp; = &amp; {\rm Pr} \left(-1.96 \frac{\sigma}{\sqrt{n}} \leq \hat{\mu} - \mu \leq 1.96\frac{\sigma}{\sqrt{n}} \right) \\
\ &amp; = &amp; {\rm Pr} \left(\mu-1.96\frac{\sigma}{\sqrt{n}} \leq \hat{\mu} \leq \mu+1.96\frac{\sigma}{\sqrt{n}} \right)
\end{eqnarray}\]</span>
<p>Therefore, <span class="math inline">\(\left(\hat{\mu} - 1.96\frac{\sigma}{\sqrt{n}}, \hat{\mu} + 1.96\frac{\sigma}{\sqrt{n}}\right)\)</span> forms a 95% confidence interval of <span class="math inline">\(\mu\)</span>.</p>
</section><section id="a-simulation" class="slide level2">
<h1>A Simulation</h1>
<pre class="r"><code>&gt; mu &lt;- 5
&gt; n &lt;- 20
&gt; x &lt;- replicate(10000, rnorm(n=n, mean=mu)) # 10000 studies
&gt; m &lt;- apply(x, 2, mean) # the estimate for each study
&gt; ci &lt;- cbind(m - 1.96/sqrt(n), m + 1.96/sqrt(n))
&gt; head(ci)
         [,1]     [,2]
[1,] 4.916469 5.793008
[2,] 4.507025 5.383563
[3,] 4.672412 5.548950
[4,] 4.791153 5.667692
[5,] 4.413962 5.290500
[6,] 4.514039 5.390578</code></pre>
<pre class="r"><code>&gt; cover &lt;- (mu &gt; ci[,1]) &amp; (mu &lt; ci[,2])
&gt; mean(cover)
[1] 0.9464</code></pre>
</section><section id="normal01-percentiles" class="slide level2">
<h1>Normal<span class="math inline">\((0,1)\)</span> Percentiles</h1>
<p>Above we constructed a 95% CI. How do we construct (1-<span class="math inline">\(\alpha\)</span>)-level CIs?</p>
<p>Let <span class="math inline">\(z_{\alpha}\)</span> be the <span class="math inline">\(\alpha\)</span> percentile of the Normal(0,1) distribution.</p>
<p>If <span class="math inline">\(Z \sim\)</span> Normal(0,1), then</p>
<span class="math display">\[\begin{eqnarray*}
1-\alpha &amp; = &amp; {\rm Pr}(z_{\alpha/2} \leq Z \leq z_{1-\alpha/2}) \\
\ &amp; = &amp; {\rm Pr}(-|z_{\alpha/2}| \leq Z \leq |z_{\alpha/2}|)
\end{eqnarray*}\]</span>
</section><section id="commonly-used-percentiles" class="slide level2">
<h1>Commonly Used Percentiles</h1>
<pre class="r"><code>&gt; # alpha/2 upper and lower percentiles for alpha=0.05
&gt; qnorm(0.025)
[1] -1.959964
&gt; qnorm(0.975)
[1] 1.959964</code></pre>
<pre class="r"><code>&gt; # alpha/2 upper and lower percentiles for alpha=0.10
&gt; qnorm(0.05)
[1] -1.644854
&gt; qnorm(0.95)
[1] 1.644854</code></pre>
</section><section id="alpha-level-cis" class="slide level2">
<h1><span class="math inline">\((1-\alpha)\)</span>-Level CIs</h1>
<p>If <span class="math inline">\(Z \sim\)</span> Normal(0,1), then <span class="math inline">\({\rm Pr}(-|z_{\alpha/2}| \leq Z \leq |z_{\alpha/2}|) = 1-\alpha.\)</span></p>
<p>Repeating the steps from the 95% CI case, we get the following is a <span class="math inline">\((1-\alpha)\)</span>-Level CI for <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[\left(\hat{\mu} - |z_{\alpha/2}| \frac{\sigma}{\sqrt{n}}, \hat{\mu} + |z_{\alpha/2}| \frac{\sigma}{\sqrt{n}}\right)\]</span></p>
</section><section id="one-sided-cis" class="slide level2">
<h1>One-Sided CIs</h1>
<p>The CIs we have considered so far are “two-sided”. Sometimes we are also interested in “one-sided” CIs.</p>
<p>If <span class="math inline">\(Z \sim\)</span> Normal(0,1), then <span class="math inline">\(1-\alpha = {\rm Pr}(Z \geq -|z_{\alpha}|)\)</span> and <span class="math inline">\(1-\alpha = {\rm Pr}(Z \leq |z_{\alpha}|).\)</span> We can use this fact along with the earlier derivations to show that the following are valid CIs:</p>
<p><span class="math display">\[(1-\alpha)\mbox{-level upper: } \left(-\infty, \hat{\mu} + |z_{\alpha}| \frac{\sigma}{\sqrt{n}}\right)\]</span></p>
<p><span class="math display">\[(1-\alpha)\mbox{-level lower: } \left(\hat{\mu} - |z_{\alpha}| \frac{\sigma}{\sqrt{n}}, \infty\right)\]</span></p>
</section></section>
<section><section id="hypothesis-tests" class="titleslide slide level1"><h1>Hypothesis Tests</h1></section><section id="example-ht-on-fairness-of-a-coin" class="slide level2">
<h1>Example: HT on Fairness of a Coin</h1>
<p>Suppose I claim that a specific coin is fair, i.e., that it lands on heads or tails with equal probability.</p>
<p>I flip it 20 times and it lands on heads 16 times.</p>
<ol type="1">
<li>My data is <span class="math inline">\(x=16\)</span> heads out of <span class="math inline">\(n=20\)</span> flips.</li>
<li>My data generation model is <span class="math inline">\(X \sim \mbox{Binomial}(20, p)\)</span>.</li>
<li>I form the statistic <span class="math inline">\(\hat{p} = 16/20\)</span> as an estimate of <span class="math inline">\(p\)</span>.</li>
</ol>
<p>More formally, I want to <strong>test the hypothesis</strong>: <span class="math inline">\(H_0: p=0.5\)</span> vs. <span class="math inline">\(H_1: p \not=0.5\)</span> under the model <span class="math inline">\(X \sim \mbox{Binomial}(20, p)\)</span> based on the <strong>test statistic</strong> <span class="math inline">\(\hat{p} = X/n\)</span>.</p>
</section><section id="example-contd-null-distribution" class="slide level2">
<h1>Example (cont’d): Null Distribution</h1>
<p>Let’s simulate 10,000 times what my estimate would look like if <span class="math inline">\(p=0.5\)</span> and I repeated the 20 coin flips over and over.</p>
<pre class="r"><code>&gt; x &lt;- replicate(n=1e4, expr=rbinom(1, size=20, prob=0.5))
&gt; sim_p_hat &lt;- x/20
&gt; my_p_hat &lt;- 16/20</code></pre>
<p>The vector <code>sim_p_hat</code> contains 10,000 draws from the <strong>null distribution</strong>, i.e., the distribution of my test statstic <span class="math inline">\(\hat{p} = X/n\)</span> when <span class="math inline">\(H_0: p=0.5\)</span> is true.</p>
</section><section class="slide level2">

<p><img src="week05_files/figure-revealjs/sampling_plot_3-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="example-contd-p-value" class="slide level2">
<h1>Example (cont’d): P-value</h1>
<p>The deviation of the test statistic from the null hypothesis can be measured by <span class="math inline">\(|\hat{p} - 0.5|\)</span>.</p>
<p>Let’s compare our observed deviation <span class="math inline">\(|16/20 - 0.5|\)</span> to the 10,000 simulated null data sets. Specifically, let’s calculate the frequency by which these 10,000 cases are <strong>as or more extreme</strong> than the observed test statistic.</p>
<pre class="r"><code>&gt; sum(abs(sim_p_hat-0.5) &gt;= abs(my_p_hat-0.5))/1e4
[1] 0.0077</code></pre>
<p>This quantity is called the <strong>p-value</strong> of the hypothesis test.</p>
</section><section id="a-caveat" class="slide level2">
<h1>A Caveat</h1>
<p>This example is a simplification of a more general framework for testing statistical hypotheses.</p>
<p>Given the intuition provided by the example, let’s now formalize these ideas.</p>
</section><section id="definition" class="slide level2">
<h1>Definition</h1>
<ul>
<li>A <strong>hypothesis test</strong> or <strong>significance test</strong> is a formal procedure for comparing observed data with a hypothesis whose truth we want to assess</li>
<li>The results of a test are expressed in terms of a probability that measures how well the data and the hypothesis agree</li>
<li>The <strong>null hypothesis</strong> (<span class="math inline">\(H_0\)</span>) is the statement being tested, typically the status quo</li>
<li>The <strong>alternative hypothesis</strong> (<span class="math inline">\(H_1\)</span>) is the complement of the null, and it is often the “interesting” state</li>
</ul>
</section><section id="return-to-normal-example" class="slide level2">
<h1>Return to Normal Example</h1>
<p>Let’s return to our Normal example in order to demonstrate the framework.</p>
<p>Suppose a simple random sample of <span class="math inline">\(n\)</span> data points is collected so that the following model of the data is reasonable: <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> are iid Normal(<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span>).</p>
<p>The goal is to do test a hypothesis on <span class="math inline">\(\mu\)</span>, the population mean.</p>
<p>For simplicity, assume that <span class="math inline">\(\sigma^2\)</span> is known (e.g., <span class="math inline">\(\sigma^2 = 1\)</span>).</p>
</section><section id="hts-on-parameter-values" class="slide level2">
<h1>HTs on Parameter Values</h1>
<p>Hypothesis tests are usually formulated in terms of values of parameters. For example:</p>
<p><span class="math display">\[H_0: \mu = 5\]</span></p>
<p><span class="math display">\[H_1: \mu \not= 5\]</span></p>
<p>Note that the choice of 5 here is arbitrary, for illustrative purposes only. In a typical real world problem, the values that define the hypotheses will be clear from the context.</p>
</section><section id="two-sided-vs.one-sided-ht" class="slide level2">
<h1>Two-Sided vs. One-Sided HT</h1>
<p>Hypothesis tests can be two-sided or one-sided:</p>
<p><span class="math display">\[H_0: \mu = 5 \mbox{ vs. } H_1: \mu \not= 5 \mbox{ (two-sided)}\]</span></p>
<p><span class="math display">\[H_0: \mu \leq 5 \mbox{ vs. } H_1: \mu &gt; 5 \mbox{ (one-sided)}\]</span></p>
<p><span class="math display">\[H_0: \mu \geq 5 \mbox{ vs. } H_1: \mu &lt; 5 \mbox{ (one-sided)}\]</span></p>
</section><section id="test-statistic" class="slide level2">
<h1>Test Statistic</h1>
<p>A <strong>test statistic</strong> is designed to <em>quantify the evidence against the null hypothesis in favor of the alternative</em>. They are usually defined (and justified using math theory) so that the larger the test statistic is, the more evidence there is.</p>
<p>For the Normal example and the two-sided hypothesis <span class="math inline">\((H_0: \mu = 5 \mbox{ vs. } H_1: \mu \not= 5)\)</span>, here is our test statistic:</p>
<p><span class="math display">\[
|z| = \frac{\left|\overline{x} - 5\right|}{\sigma/\sqrt{n}}
\]</span></p>
<p>What would the test statistic be for the one-sided hypothesis tests?</p>
</section><section id="null-distribution-two-sided" class="slide level2">
<h1>Null Distribution (Two-Sided)</h1>
<p>The <strong>null distribution</strong> is the sampling distribution of the test statistic when <span class="math inline">\(H_0\)</span> is true.</p>
<p>We saw earlier that <span class="math inline">\(\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim \mbox{Normal}(0,1).\)</span></p>
<p>When <span class="math inline">\(H_0\)</span> is true, then <span class="math inline">\(\mu=5\)</span>. So when <span class="math inline">\(H_0\)</span> is true it follows that</p>
<p><span class="math display">\[Z = \frac{\overline{X} - 5}{\sigma/\sqrt{n}} \sim \mbox{Normal}(0,1)\]</span></p>
<p>and then probabiliy calculations on <span class="math inline">\(|Z|\)</span> are straightforward. Note that <span class="math inline">\(Z\)</span> is pivotal when <span class="math inline">\(H_0\)</span> is true!</p>
</section><section id="null-distribution-one-sided" class="slide level2">
<h1>Null Distribution (One-Sided)</h1>
<p>When performing a one-sided hypothesis test, such as <span class="math inline">\(H_0: \mu \leq 5 \mbox{ vs. } H_1: \mu &gt; 5\)</span>, the null distribution is typically calculated under the “least favorable” value, which is the boundary value.</p>
<p>In this example it would be <span class="math inline">\(\mu=5\)</span> and we would again utilize the null distribution <span class="math display">\[Z = \frac{\overline{X} - 5}{\sigma/\sqrt{n}} \sim \mbox{Normal}(0,1).\]</span></p>
</section><section id="p-values" class="slide level2">
<h1>P-values</h1>
<p>The <strong>p-value</strong> is defined to be the probability that a test statistic from the null distribution is <em>as or more extreme than the observed statistic</em>. In our Normal example on the two-sided hypothesis test, the p-value is</p>
<p><span class="math display">\[{\rm Pr}(|Z^*| \geq |z|)\]</span></p>
<p>where <span class="math inline">\(Z^* \sim \mbox{Normal}(0,1)\)</span> and <span class="math inline">\(|z|\)</span> is the value of the test statistic calculated on the data (so it is a fixed number once we observe the data).</p>
</section><section id="calling-a-test-significant" class="slide level2">
<h1>Calling a Test “Significant”</h1>
<p>A hypothesis test is called <strong>statistically significant</strong> — meaning we reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> — if its p-value is sufficiently small.</p>
<p>Commonly used cut-offs are 0.01 or 0.05, although these are not always appropriate and they are historical artifacts.</p>
<p>Applying a specific p-value cut-off to determine significance determines an error rate, which we define next.</p>
</section><section id="types-of-errors" class="slide level2">
<h1>Types of Errors</h1>
<p>There are two types of errors that can be committed when performing a hypothesis test.</p>
<ol type="1">
<li>A <strong>Type I error</strong> or <strong>false positive</strong> is when a hypothesis test is called signficant and the null hypothesis is actually true.</li>
<li>A <strong>Type II error</strong> or <strong>false negative</strong> is when a hypothesis test is not called signficant and the alternative hypothesis is actually true.</li>
</ol>
</section><section id="error-rates" class="slide level2">
<h1>Error Rates</h1>
<ul>
<li>The <strong>Type I error rate</strong> or <strong>false positive rate</strong> is the probability of this type of error given that <span class="math inline">\(H_0\)</span> is true.</li>
<li>If a hypothesis test is called significant when p-value <span class="math inline">\(\leq \alpha\)</span> then it has a Type I error rate equal to <span class="math inline">\(\alpha\)</span>.</li>
<li>The <strong>Type II error rate</strong> or <strong>false negative rate</strong> is the probability of this type of error given that <span class="math inline">\(H_1\)</span> is true.</li>
<li>The <strong>power</strong> of a hypothesis test is <span class="math inline">\(1 -\)</span> Type II error rate.</li>
</ul>
<p>Hypothesis tests are usually derived with a goal to control the Type I error rate while maximizing the power.</p>
</section></section>
<section><section id="maximum-likelihood-estimation" class="titleslide slide level1"><h1>Maximum Likelihood Estimation</h1></section><section id="the-normal-example" class="slide level2">
<h1>The Normal Example</h1>
<p>We formulated both confidence intervals and hypothesis tests under the following idealized scenario:</p>
<blockquote>
<p>Suppose a simple random sample of <span class="math inline">\(n\)</span> data points is collected so that the following model of the data is reasonable: <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> are iid Normal(<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span>). The goal is to do inference on <span class="math inline">\(\mu\)</span>, the population mean. For simplicity, assume that <span class="math inline">\(\sigma^2\)</span> is known (e.g., <span class="math inline">\(\sigma^2 = 1\)</span>).</p>
</blockquote>
<p>There is a good reason why we did this.</p>
</section><section id="mle-rightarrow-normal-pivotal-statistics" class="slide level2">
<h1>MLE <span class="math inline">\(\rightarrow\)</span> Normal Pivotal Statistics</h1>
<p>The random variable distributions we introduced last week have maximum likelihood estimators (MLEs) that can be standardized to yield a pivotal statistic with a Normal(0,1) distribution based on MLE theory.</p>
<p>For example, if <span class="math inline">\(X \sim \mbox{Binomial}(n,p)\)</span> then <span class="math inline">\(\hat{p}=X/n\)</span> is the MLE. For large <span class="math inline">\(n\)</span> it approximately holds that <span class="math display">\[
\frac{\hat{p} - p}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}} \sim \mbox{Normal}(0,1).
\]</span></p>
</section><section id="likelihood-function" class="slide level2">
<h1>Likelihood Function</h1>
<p>Suppose that we observe <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> according to the model <span class="math inline">\(X_1, X_2, \ldots, X_n \sim F_{\theta}\)</span>. The joint pdf is <span class="math inline">\(f(\boldsymbol{x} ; \theta)\)</span>. We view the pdf as being a function of <span class="math inline">\(\boldsymbol{x}\)</span> for a fixed <span class="math inline">\(\theta\)</span>.</p>
<p>The <strong>likelihood function</strong> is obtained by reversing the arguments and viewing this as a function of <span class="math inline">\(\theta\)</span> for a fixed, observed <span class="math inline">\(\boldsymbol{x}\)</span>:</p>
<p><span class="math display">\[L(\theta ; \boldsymbol{x}) = f(\boldsymbol{x} ; \theta).\]</span></p>
</section><section id="log-likelihood-function" class="slide level2">
<h1>Log-Likelihood Function</h1>
<p>The log-likelihood function is</p>
<p><span class="math display">\[ \ell(\theta ; \boldsymbol{x}) = \log L(\theta ; \boldsymbol{x}).\]</span></p>
<p>When the data are iid, we have</p>
<p><span class="math display">\[ \ell(\theta ; \boldsymbol{x}) = \log \prod_{i=1}^n f(x_i ; \theta) = \sum_{i=1}^n \log f(x_i ; \theta).\]</span></p>
</section><section id="calculating-mles" class="slide level2">
<h1>Calculating MLEs</h1>
<p>The <strong>maximum likelihood estimate</strong> is the value of <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(L(\theta ; \boldsymbol{x})\)</span> for an observe data set <span class="math inline">\(\boldsymbol{x}\)</span>.</p>
<span class="math display">\[\begin{align*}
\hat{\theta}_{{\rm MLE}} &amp; = \operatorname{argmax}_{\theta} L(\theta ; \boldsymbol{x}) \\
 &amp; = \operatorname{argmax}_{\theta} \ell (\theta ; \boldsymbol{x}) \\
 &amp; = \operatorname{argmax}_{\theta} L (\theta ; T(\boldsymbol{x}))
\end{align*}\]</span>
<p>where the last equality holds for sufficient statistics <span class="math inline">\(T(\boldsymbol{x})\)</span>.</p>
<p>The MLE can usually be calculated analytically or numerically.</p>
</section><section id="properties" class="slide level2">
<h1>Properties</h1>
<p>When “certain regularity assumptions” are true, the following properties hold for MLEs.</p>
<ul>
<li>Consistent</li>
<li>Equivariant</li>
<li>Asymptotically Normal</li>
<li>Asymptotically Efficient (or Optimal)</li>
<li>Approximate Bayes Estimator</li>
</ul>
</section><section id="assumptions-and-notation" class="slide level2">
<h1>Assumptions and Notation</h1>
<p>We will assume that <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{{\rm iid}}{\sim} F_{\theta}\)</span> and let <span class="math inline">\(\hat{\theta}_n\)</span> be the MLE of <span class="math inline">\(\theta\)</span> based on the <span class="math inline">\(n\)</span> observations.</p>
<p>The only exception is for the Binomial distribution where we will assume that <span class="math inline">\(X \sim \mbox{Binomial}(n, p)\)</span>, which is the sum of <span class="math inline">\(n\)</span> iid <span class="math inline">\(\mbox{Bernoulli}(p)\)</span> rv’s.</p>
<p>We will assume that the “certain regularity assumptions” are true in the following results.</p>
</section><section id="consistency" class="slide level2">
<h1>Consistency</h1>
<p>An estimator is consistent if it converges in probability to the true parameter value. MLEs are consistent so that as <span class="math inline">\(n \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[\hat{\theta}_n \stackrel{P}{\rightarrow} \theta,\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the true value.</p>
</section><section id="equivariance" class="slide level2">
<h1>Equivariance</h1>
<p> </p>
<p>If <span class="math inline">\(\hat{\theta}_n\)</span> is the MLE of <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(g\left(\hat{\theta}_n\right)\)</span> is the MLE of <span class="math inline">\(g(\theta)\)</span>.</p>
<p> </p>
<p>Example: For the Normal<span class="math inline">\((\mu, \sigma^2)\)</span> the MLE of <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\overline{X}\)</span>. Therefore, the MLE of <span class="math inline">\(e^\mu\)</span> is <span class="math inline">\(e^{\overline{X}}\)</span>.</p>
</section><section id="fisher-information" class="slide level2">
<h1>Fisher Information</h1>
<p>The <strong>Fisher Information</strong> of <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{{\rm iid}}{\sim} F_{\theta}\)</span> is:</p>
<p> </p>
<span class="math display">\[\begin{align*}
I_n(\theta) &amp; = \operatorname{Var}\left( \frac{d}{d\theta} \log f(\boldsymbol{X}; \theta) \right)
= \sum_{i=1}^n \operatorname{Var}\left( \frac{d}{d\theta} \log f(X_i; \theta) \right) \\
&amp; = - \operatorname{E}\left( \frac{d^2}{d\theta^2} \log f(\boldsymbol{X}; \theta) \right) 
= - \sum_{i=1}^n \operatorname{E}\left( \frac{d^2}{d\theta^2} \log f(X_i; \theta) \right)
\end{align*}\]</span>
</section><section id="standard-error" class="slide level2">
<h1>Standard Error</h1>
<p>In general, the <strong>standard error</strong> of the standard deviation of sampling distribution of an estimate or statistic.</p>
<p>For MLEs, the standard error is <span class="math inline">\(\sqrt{{\operatorname{Var}}\left(\hat{\theta}_n\right)}\)</span>. It has the approximation</p>
<p><span class="math display">\[\operatorname{se}\left(\hat{\theta}_n\right) \approx \frac{1}{\sqrt{I_n(\theta)}}\]</span> and the standard error estimate is</p>
<p><span class="math display">\[\hat{\operatorname{se}}\left(\hat{\theta}_n\right) = \frac{1}{\sqrt{I_n\left(\hat{\theta}_n\right)}}.\]</span></p>
</section><section id="asymptotic-normal" class="slide level2">
<h1>Asymptotic Normal</h1>
<p>MLEs converge in distribution to the Normal distribution. Specifically, as <span class="math inline">\(n \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[\frac{\hat{\theta}_n - \theta}{{\operatorname{se}}\left(\hat{\theta}_n\right)} \stackrel{D}{\longrightarrow} \mbox{Normal}(0,1)\]</span> and</p>
<p><span class="math display">\[\frac{\hat{\theta}_n - \theta}{\hat{{\operatorname{se}}}\left(\hat{\theta}_n\right)} \stackrel{D}{\longrightarrow} \mbox{Normal}(0,1).\]</span></p>
</section><section id="asymptotic-pivotal-statistic" class="slide level2">
<h1>Asymptotic Pivotal Statistic</h1>
<p>By the previous result, we now have an approximate (asymptotic) pivotal statistic:</p>
<p><span class="math display">\[Z = \frac{\hat{\theta}_n - \theta}{\hat{{\operatorname{se}}}\left(\hat{\theta}_n\right)} \stackrel{D}{\longrightarrow} \mbox{Normal}(0,1).\]</span></p>
<p>This allows us to construct approximate confidence intervals and hypothesis test as in the idealized <span class="math inline">\(\mbox{Normal}(\mu, \sigma^2)\)</span> (with <span class="math inline">\(\sigma^2\)</span> known) scenario from the previous sections.</p>
</section><section id="wald-test" class="slide level2">
<h1>Wald Test</h1>
<p>Consider the hypothesis test, <span class="math inline">\(H_0: \theta=\theta_0\)</span> vs <span class="math inline">\(H_1: \theta \not= \theta_0\)</span>. We form test statistic</p>
<p><span class="math display">\[z = \frac{\hat{\theta}_n - \theta_0}{\hat{{\operatorname{se}}}\left(\hat{\theta}_n\right)},\]</span></p>
<p>which has approximate p-value</p>
<p><span class="math display">\[\mbox{p-value} = {\rm Pr}(|Z^*| \geq |z|),\]</span></p>
<p>where <span class="math inline">\(Z^*\)</span> is a Normal<span class="math inline">\((0,1)\)</span> random variable.</p>
</section><section id="confidence-intervals-1" class="slide level2">
<h1>Confidence Intervals</h1>
<p>Using this MLE theory, we can form approximate <span class="math inline">\((1-\alpha)\)</span> level confidence intervals as follows.</p>
<p>Two-sided: <span class="math display">\[\left(\hat{\theta}_n - |z_{\alpha/2}| \hat{{\operatorname{se}}}\left(\hat{\theta}_n\right), \hat{\theta}_n + |z_{\alpha/2}| \hat{{\operatorname{se}}}\left(\hat{\theta}_n\right)\right)\]</span></p>
<p>Upper: <span class="math display">\[\left(-\infty, \hat{\theta}_n + |z_{\alpha}| \hat{{\operatorname{se}}}\left(\hat{\theta}_n\right)\right)\]</span></p>
<p>Lower: <span class="math display">\[\left(\hat{\theta}_n - |z_{\alpha}| \hat{{\operatorname{se}}}\left(\hat{\theta}_n\right), \infty\right)\]</span></p>
</section><section id="optimality" class="slide level2">
<h1>Optimality</h1>
<p>The MLE is such that</p>
<p><span class="math display">\[ \sqrt{n} \left( \hat{\theta}_n - \theta \right) \stackrel{D}{\longrightarrow} \mbox{Normal}(0, \tau^2)\]</span></p>
<p>for some <span class="math inline">\(\tau^2\)</span>. Suppose that <span class="math inline">\(\tilde{\theta}_n\)</span> is any other estimate so that</p>
<p><span class="math display">\[ \sqrt{n} \left( \tilde{\theta}_n - \theta \right) \stackrel{D}{\longrightarrow} \mbox{Normal}(0, \gamma^2).\]</span></p>
<p>It follows that</p>
<p><span class="math display">\[\frac{\tau^2}{\gamma^2} \leq 1.\]</span></p>
</section><section id="delta-method" class="slide level2">
<h1>Delta Method</h1>
<p>Suppose that <span class="math inline">\(g()\)</span> is a differentiable function and <span class="math inline">\(g&#39;(\theta) \not= 0\)</span>. Note that for some <span class="math inline">\(t\)</span> in a neighborhood of <span class="math inline">\(\theta\)</span>, a first-order Taylor expansion tells us that <span class="math inline">\(g(t) \approx g&#39;(\theta) (t - \theta)\)</span>. From this we know that</p>
<p><span class="math display">\[{\operatorname{Var}}\left(g(\hat{\theta}_n) \right) \approx g&#39;(\theta)^2 {\operatorname{Var}}(\hat{\theta}_n)\]</span></p>
<p>The delta method shows that <span class="math inline">\(\hat{{\operatorname{se}}}\left(g(\hat{\theta}_n)\right) = |g&#39;(\hat{\theta}_n)| \hat{{\operatorname{se}}}\left(\hat{\theta}_n\right)\)</span> and</p>
<p><span class="math display">\[\frac{g(\hat{\theta}_n) - g(\theta)}{|g&#39;(\hat{\theta}_n)| \hat{{\operatorname{se}}}\left(\hat{\theta}_n\right)} \stackrel{D}{\longrightarrow} \mbox{Normal}(0,1).\]</span></p>
</section><section id="delta-method-example" class="slide level2">
<h1>Delta Method Example</h1>
<p>Suppose <span class="math inline">\(X \sim \mbox{Binomial}(n,p)\)</span> which has MLE, <span class="math inline">\(\hat{p} = X/n\)</span>. By the equivariance property, the MLE of the per-trial variance <span class="math inline">\(p(1-p)\)</span> is <span class="math inline">\(\hat{p}(1-\hat{p})\)</span>. It can be calculated that <span class="math inline">\(\hat{{\operatorname{se}}}(\hat{p}) = \sqrt{\hat{p}(1-\hat{p})/n}\)</span>.</p>
<p>Let <span class="math inline">\(g(p) = p(1-p)\)</span>. Then <span class="math inline">\(g&#39;(p) = 1-2p\)</span>. By the delta method,</p>
<p><span class="math display">\[\hat{{\operatorname{se}}}\left( \hat{p}(1-\hat{p}) \right) = \left| (1-2\hat{p}) \right| \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.\]</span></p>
</section><section id="multiparameter-fisher-info-matrix" class="slide level2">
<h1>Multiparameter Fisher Info Matrix</h1>
<p>Suppose that <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{{\rm iid}}{\sim} F_{\boldsymbol{\theta}}\)</span> where <span class="math inline">\(\boldsymbol{\theta} = (\theta_1, \theta_2, \ldots, \theta_d)^T\)</span> has MLE <span class="math inline">\(\hat{\boldsymbol{\theta}}_n\)</span>.</p>
<p>The Fisher Information Matrix <span class="math inline">\(I_n(\boldsymbol{\theta})\)</span> is the <span class="math inline">\(d \times d\)</span> matrix with <span class="math inline">\((i, j)\)</span> entry</p>
<p><span class="math display">\[ -\sum_{k=1}^n \operatorname{E}\left( \frac{\partial^2}{\partial\theta_i \partial\theta_j} \log f(X_k; \boldsymbol{\theta}) \right).\]</span></p>
</section><section id="multiparameter-asymptotic-mvn" class="slide level2">
<h1>Multiparameter Asymptotic MVN</h1>
<p>Under appropriate regularity conditions, as <span class="math inline">\(n \rightarrow \infty\)</span>,</p>
<p> </p>
<p><span class="math display">\[
\left( \hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta} \right) \stackrel{D}{\longrightarrow} \mbox{MVN}_d \left( \boldsymbol{0}, I_n(\boldsymbol{\theta})^{-1} \right) \mbox{ and }
\]</span></p>
<p> </p>
<p><span class="math display">\[
\left( \hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta} \right)^T I_n(\hat{\boldsymbol{\theta}}_n) \left( \hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta} \right) \stackrel{D}{\longrightarrow} \mbox{MVN}_d \left( \boldsymbol{0}, \boldsymbol{I} \right).
\]</span></p>
</section></section>
<section><section id="mle-examples-one-sample" class="titleslide slide level1"><h1>MLE Examples: One Sample</h1></section><section id="exponential-family-distributions" class="slide level2">
<h1>Exponential Family Distributions</h1>
<p>Suppose <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> are iid from some EFD. Then,</p>
<p><span class="math display">\[
\frac{\partial}{\partial \eta_k} \ell(\boldsymbol{\eta} ; \boldsymbol{x}) = \sum_{i=1}^n T_k(x_i) - n \frac{\partial}{\partial \eta_k} A(\boldsymbol{\eta})
\]</span> Setting the second equation to 0, it follows that the MLE of <span class="math inline">\(\eta_k\)</span> is the solution to</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^n T_k(x_i) = \frac{\partial}{\partial \eta_k} A(\boldsymbol{\eta}).
\]</span></p>
<p>where note that <span class="math inline">\(\frac{\partial}{\partial \eta_k} A(\boldsymbol{\eta}) = {\operatorname{E}}[T_k(X)]\)</span>.</p>
</section><section id="summary-of-mle-statistics" class="slide level2">
<h1>Summary of MLE Statistics</h1>
<p>In all of these scenarios, <span class="math inline">\(Z\)</span> converges in distribution to Normal<span class="math inline">\((0,1)\)</span> for large <span class="math inline">\(n\)</span>.</p>
<table style="width:71%;">
<colgroup>
<col style="width: 18%" />
<col style="width: 16%" />
<col style="width: 13%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th style="text-align: center;">MLE</th>
<th style="text-align: center;">Std Err</th>
<th style="text-align: center;"><span class="math inline">\(Z\)</span> Statistic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Binomial<span class="math inline">\((n,p)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\hat{p} = X/n\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\hat{p} - p}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}}\)</span></td>
</tr>
<tr class="even">
<td>Normal<span class="math inline">\((\mu, \sigma^2)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\hat{\mu} = \overline{X}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\hat{\sigma}}{\sqrt{n}}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\hat{\mu} - \mu}{\hat{\sigma}/\sqrt{n}}\)</span></td>
</tr>
<tr class="odd">
<td>Poisson<span class="math inline">\((\lambda)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\hat{\lambda} = \overline{X}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\sqrt{\frac{\hat{\lambda}}{n}}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{\hat{\lambda} - \lambda}{\sqrt{\hat{\lambda}/n}}\)</span></td>
</tr>
</tbody>
</table>
</section><section id="notes" class="slide level2">
<h1>Notes</h1>
<ul>
<li>For the Normal and Poisson distributions, our model is <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> iid from each respective distribution</li>
<li>For the Binomial distribution, our model is <span class="math inline">\(X \sim \mbox{Binomial}(n, p)\)</span><br />
</li>
<li>In the Normal model, <span class="math inline">\(\hat{\sigma} = \sqrt{\frac{\sum_{i=1}^n (X_i - \overline{X})^2}{n}}\)</span> is the MLE of <span class="math inline">\(\sigma\)</span></li>
<li>The above formulas were given in terms of the random variable probability models; on observed data the same formulas are used except we observed data lower case letters, e.g., replace <span class="math inline">\(\overline{X}\)</span> with <span class="math inline">\(\overline{x}\)</span></li>
</ul>
</section><section id="binomial" class="slide level2">
<h1>Binomial</h1>
<p>Approximate <span class="math inline">\((1-\alpha)\)</span>-level two-sided CI:</p>
<p><span class="math display">\[\left(\hat{p} - |z_{\alpha/2}| \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}, \hat{p} + |z_{\alpha/2}| \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \right)\]</span></p>
<p>Hypothesis test, <span class="math inline">\(H_0: p=p_0\)</span> vs <span class="math inline">\(H_1: p \not= p_0\)</span>:</p>
<p><span class="math display">\[z = \frac{\hat{p} - p_0}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}} \mbox{ and } \mbox{p-value} = {\rm Pr}(|Z^*| \geq |z|)\]</span></p>
<p>where <span class="math inline">\(Z^*\)</span> is a Normal<span class="math inline">\((0,1)\)</span> random variable.</p>
</section><section id="normal" class="slide level2">
<h1>Normal</h1>
<p>Approximate <span class="math inline">\((1-\alpha)\)</span>-level two-sided CI:</p>
<p><span class="math display">\[\left(\hat{\mu} - |z_{\alpha/2}| \frac{\hat{\sigma}}{\sqrt{n}}, \hat{\mu} + |z_{\alpha/2}| \frac{\hat{\sigma}}{\sqrt{n}} \right)\]</span></p>
<p>Hypothesis test, <span class="math inline">\(H_0: \mu=\mu_0\)</span> vs <span class="math inline">\(H_1: \mu \not= \mu_0\)</span>:</p>
<p><span class="math display">\[z = \frac{\hat{\mu} - \mu_0}{\hat{\sigma}/\sqrt{n}} \mbox{ and } \mbox{p-value} = {\rm Pr}(|Z^*| \geq |z|)\]</span></p>
<p>where <span class="math inline">\(Z^*\)</span> is a Normal<span class="math inline">\((0,1)\)</span> random variable.</p>
</section><section id="poisson" class="slide level2">
<h1>Poisson</h1>
<p>Approximate <span class="math inline">\((1-\alpha)\)</span>-level two-sided CI:</p>
<p><span class="math display">\[\left(\hat{\lambda} - |z_{\alpha/2}| \sqrt{\frac{\hat{\lambda}}{n}}, \hat{\lambda} + |z_{\alpha/2}| \sqrt{\frac{\hat{\lambda}}{n}} \right)\]</span></p>
<p>Hypothesis test, <span class="math inline">\(H_0: \lambda=\lambda_0\)</span> vs <span class="math inline">\(H_1: \lambda \not= \lambda_0\)</span>:</p>
<p><span class="math display">\[z = \frac{\hat{\lambda} - \lambda_0}{\sqrt{\frac{\hat{\lambda}}{n}}} \mbox{ and } \mbox{p-value} = {\rm Pr}(|Z^*| \geq |z|)\]</span></p>
<p>where <span class="math inline">\(Z^*\)</span> is a Normal<span class="math inline">\((0,1)\)</span> random variable.</p>
</section><section id="one-sided-cis-and-hts" class="slide level2">
<h1>One-Sided CIs and HTs</h1>
<p>The one-sided versions of these approximate confidence intervals and hypothesis tests work analogously.</p>
<p>The procedures shown for the <span class="math inline">\(\mbox{Normal}(\mu, \sigma^2)\)</span> case with known <span class="math inline">\(\sigma^2\)</span> from last week are utilzied with the appropriate subsitutions as in the above examples.</p>
</section></section>
<section><section id="mle-examples-two-samples" class="titleslide slide level1"><h1>MLE Examples: Two-Samples</h1></section><section id="comparing-two-populations" class="slide level2">
<h1>Comparing Two Populations</h1>
<p>So far we have concentrated on analyzing <span class="math inline">\(n\)</span> observations from a single population.</p>
<p>However, suppose that we want to do inference to compare two populations?</p>
<p>The framework we have described so far is easily extended to accommodate this.</p>
</section><section id="two-rvs" class="slide level2">
<h1>Two RVs</h1>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent rv’s then:</p>
<p><span class="math display">\[{\rm E}[X - Y] = {\rm E}[X] - {\rm E}[Y]\]</span></p>
<p><span class="math display">\[{\rm Var}(X-Y) =  {\rm Var}(X) + {\rm Var}(Y)\]</span></p>
</section><section id="two-sample-means" class="slide level2">
<h1>Two Sample Means</h1>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_{n_1}\)</span> be iid rv’s with population mean <span class="math inline">\(\mu_1\)</span> and population variance <span class="math inline">\(\sigma^2_1\)</span>.</p>
<p>Let <span class="math inline">\(Y_1, Y_2, \ldots, Y_{n_2}\)</span> be iid rv’s with population mean <span class="math inline">\(\mu_2\)</span> and population variance <span class="math inline">\(\sigma^2_2\)</span>.</p>
<p>Assume that the two sets of rv’s are independent. Then when the CLT applies to each set of rv’s, as <span class="math inline">\(\min(n_1, n_2) \rightarrow \infty\)</span>, it follows that</p>
<p><span class="math display">\[  \frac{\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2}}} \stackrel{D}{\longrightarrow} \mbox{Normal}(0,1)\]</span></p>
</section><section id="two-mles" class="slide level2">
<h1>Two MLEs</h1>
<p>Suppose <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{{\rm iid}}{\sim} F_{\theta}\)</span> and <span class="math inline">\(Y_1, Y_2, \ldots, Y_m \stackrel{{\rm iid}}{\sim} F_{\gamma}\)</span> with MLEs <span class="math inline">\(\hat{\theta}_n\)</span> and <span class="math inline">\(\hat{\gamma}_m\)</span>, respectively. Then as <span class="math inline">\(\min(n, m) \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[\frac{\hat{\theta}_n - \hat{\gamma}_m - (\theta - \gamma)}{\sqrt{\hat{{\operatorname{se}}}(\hat{\theta}_n)^2 + \hat{{\operatorname{se}}}(\hat{\gamma}_m)^2}} \stackrel{D}{\longrightarrow} \mbox{Normal}(0,1).\]</span></p>
</section><section id="poisson-1" class="slide level2">
<h1>Poisson</h1>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_{n_1}\)</span> be iid <span class="math inline">\(\mbox{Poisson}(\lambda_1)\)</span> and <span class="math inline">\(Y_1, Y_2, \ldots, Y_{n_2}\)</span> be iid <span class="math inline">\(\mbox{Poisson}(\lambda_2)\)</span>.</p>
<p>We have <span class="math inline">\(\hat{\lambda}_1 = \overline{X}\)</span> and <span class="math inline">\(\hat{\lambda}_2 = \overline{Y}\)</span>. As <span class="math inline">\(\min(n_1, n_2) \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[ 
\frac{\hat{\lambda}_1 - \hat{\lambda}_2 - (\lambda_1 - \lambda_2)}{\sqrt{\frac{\hat{\lambda}_1}{n_1} + \frac{\hat{\lambda}_2}{n_2}}} \stackrel{D}{\longrightarrow} \mbox{Normal}(0,1).
\]</span></p>
</section><section id="normal-unequal-variances" class="slide level2">
<h1>Normal (Unequal Variances)</h1>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_{n_1}\)</span> be iid <span class="math inline">\(\mbox{Normal}(\mu_1, \sigma^2_1)\)</span> and <span class="math inline">\(Y_1, Y_2, \ldots, Y_{n_2}\)</span> be iid <span class="math inline">\(\mbox{Normal}(\mu_2, \sigma^2_2)\)</span>.</p>
<p>We have <span class="math inline">\(\hat{\mu}_1 = \overline{X}\)</span> and <span class="math inline">\(\hat{\mu}_2 = \overline{Y}\)</span>. As <span class="math inline">\(\min(n_1, n_2) \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[ 
\frac{\hat{\mu}_1 - \hat{\mu}_2 - (\mu_1 - \mu_2)}{\sqrt{\frac{\hat{\sigma}^2_1}{n_1} + \frac{\hat{\sigma}^2_2}{n_2}}} \stackrel{D}{\longrightarrow} \mbox{Normal}(0,1).
\]</span></p>
</section><section id="normal-equal-variances" class="slide level2">
<h1>Normal (Equal Variances)</h1>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_{n_1}\)</span> be iid <span class="math inline">\(\mbox{Normal}(\mu_1, \sigma^2)\)</span> and <span class="math inline">\(Y_1, Y_2, \ldots, Y_{n_2}\)</span> be iid <span class="math inline">\(\mbox{Normal}(\mu_2, \sigma^2)\)</span>.</p>
<p>We have <span class="math inline">\(\hat{\mu}_1 = \overline{X}\)</span> and <span class="math inline">\(\hat{\mu}_2 = \overline{Y}\)</span>. As <span class="math inline">\(\min(n_1, n_2) \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[ 
\frac{\hat{\mu}_1 - \hat{\mu}_2 - (\mu_1 - \mu_2)}{\sqrt{\frac{\hat{\sigma}^2}{n_1} + \frac{\hat{\sigma}^2}{n_2}}} \stackrel{D}{\longrightarrow} \mbox{Normal}(0,1)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\hat{\sigma}^2 = \frac{\sum_{i=1}^{n_1}(X_i - \overline{X})^2 + \sum_{i=1}^{n_2}(Y_i - \overline{Y})^2}{n_1 + n_2}
\]</span></p>
</section><section id="binomial-1" class="slide level2">
<h1>Binomial</h1>
<p>Let <span class="math inline">\(X \sim \mbox{Binomial}(n_1, p_1)\)</span> and <span class="math inline">\(Y \sim \mbox{Binomial}(n_2, p_2)\)</span>.</p>
<p>We have <span class="math inline">\(\hat{p}_1 = X/n_1\)</span> and <span class="math inline">\(\hat{p}_2 = Y/n_2\)</span>. As <span class="math inline">\(\min(n_1, n_2) \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[ 
\frac{\hat{p}_1 - \hat{p}_2 - (p_1 - p_2)}{\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}} \stackrel{D}{\longrightarrow} \mbox{Normal}(0,1).
\]</span></p>
</section><section id="example-binomial-ci" class="slide level2">
<h1>Example: Binomial CI</h1>
<p>A 95% CI for the difference <span class="math inline">\(p_1 - p_2\)</span> can be obtained by unfolding the above pivotal statistic:</p>
<p><span class="math display">\[\left((\hat{p}_1 - \hat{p}_2) - 1.96 \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}} \right.,\]</span></p>
<p><span class="math display">\[\left. (\hat{p}_1 - \hat{p}_2) + 1.96 \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}} \right)\]</span></p>
</section><section id="example-binomial-ht" class="slide level2">
<h1>Example: Binomial HT</h1>
<p>Suppose we wish to test <span class="math inline">\(H_0: p_1 = p_2\)</span> vs <span class="math inline">\(H_1: p_1 \not= p_2\)</span>.</p>
<p>First form the <span class="math inline">\(z\)</span>-statistic:</p>
<p><span class="math display">\[ 
z = \frac{\hat{p}_1 - \hat{p}_2 }{\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}}.
\]</span></p>
<p>Now, calculate the p-value:</p>
<p><span class="math display">\[
{\rm Pr}(|Z^*| \geq |z|)
\]</span></p>
<p>where <span class="math inline">\(Z^*\)</span> is a Normal(0,1) random variable.</p>
</section></section>
<section><section id="the-t-distribution" class="titleslide slide level1"><h1>The <em>t</em> Distribution</h1></section><section id="normal-unknown-variance" class="slide level2">
<h1>Normal, Unknown Variance</h1>
<p>Suppose a sample of <span class="math inline">\(n\)</span> data points is modeled by <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{{\rm iid}}{\sim} \mbox{Normal}(\mu, \sigma^2)\)</span> where <span class="math inline">\(\sigma^2\)</span> is <em>unknown</em>. Recall that <span class="math inline">\(S = \sqrt{\frac{\sum_{i=1}^n (X_i - \overline{X})^2}{n-1}}\)</span> is the sample standard deviation.</p>
<p>The statistic <span class="math display">\[\frac{\overline{X} - \mu}{S/\sqrt{n}}\]</span></p>
<p>has a <span class="math inline">\(t_{n-1}\)</span> distribution, a <em>t</em>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</p>
</section><section id="aside-chi-square-distribution" class="slide level2">
<h1>Aside: Chi-Square Distribution</h1>
<p>Suppose <span class="math inline">\(Z_1, Z_2, \ldots, Z_v \stackrel{{\rm iid}}{\sim} \mbox{Normal}(0, 1)\)</span>. Then <span class="math inline">\(Z_1^2 + Z_2^2 + \cdots + Z_v^2\)</span> has a <span class="math inline">\(\chi^2_v\)</span> distribution, where <span class="math inline">\(v\)</span> is the degrees of freedom.</p>
<p>This <span class="math inline">\(\chi^2_v\)</span> rv has a <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution#Probability_density_function">pdf</a>, expected value equal to <span class="math inline">\(v\)</span>, and variance equal to <span class="math inline">\(2v\)</span>.</p>
<p>Also,</p>
<p><span class="math display">\[\frac{(n-1) S^2}{\sigma^2} \sim \chi^2_{n-1}.\]</span></p>
</section><section id="theoretical-basis-of-the-t" class="slide level2">
<h1>Theoretical Basis of the <em>t</em></h1>
<p>Suppose that <span class="math inline">\(Z \sim \mbox{Normal}(0,1)\)</span>, <span class="math inline">\(X \sim \chi^2_v\)</span>, and <span class="math inline">\(Z\)</span> and <span class="math inline">\(X\)</span> are independent. Then <span class="math inline">\(\frac{Z}{\sqrt{X/v}}\)</span> has a <span class="math inline">\(t_v\)</span> distribution.</p>
<p>Since <span class="math inline">\(\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim \mbox{Normal}(0,1)\)</span> and <span class="math inline">\(\overline{X}\)</span> and <span class="math inline">\(S^2\)</span> are independent (shown later), it follows that the following has a <span class="math inline">\(t_{n-1}\)</span> distribution:</p>
<p><span class="math display">\[\frac{\overline{X} - \mu}{S/\sqrt{n}}.\]</span></p>
</section><section id="when-is-t-utilized" class="slide level2">
<h1>When Is <em>t</em> Utilized?</h1>
<ul>
<li>The <em>t</em> distribution and its corresponding CI’s and HT’s are utilized when the data are Normal (or approximately Normal) and <span class="math inline">\(n\)</span> is small<br />
</li>
<li>Small typically means that <span class="math inline">\(n &lt; 30\)</span><br />
</li>
<li>In this case the inference based on the <em>t</em> distribution will be more accurate</li>
<li>When <span class="math inline">\(n \geq 30\)</span>, there is very little difference between using <span class="math inline">\(t\)</span>-statistics and <span class="math inline">\(z\)</span>-statistics</li>
</ul>
</section><section id="t-vs-normal" class="slide level2">
<h1><em>t</em> vs Normal</h1>
<p><img src="week05_files/figure-revealjs/unnamed-chunk-6-1.png" width="864" style="display: block; margin: auto;" /></p>
</section><section id="t-percentiles" class="slide level2">
<h1><em>t</em> Percentiles</h1>
<p>We calculated percentiles of the Normal(0,1) distribution (e.g., <span class="math inline">\(z_\alpha\)</span>). We can do the analogous calculation with the <em>t</em> distribution.</p>
<p>Let <span class="math inline">\(t_\alpha\)</span> be the <span class="math inline">\(\alpha\)</span> percentile of the <em>t</em> distribution. Examples:</p>
<pre class="r"><code>&gt; qt(0.025, df=4) # alpha = 0.025
[1] -2.776445
&gt; qt(0.05, df=4)
[1] -2.131847
&gt; qt(0.95, df=4)
[1] 2.131847
&gt; qt(0.975, df=4)
[1] 2.776445</code></pre>
</section><section id="confidence-intervals-2" class="slide level2">
<h1>Confidence Intervals</h1>
<p>Here is a <span class="math inline">\((1-\alpha)\)</span>-level CI for <span class="math inline">\(\mu\)</span> using this distribution:</p>
<p><span class="math display">\[
\left(\hat{\mu} - |t_{\alpha/2}| \frac{s}{\sqrt{n}}, \hat{\mu} + |t_{\alpha/2}| \frac{s}{\sqrt{n}} \right), 
\]</span></p>
<p>where as before <span class="math inline">\(\hat{\mu} = \overline{x}\)</span>. This produces a wider CI than the <span class="math inline">\(z\)</span> statistic analogue.</p>
</section><section id="hypothesis-tests-1" class="slide level2">
<h1>Hypothesis Tests</h1>
<p>Suppose we want to test <span class="math inline">\(H_0: \mu = \mu_0\)</span> vs <span class="math inline">\(H_1: \mu \not= \mu_0\)</span> where <span class="math inline">\(\mu_0\)</span> is a known, given number.</p>
<p>The <em>t</em>-statistic is</p>
<p><span class="math display">\[
t = \frac{\hat{\mu} - \mu_0}{\frac{s}{\sqrt{n}}}
\]</span></p>
<p>with p-value</p>
<p><span class="math display">\[
{\rm Pr}(|T^*| \geq |t|)
\]</span></p>
<p>where <span class="math inline">\(T^* \sim t_{n-1}\)</span>.</p>
</section><section id="two-sample-t-distribution" class="slide level2">
<h1>Two-Sample <em>t</em>-Distribution</h1>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_{n_1} \stackrel{{\rm iid}}{\sim} \mbox{Normal}(\mu_1, \sigma^2_1)\)</span> and <span class="math inline">\(Y_1, Y_2, \ldots, Y_{n_2} \stackrel{{\rm iid}}{\sim} \mbox{Normal}(\mu_2, \sigma^2_2)\)</span> have unequal variances.</p>
<p>We have <span class="math inline">\(\hat{\mu}_1 = \overline{X}\)</span> and <span class="math inline">\(\hat{\mu}_2 = \overline{Y}\)</span>. The unequal variance two-sample t-statistic is <span class="math display">\[ 
t = \frac{\hat{\mu}_1 - \hat{\mu}_2 - (\mu_1 - \mu_2)}{\sqrt{\frac{S^2_1}{n_1} + \frac{S^2_2}{n_2}}}.
\]</span></p>
</section><section id="two-sample-t-distribution-1" class="slide level2">
<h1>Two-Sample <em>t</em>-Distribution</h1>
<p style="font-size: 1em;">
Let <span class="math inline">\(X_1, X_2, \ldots, X_{n_1} \stackrel{{\rm iid}}{\sim} \mbox{Normal}(\mu_1, \sigma^2)\)</span> and <span class="math inline">\(Y_1, Y_2, \ldots, Y_{n_2} \stackrel{{\rm iid}}{\sim} \mbox{Normal}(\mu_2, \sigma^2)\)</span> have equal variance.
</p>
<p>We have <span class="math inline">\(\hat{\mu}_1 = \overline{X}\)</span> and <span class="math inline">\(\hat{\mu}_2 = \overline{Y}\)</span>. The equal variance two-sample t-statistic is</p>
<p><span class="math display">\[ 
t = \frac{\hat{\mu}_1 - \hat{\mu}_2 - (\mu_1 - \mu_2)}{\sqrt{\frac{S^2}{n_1} + \frac{S^2}{n_2}}}.
\]</span></p>
<p>where</p>
<p><span class="math display">\[
S^2 = \frac{\sum_{i=1}^{n_1}(X_i - \overline{X})^2 + \sum_{i=1}^{n_2}(Y_i - \overline{Y})^2}{n_1 + n_2 - 2}.
\]</span></p>
</section><section id="two-sample-t-distributions" class="slide level2">
<h1>Two-Sample <em>t</em>-Distributions</h1>
<p>When the two populations have equal variances, the pivotal <em>t</em>-statistic follows a <span class="math inline">\(t_{n_1 + n_2 -2}\)</span> distribution.</p>
<p>When there are unequal variances, the pivotal <em>t</em>-statistic follows a <em>t</em> distribution where the degrees of freedom comes from an approximation using the Welch–Satterthwaite equation (which R calculates).</p>
</section></section>
<section><section id="inference-in-r" class="titleslide slide level1"><h1>Inference in R</h1></section><section id="bsda-package" class="slide level2">
<h1><code>BSDA</code> Package</h1>
<pre class="r"><code>&gt; install.packages(&quot;BSDA&quot;)</code></pre>
<pre class="r"><code>&gt; library(BSDA)
&gt; str(z.test)
function (x, y = NULL, alternative = &quot;two.sided&quot;, mu = 0, 
    sigma.x = NULL, sigma.y = NULL, conf.level = 0.95)  </code></pre>
</section><section id="example-poisson" class="slide level2">
<h1>Example: Poisson</h1>
<p>Apply <code>z.test()</code>:</p>
<pre class="r"><code>&gt; set.seed(210)</code></pre>
<pre class="r"><code>&gt; n &lt;- 40
&gt; lam &lt;- 14
&gt; x &lt;- rpois(n=n, lambda=lam)
&gt; lam.hat &lt;- mean(x)
&gt; stddev &lt;- sqrt(lam.hat)
&gt; z.test(x=x, sigma.x=stddev, mu=lam)

    One-sample z-Test

data:  x
z = 0.41885, p-value = 0.6753
alternative hypothesis: true mean is not equal to 14
95 percent confidence interval:
 13.08016 15.41984
sample estimates:
mean of x 
    14.25 </code></pre>
</section><section id="direct-calculations" class="slide level2">
<h1>Direct Calculations</h1>
<p>Confidence interval:</p>
<pre class="r"><code>&gt; lam.hat &lt;- mean(x)
&gt; lam.hat
[1] 14.25
&gt; stderr &lt;- sqrt(lam.hat)/sqrt(n)
&gt; lam.hat - abs(qnorm(0.025)) * stderr # lower bound
[1] 13.08016
&gt; lam.hat + abs(qnorm(0.025)) * stderr # upper bound
[1] 15.41984</code></pre>
<p>Hypothesis test:</p>
<pre class="r"><code>&gt; z &lt;- (lam.hat - lam)/stderr
&gt; z # test statistic
[1] 0.4188539
&gt; 2 * pnorm(-abs(z)) # two-sided p-value
[1] 0.6753229</code></pre>
</section><section id="commonly-used-functions" class="slide level2">
<h1>Commonly Used Functions</h1>
<p>R has the following functions for doing inference on some of the distributions we have considered.</p>
<ul>
<li>Normal: <code>t.test()</code></li>
<li>Binomial: <code>binomial.test()</code> or <code>prop.test()</code></li>
<li>Poisson: <code>poisson.test()</code></li>
</ul>
<p>These perform one-sample and two-sample hypothesis testing and confidence interval construction for both the one-sided and two-sided cases.</p>
</section><section id="about-these-functions" class="slide level2">
<h1>About These Functions</h1>
<ul>
<li><p>We covered a convenient, unified MLE framework that allows us to better understand how confidence intervals and hypothesis testing are performed</p></li>
<li><p>However, this framework requires large sample sizes and is not necessarily the best method to apply in all circumstances</p></li>
</ul>
</section><section id="about-these-functions-contd" class="slide level2">
<h1>About These Functions (cont’d)</h1>
<ul>
<li><p>The above R functions are versatile functions for analyzing Normal, Binomial, and Poisson distributed data (or approximations thereof) that use much broader theory and methods than we have covered</p></li>
<li><p>However, the arguments these functions take and the ouput of the functions are in line with the framework that we have covered</p></li>
</ul>
</section><section id="normal-data-davis-data-set" class="slide level2">
<h1>Normal Data: “Davis” Data Set</h1>
<pre class="r"><code>&gt; library(&quot;car&quot;)
&gt; data(&quot;Davis&quot;)</code></pre>
<pre class="r"><code>&gt; htwt &lt;- tbl_df(Davis)
&gt; htwt
# A tibble: 200 × 5
      sex weight height repwt repht
*  &lt;fctr&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;
1       M     77    182    77   180
2       F     58    161    51   159
3       F     53    161    54   158
4       M     68    177    70   175
5       F     59    157    59   155
6       M     76    170    76   165
7       M     76    167    77   165
8       M     69    186    73   180
9       M     71    178    71   175
10      M     65    171    64   170
# ... with 190 more rows</code></pre>
</section><section id="height-vs-weight" class="slide level2">
<h1>Height vs Weight</h1>
<pre class="r"><code>&gt; ggplot(htwt) + 
+ geom_point(aes(x=height, y=weight, color=sex), size=2, alpha=0.5) +
+ scale_colour_manual(values=c(&quot;red&quot;, &quot;blue&quot;))</code></pre>
<p><img src="week05_files/figure-revealjs/unnamed-chunk-16-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="an-error" class="slide level2">
<h1>An Error?</h1>
<pre class="r"><code>&gt; which(htwt$height &lt; 100)
[1] 12
&gt; htwt[12,]
# A tibble: 1 × 5
     sex weight height repwt repht
  &lt;fctr&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;
1      F    166     57    56   163</code></pre>
<pre class="r"><code>&gt; htwt[12,c(2,3)] &lt;- htwt[12,c(3,2)]</code></pre>
</section><section id="updated-height-vs-weight" class="slide level2">
<h1>Updated Height vs Weight</h1>
<pre class="r"><code>&gt; ggplot(htwt) + 
+   geom_point(aes(x=height, y=weight, color=sex), size=2, alpha=0.5) +
+   scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;))</code></pre>
<p><img src="week05_files/figure-revealjs/unnamed-chunk-19-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="density-plots-of-height" class="slide level2">
<h1>Density Plots of Height</h1>
<pre class="r"><code>&gt; ggplot(htwt) + 
+   geom_density(aes(x=height, color=sex), size=1.5) + 
+   scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;))</code></pre>
<p><img src="week05_files/figure-revealjs/unnamed-chunk-20-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="density-plots-of-weight" class="slide level2">
<h1>Density Plots of Weight</h1>
<pre class="r"><code>&gt; ggplot(htwt) + 
+   geom_density(aes(x=weight, color=sex), size=1.5) + 
+   scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;))</code></pre>
<p><img src="week05_files/figure-revealjs/unnamed-chunk-21-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="t.test-function" class="slide level2">
<h1><code>t.test()</code> Function</h1>
<p>From the help file…</p>
<pre><code>Usage

t.test(x, ...)

## Default S3 method:
t.test(x, y = NULL,
       alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;),
       mu = 0, paired = FALSE, var.equal = FALSE,
       conf.level = 0.95, ...)

## S3 method for class &#39;formula&#39;
t.test(formula, data, subset, na.action, ...)</code></pre>
</section><section id="two-sided-test-of-male-height" class="slide level2">
<h1>Two-Sided Test of Male Height</h1>
<pre class="r"><code>&gt; m_ht &lt;- htwt %&gt;% filter(sex==&quot;M&quot;) %&gt;% select(height)
&gt; testresult &lt;- t.test(x = m_ht$height, mu=177)</code></pre>
<pre class="r"><code>&gt; class(testresult)
[1] &quot;htest&quot;
&gt; is.list(testresult)
[1] TRUE</code></pre>
</section><section id="output-of-t.test" class="slide level2">
<h1>Output of <code>t.test()</code></h1>
<pre class="r"><code>&gt; names(testresult)
[1] &quot;statistic&quot;   &quot;parameter&quot;   &quot;p.value&quot;     &quot;conf.int&quot;   
[5] &quot;estimate&quot;    &quot;null.value&quot;  &quot;alternative&quot; &quot;method&quot;     
[9] &quot;data.name&quot;  
&gt; testresult

    One Sample t-test

data:  m_ht$height
t = 1.473, df = 87, p-value = 0.1443
alternative hypothesis: true mean is not equal to 177
95 percent confidence interval:
 176.6467 179.3760
sample estimates:
mean of x 
 178.0114 </code></pre>
</section><section id="tidying-the-output" class="slide level2">
<h1>Tidying the Output</h1>
<pre class="r"><code>&gt; library(broom)
&gt; tidy(testresult)
  estimate statistic   p.value parameter conf.low conf.high
1 178.0114  1.473043 0.1443482        87 176.6467   179.376
             method alternative
1 One Sample t-test   two.sided</code></pre>
</section><section id="two-sided-test-of-female-height" class="slide level2">
<h1>Two-Sided Test of Female Height</h1>
<pre class="r"><code>&gt; f_ht &lt;- htwt %&gt;% filter(sex==&quot;F&quot;) %&gt;% select(height)
&gt; t.test(x = f_ht$height, mu = 164)

    One Sample t-test

data:  f_ht$height
t = 1.3358, df = 111, p-value = 0.1844
alternative hypothesis: true mean is not equal to 164
95 percent confidence interval:
 163.6547 165.7739
sample estimates:
mean of x 
 164.7143 </code></pre>
</section><section id="difference-of-two-means" class="slide level2">
<h1>Difference of Two Means</h1>
<pre class="r"><code>&gt; t.test(x = m_ht$height, y = f_ht$height)

    Welch Two Sample t-test

data:  m_ht$height and f_ht$height
t = 15.28, df = 174.29, p-value &lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 11.57949 15.01467
sample estimates:
mean of x mean of y 
 178.0114  164.7143 </code></pre>
</section><section id="test-with-equal-variances" class="slide level2">
<h1>Test with Equal Variances</h1>
<pre class="r"><code>&gt; htwt %&gt;% group_by(sex) %&gt;% summarize(sd(height))
# A tibble: 2 × 2
     sex `sd(height)`
  &lt;fctr&gt;        &lt;dbl&gt;
1      F     5.659129
2      M     6.440701
&gt; t.test(x = m_ht$height, y = f_ht$height, var.equal = TRUE)

    Two Sample t-test

data:  m_ht$height and f_ht$height
t = 15.519, df = 198, p-value &lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 11.60735 14.98680
sample estimates:
mean of x mean of y 
 178.0114  164.7143 </code></pre>
</section><section id="paired-sample-test-v.-1" class="slide level2">
<h1>Paired Sample Test (v. 1)</h1>
<p>First take the difference between the paired observations. Then apply the one-sample t-test.</p>
<pre class="r"><code>&gt; htwt &lt;- htwt %&gt;% mutate(diffwt = (weight - repwt), 
+                         diffht = (height - repht))
&gt; t.test(x = htwt$diffwt) %&gt;% tidy()
     estimate statistic   p.value parameter   conf.low
1 0.005464481 0.0319381 0.9745564       182 -0.3321223
  conf.high            method alternative
1 0.3430513 One Sample t-test   two.sided
&gt; t.test(x = htwt$diffht) %&gt;% tidy()
  estimate statistic      p.value parameter conf.low conf.high
1 2.076503  13.52629 2.636736e-29       182 1.773603  2.379403
             method alternative
1 One Sample t-test   two.sided</code></pre>
</section><section id="paired-sample-test-v.-2" class="slide level2">
<h1>Paired Sample Test (v. 2)</h1>
<p>Enter each sample into the <code>t.test()</code> function, but use the <code>paired=TRUE</code> argument. This is operationally equivalent to the previous version.</p>
<pre class="r"><code>&gt; t.test(x=htwt$weight, y=htwt$repwt, paired=TRUE) %&gt;% tidy()
     estimate statistic   p.value parameter   conf.low
1 0.005464481 0.0319381 0.9745564       182 -0.3321223
  conf.high        method alternative
1 0.3430513 Paired t-test   two.sided
&gt; t.test(x=htwt$height, y=htwt$repht, paired=TRUE) %&gt;% tidy()
  estimate statistic      p.value parameter conf.low conf.high
1 2.076503  13.52629 2.636736e-29       182 1.773603  2.379403
         method alternative
1 Paired t-test   two.sided
&gt; htwt %&gt;% select(height, repht) %&gt;% na.omit() %&gt;% 
+   summarize(mean(height), mean(repht))
# A tibble: 1 × 2
  `mean(height)` `mean(repht)`
           &lt;dbl&gt;         &lt;dbl&gt;
1       170.5738      168.4973</code></pre>
</section><section id="the-coin-flip-example" class="slide level2">
<h1>The Coin Flip Example</h1>
<p>I flip it 20 times and it lands on heads 16 times.</p>
<ol type="1">
<li>My data is <span class="math inline">\(x=16\)</span> heads out of <span class="math inline">\(n=20\)</span> flips.</li>
<li>My data generation model is <span class="math inline">\(X \sim \mbox{Binomial}(20, p)\)</span>.</li>
<li>I form the statistic <span class="math inline">\(\hat{p} = 16/20\)</span> as an estimate of <span class="math inline">\(p\)</span>.</li>
</ol>
<p>Let’s do hypothesis testing and confidence interval construction on these data.</p>
</section><section id="binom.test" class="slide level2">
<h1><code>binom.test()</code></h1>
<pre class="r"><code>&gt; str(binom.test)
function (x, n, p = 0.5, alternative = c(&quot;two.sided&quot;, 
    &quot;less&quot;, &quot;greater&quot;), conf.level = 0.95)  
&gt; binom.test(x=16, n=20, p = 0.5)

    Exact binomial test

data:  16 and 20
number of successes = 16, number of trials = 20,
p-value = 0.01182
alternative hypothesis: true probability of success is not equal to 0.5
95 percent confidence interval:
 0.563386 0.942666
sample estimates:
probability of success 
                   0.8 </code></pre>
</section><section id="alternative-greater" class="slide level2">
<h1><code>alternative = &quot;greater&quot;</code></h1>
<p>Tests <span class="math inline">\(H_0: p \leq 0.5\)</span> vs. <span class="math inline">\(H_1: p &gt; 0.5\)</span>.</p>
<pre class="r"><code>&gt; binom.test(x=16, n=20, p = 0.5, alternative=&quot;greater&quot;)

    Exact binomial test

data:  16 and 20
number of successes = 16, number of trials = 20,
p-value = 0.005909
alternative hypothesis: true probability of success is greater than 0.5
95 percent confidence interval:
 0.5989719 1.0000000
sample estimates:
probability of success 
                   0.8 </code></pre>
</section><section id="alternative-less" class="slide level2">
<h1><code>alternative = &quot;less&quot;</code></h1>
<p>Tests <span class="math inline">\(H_0: p \geq 0.5\)</span> vs. <span class="math inline">\(H_1: p &lt; 0.5\)</span>.</p>
<pre class="r"><code>&gt; binom.test(x=16, n=20, p = 0.5, alternative=&quot;less&quot;)

    Exact binomial test

data:  16 and 20
number of successes = 16, number of trials = 20,
p-value = 0.9987
alternative hypothesis: true probability of success is less than 0.5
95 percent confidence interval:
 0.0000000 0.9286461
sample estimates:
probability of success 
                   0.8 </code></pre>
</section><section id="prop.test" class="slide level2">
<h1><code>prop.test()</code></h1>
<p>This is a “large <span class="math inline">\(n\)</span>” inference method that is very similar to our <span class="math inline">\(z\)</span>-statistic approach.</p>
<pre class="r"><code>&gt; str(prop.test)
function (x, n, p = NULL, alternative = c(&quot;two.sided&quot;, 
    &quot;less&quot;, &quot;greater&quot;), conf.level = 0.95, correct = TRUE)  
&gt; prop.test(x=16, n=20, p=0.5)

    1-sample proportions test with continuity correction

data:  16 out of 20, null probability 0.5
X-squared = 6.05, df = 1, p-value = 0.01391
alternative hypothesis: true p is not equal to 0.5
95 percent confidence interval:
 0.5573138 0.9338938
sample estimates:
  p 
0.8 </code></pre>
</section><section id="an-observation" class="slide level2">
<h1>An Observation</h1>
<pre class="r"><code>&gt; p &lt;- binom.test(x=16, n=20, p = 0.5)$p.value
&gt; binom.test(x=16, n=20, p = 0.5, conf.level=(1-p))

    Exact binomial test

data:  16 and 20
number of successes = 16, number of trials = 20,
p-value = 0.01182
alternative hypothesis: true probability of success is not equal to 0.5
98.81821 percent confidence interval:
 0.5000000 0.9625097
sample estimates:
probability of success 
                   0.8 </code></pre>
<p>Exercise: Figure out what happened here.</p>
</section><section id="wording-of-surveys" class="slide level2">
<h1>Wording of Surveys</h1>
<p>The way a question is phrased can influence a person’s response. For example, Pew Research Center conducted a survey with the following question:</p>
<p>“As you may know, by 2014 nearly all Americans will be required to have health insurance. [People who do not buy insurance will pay a penalty] while [People who cannot afford it will receive financial help from the government]. Do you approve or disapprove of this policy?”</p>
<p>For each randomly sampled respondent, the statements in brackets were randomized: either they were kept in the order given above, or the two statements were reversed.</p>
<p style="font-size: 0.5em;">
Credit: This example comes from <a href="https://www.openintro.org/stat/textbook.php"><em>Open Intro Statistics</em></a>, Exercise 6.10.
</p>
</section><section id="the-data" class="slide level2">
<h1>The Data</h1>
<p>The following table shows the results of this experiment.</p>
<table style="width:82%;">
<colgroup>
<col style="width: 19%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 20%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="header">
<th>2nd Statement</th>
<th>Sample Size</th>
<th>Approve Law</th>
<th>Disapprove Law</th>
<th>Other</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“people who cannot afford it will receive financial help from the government”</td>
<td>771</td>
<td>47</td>
<td>49</td>
<td>3</td>
</tr>
<tr class="even">
<td>“people who do not buy it will pay a penalty”</td>
<td>732</td>
<td>34</td>
<td>63</td>
<td>3</td>
</tr>
</tbody>
</table>
</section><section id="inference-on-the-difference" class="slide level2">
<h1>Inference on the Difference</h1>
<p>Create and interpret a 90% confidence interval of the difference in approval. Also perform a hyppthesis test that the approval rates are equal.</p>
<pre class="r"><code>&gt; x &lt;- round(c(0.47*771, 0.34*732))
&gt; n &lt;- round(c(771*0.97, 732*0.97))
&gt; prop.test(x=x, n=n, conf.level=0.90)

    2-sample test for equality of proportions with
    continuity correction

data:  x out of n
X-squared = 26.023, df = 1, p-value = 3.374e-07
alternative hypothesis: two.sided
90 percent confidence interval:
 0.08979649 0.17670950
sample estimates:
   prop 1    prop 2 
0.4839572 0.3507042 </code></pre>
</section><section id="confidence-interval" class="slide level2">
<h1>90% Confidence Interval</h1>
<p>Let’s use MLE theory to construct of a two-sided 90% CI.</p>
<pre class="r"><code>&gt; p1.hat &lt;- 0.47
&gt; n1 &lt;- 771
&gt; p2.hat &lt;- 0.34
&gt; n2 &lt;- 732
&gt; stderr &lt;- sqrt(p1.hat*(1-p1.hat)/n1 + p2.hat*(1-p2.hat)/n2)
&gt; 
&gt; # the 90% CI
&gt; (p1.hat - p2.hat) + c(-1,1)*abs(qnorm(0.05))*stderr
[1] 0.08872616 0.17127384</code></pre>
</section><section id="poisson-data-poisson.test" class="slide level2">
<h1>Poisson Data: <code>poisson.test()</code></h1>
<pre class="r"><code>&gt; str(poisson.test)
function (x, T = 1, r = 1, alternative = c(&quot;two.sided&quot;, 
    &quot;less&quot;, &quot;greater&quot;), conf.level = 0.95)  </code></pre>
<p>From the help:</p>
<pre><code>Arguments

x    number of events. A vector of length one or two.

T    time base for event count. A vector of length one or two.

r    hypothesized rate or rate ratio

alternative  indicates the alternative hypothesis and must be one of 
&quot;two.sided&quot;, &quot;greater&quot; or &quot;less&quot;. You can specify just the initial letter.

conf.level  confidence level for the returned confidence interval.</code></pre>
</section><section id="example-rna-seq" class="slide level2">
<h1>Example: RNA-Seq</h1>
<p>RNA-Seq gene expression was measured for p53 lung tissue in 12 healthy individuals and 14 individuals with lung cancer.</p>
<p>The counts were given as follows.</p>
<p>Healthy: 82 64 66 88 65 81 85 87 60 79 80 72</p>
<p>Cancer: 59 50 60 60 78 69 70 67 72 66 66 68 54 62</p>
<p>It is hypothesized that p53 expression is higher in healthy individuals. Test this hypothesis, and form a 99% CI.</p>
</section><section id="h_1-lambda_1-not-lambda_2" class="slide level2">
<h1><span class="math inline">\(H_1: \lambda_1 \not= \lambda_2\)</span></h1>
<pre class="r"><code>&gt; healthy &lt;- c(82, 64, 66, 88, 65, 81, 85, 87, 60, 79, 80, 72)
&gt; cancer &lt;- c(59, 50, 60, 60, 78, 69, 70, 67, 72, 66, 66, 68, 
+             54, 62)</code></pre>
<pre class="r"><code>&gt; poisson.test(x=c(sum(healthy), sum(cancer)), T=c(12,14), 
+              conf.level=0.99)

    Comparison of Poisson rates

data:  c(sum(healthy), sum(cancer)) time base: c(12, 14)
count1 = 909, expected count1 = 835.38, p-value =
0.0005739
alternative hypothesis: true rate ratio is not equal to 1
99 percent confidence interval:
 1.041626 1.330051
sample estimates:
rate ratio 
  1.177026 </code></pre>
</section><section id="h_1-lambda_1-lambda_2" class="slide level2">
<h1><span class="math inline">\(H_1: \lambda_1 &lt; \lambda_2\)</span></h1>
<pre class="r"><code>&gt; poisson.test(x=c(sum(healthy), sum(cancer)), T=c(12,14), 
+              alternative=&quot;less&quot;, conf.level=0.99)

    Comparison of Poisson rates

data:  c(sum(healthy), sum(cancer)) time base: c(12, 14)
count1 = 909, expected count1 = 835.38, p-value =
0.9998
alternative hypothesis: true rate ratio is less than 1
99 percent confidence interval:
 0.000000 1.314529
sample estimates:
rate ratio 
  1.177026 </code></pre>
</section><section id="h_1-lambda_1-lambda_2-1" class="slide level2">
<h1><span class="math inline">\(H_1: \lambda_1 &gt; \lambda_2\)</span></h1>
<pre class="r"><code>&gt; poisson.test(x=c(sum(healthy), sum(cancer)), T=c(12,14), 
+              alternative=&quot;greater&quot;, conf.level=0.99)

    Comparison of Poisson rates

data:  c(sum(healthy), sum(cancer)) time base: c(12, 14)
count1 = 909, expected count1 = 835.38, p-value =
0.0002881
alternative hypothesis: true rate ratio is greater than 1
99 percent confidence interval:
 1.053921      Inf
sample estimates:
rate ratio 
  1.177026 </code></pre>
</section><section id="question" class="slide level2">
<h1>Question</h1>
<p>Which analysis is the more informative and scientifically correct one, and why?</p>
</section></section>
<section><section id="likelihood-ratio-tests" class="titleslide slide level1"><h1>Likelihood Ratio Tests</h1></section><section id="general-set-up" class="slide level2">
<h1>General Set-up</h1>
<p>Most hypothesis testing procedures can be formulated so that a test statistic, <span class="math inline">\(S(\boldsymbol{x})\)</span> is applied to the data <span class="math inline">\(\boldsymbol{x} = (x_1, x_2, \ldots, x_n)^T\)</span> so that:</p>
<ol type="1">
<li><span class="math inline">\(S(\boldsymbol{x}) \geq 0\)</span></li>
<li>The larger <span class="math inline">\(S(\boldsymbol{x})\)</span> is, the more significant the test is (i.e., the more evidence against the null in favor of the alternative)</li>
<li>The p-value is <span class="math inline">\(p(\boldsymbol{x}) = \Pr(S(\boldsymbol{X^*}) \geq S(\boldsymbol{x}))\)</span> where <span class="math inline">\(S(\boldsymbol{X^*})\)</span> is distributed according to the null distribution</li>
</ol>
</section><section id="significance-regions" class="slide level2">
<h1>Significance Regions</h1>
<p>A level <span class="math inline">\(\alpha\)</span> test is a signficance rule (i.e., a rule for calling a test statistically significant) that results in a false positive rate (i.e., Type I error rate) of <span class="math inline">\(\alpha\)</span>. Under our set-up, significance regions take the form:</p>
<p><span class="math display">\[\Gamma_\alpha = \left\{\boldsymbol{x}: S(\boldsymbol{x}) \geq c_{1-\alpha} \right\},\]</span></p>
<p>where <span class="math inline">\(c_{1-\alpha}\)</span> is the <span class="math inline">\((1-\alpha)\)</span> percentile of <span class="math inline">\(S(\boldsymbol{X}^*)\)</span> so that <span class="math inline">\(\Pr(S(\boldsymbol{X}^*) \geq c_{1-\alpha}) = \alpha\)</span>. We restrict <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>.</p>
<p>Note that if <span class="math inline">\(\alpha&#39; \leq \alpha\)</span> then <span class="math inline">\(\Gamma_{\alpha&#39;} \subseteq \Gamma_\alpha\)</span>.</p>
</section><section id="p-values-1" class="slide level2">
<h1>P-values</h1>
<p>A p-value can be defined in terms of significance regions:</p>
<p><span class="math display">\[p(\boldsymbol{x}) = \min \left\{\alpha: \boldsymbol{x} \in \Gamma_\alpha \right\}\]</span></p>
</section><section id="example-wald-test" class="slide level2">
<h1>Example: Wald Test</h1>
<p>Consider the hypothesis test, <span class="math inline">\(H_0: \theta=\theta_0\)</span> vs <span class="math inline">\(H_1: \theta \not= \theta_0\)</span>. Let <span class="math inline">\(\hat{\theta}_n(\boldsymbol{x})\)</span> be the MLE of <span class="math inline">\(\theta\)</span>. We have</p>
<p><span class="math display">\[S(\boldsymbol{x}) = \frac{\left| \hat{\theta}_n(\boldsymbol{x}) - \theta_0 \right|}{\hat{{\operatorname{se}}}\left(\hat{\theta}_n(\boldsymbol{x})\right)},\]</span></p>
<p><span class="math display">\[\Gamma_\alpha = \left\{\boldsymbol{x}: S(\boldsymbol{x}) \geq |z_{\alpha/2}| \right\},\]</span></p>
<p>where <span class="math inline">\(z_{\alpha/2}\)</span> is the <span class="math inline">\(\alpha/2\)</span> percentile of the Normal<span class="math inline">\((0,1)\)</span> distribution.</p>
</section><section id="neyman-pearson-lemma" class="slide level2">
<h1>Neyman-Pearson Lemma</h1>
<p>Suppose we are testing <span class="math inline">\(H_0: \theta = \theta_0\)</span> vs <span class="math inline">\(H_1: \theta = \theta_1\)</span> where in practice <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> are known, fixed quantities. The most powerful test has statistic and significance regions:</p>
<p><span class="math display">\[
S(\boldsymbol{x}) = \frac{f(\boldsymbol{x}; \theta_1)}{f(\boldsymbol{x}; \theta_0)}
= \frac{L(\theta_1; \boldsymbol{x})}{L(\theta_0; \boldsymbol{x})}
\]</span></p>
<p><span class="math display">\[\Gamma_\alpha = \left\{\boldsymbol{x}: S(\boldsymbol{x}) \geq c_{1-\alpha} \right\},\]</span></p>
<p>where <span class="math inline">\(c_{1-\alpha}\)</span> is the <span class="math inline">\((1-\alpha)\)</span> percentile of <span class="math inline">\(S(\boldsymbol{X}^*)\)</span> so that <span class="math inline">\(\operatorname{Pr}_{\theta_0}(S(\boldsymbol{X}^*) \geq c_{1-\alpha}) = \alpha\)</span>.</p>
</section><section id="simple-vs.composite-hypotheses" class="slide level2">
<h1>Simple vs. Composite Hypotheses</h1>
<p>A simple hypothesis is defined in terms of a single value, e.g.,</p>
<ul>
<li><span class="math inline">\(H_0: \mu=0\)</span></li>
<li><span class="math inline">\(H_0: p = p_0\)</span> where <span class="math inline">\(p_0\)</span> is a placehold for a known, fixed number in practice</li>
<li><span class="math inline">\(H_1: \lambda=5\)</span></li>
</ul>
<p>A composite hypothesis is defined by multiple values, e.g.,</p>
<ul>
<li><span class="math inline">\(H_0: \mu \leq 0\)</span> vs <span class="math inline">\(H_1: \mu &gt; 0\)</span></li>
<li><span class="math inline">\(H_0: p_1 = p_2\)</span>, where <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> are two unknown parameters corresponding to two populations</li>
<li><span class="math inline">\(H_1: \mu \not= 0\)</span></li>
</ul>
</section><section id="general-hypothesis-tests" class="slide level2">
<h1>General Hypothesis Tests</h1>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{{\rm iid}}{\sim} F_\theta\)</span> where <span class="math inline">\(\theta \in \Theta\)</span>. Let <span class="math inline">\(\Theta_0, \Theta_1 \subseteq \Theta\)</span> so that <span class="math inline">\(\Theta_0 \cap \Theta_1 = \varnothing\)</span> and <span class="math inline">\(\Theta_0 \cup \Theta_1 = \Theta\)</span>. The hypothesis test is:</p>
<p><span class="math inline">\(H_0: \theta \in \Theta_0\)</span> vs <span class="math inline">\(H_1: \theta \in \Theta_1\)</span></p>
<p>If <span class="math inline">\(\Theta_0\)</span> or <span class="math inline">\(\Theta_1\)</span> contain more than one value then the corresponding hypothesis is composite.</p>
</section><section id="composite-h_0" class="slide level2">
<h1>Composite <span class="math inline">\(H_0\)</span></h1>
<p>The significance regions indexed by their level <span class="math inline">\(\alpha\)</span> are determined so that:</p>
<p><span class="math display">\[\Gamma_\alpha = \left\{\boldsymbol{x}: S(\boldsymbol{x}) \geq c_{1-\alpha} \right\},\]</span></p>
<p>where <span class="math inline">\(c_{1-\alpha}\)</span> is such that <span class="math display">\[\max_{\theta \in \Theta_0} \Pr(S(\boldsymbol{X^*}) \geq c_{1-\alpha}) = \alpha.\]</span></p>
<p>In this case,</p>
<span class="math display">\[\begin{align*}
p(\boldsymbol{x}) &amp; = \min \left\{\alpha: \boldsymbol{x} \in \Gamma_\alpha \right\} \\
 &amp; = \max_{\theta \in \Theta_0} \operatorname{Pr}_\theta (S(\boldsymbol{X}^*) \geq S(\boldsymbol{x}))
\end{align*}\]</span>
</section><section id="generalized-lrt" class="slide level2">
<h1>Generalized LRT</h1>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{{\rm iid}}{\sim} F_\theta\)</span> where <span class="math inline">\(\theta \in \Theta\)</span> and we are testing <span class="math inline">\(H_0: \theta \in \Theta_0\)</span> vs <span class="math inline">\(H_1: \theta \in \Theta_1\)</span>.</p>
<p>The generalized LRT utilizes test statistic and significance regions:</p>
<p><span class="math display">\[\lambda(\boldsymbol{x}) = \frac{\max_{\theta \in \Theta} L(\theta; \boldsymbol{x})}{\max_{\theta \in \Theta_0} L(\theta; \boldsymbol{x})}
= \frac{L\left(\hat{\theta}; \boldsymbol{x}\right)}{L\left(\hat{\theta}_0; \boldsymbol{x}\right)}
\]</span></p>
<p><span class="math display">\[\Gamma_\alpha = \left\{\boldsymbol{x}: \lambda(\boldsymbol{x}) \geq c_{1-\alpha} \right\}\]</span></p>
</section><section id="null-distribution-of-gen.lrt" class="slide level2">
<h1>Null Distribution of Gen. LRT</h1>
<p>The null distribution of <span class="math inline">\(\lambda(\boldsymbol{x})\)</span> under “certain regularity assumptions” can be shown to be such that, as <span class="math inline">\(n \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[ 2 \log \lambda(\boldsymbol{x}) \stackrel{D}{\longrightarrow} \chi^2_v \]</span></p>
<p>where <span class="math inline">\(v = \operatorname{dim}(\Theta) - \operatorname{dim}(\Theta_0)\)</span>.</p>
<p>The significance regions can be more easily written as <span class="math inline">\(\Gamma_\alpha = \left\{\boldsymbol{x}: 2 \log \lambda(\boldsymbol{x}) \geq c_{1-\alpha} \right\}\)</span> where <span class="math inline">\(c_{1-\alpha}\)</span> is the <span class="math inline">\(1-\alpha\)</span> percentile of the <span class="math inline">\(\chi^2_v\)</span> distribution.</p>
</section><section id="example-poisson-1" class="slide level2">
<h1>Example: Poisson</h1>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{{\rm iid}}{\sim} \mbox{Poisson}(\theta)\)</span> where <span class="math inline">\(\theta &gt; 0\)</span> and we are testing <span class="math inline">\(H_0: \theta = \theta_0\)</span> vs <span class="math inline">\(H_1: \theta \not= \theta_0\)</span>. The unconstrained MLE is <span class="math inline">\(\hat{\theta} = \overline{x}\)</span>. The generalized LRT statistic</p>
<p><span class="math display">\[
2 \log \lambda(\boldsymbol{x}) 
= 2 \log \frac{e^{-n \hat{\theta}} \hat{\theta}^{\sum x_i} }{e^{-n \theta_0} \theta_0^{\sum x_i} }
= 2 n \left[ (\theta_0 - \hat{\theta}) - \hat{\theta} \log(\theta_0 / \hat{\theta}) \right]
\]</span></p>
<p>which has an asymptotic <span class="math inline">\(\chi^2_1\)</span> null distribution.</p>
</section><section id="example-normal" class="slide level2">
<h1>Example: Normal</h1>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{{\rm iid}}{\sim} \mbox{Normal}(\mu, \sigma^2)\)</span> and we are testing <span class="math inline">\(H_0: \mu = \mu_0\)</span> vs <span class="math inline">\(H_1: \mu \not= \mu_0\)</span>. The generalized LRT can be applied for multidimensional parameter spaces <span class="math inline">\(\boldsymbol{\Theta}\)</span> as well. The statistic, which has asymptotic null distribution <span class="math inline">\(\chi^2_1\)</span>, is</p>
<p><span class="math display">\[
2 \log \lambda(\boldsymbol{x}) 
= 2 \log \left( \frac{\hat{\sigma}^2_0}{\hat{\sigma}^2} \right)^{n/2}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\hat{\sigma}^2_0 = \frac{\sum_{i=1}^n (x_i - \mu_0)^2}{n}, \quad \hat{\sigma}^2 = \frac{\sum_{i=1}^n (x_i - \overline{x})^2}{n}.
\]</span></p>
</section></section>
<section><section id="extras" class="titleslide slide level1"><h1>Extras</h1></section><section id="source" class="slide level2">
<h1>Source</h1>
<p><a href="https://github.com/jdstorey/asdslectures/blob/master/LICENSE.md">License</a></p>
<p><a href="https://github.com/jdstorey/asdslectures/">Source Code</a></p>
</section><section id="session-information" class="slide level2">
<h1>Session Information</h1>
<section style="font-size: 0.75em;">
<pre class="r"><code>&gt; sessionInfo()
R version 3.3.2 (2016-10-31)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: macOS Sierra 10.12.3

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods  
[7] base     

other attached packages:
 [1] broom_0.4.2     car_2.1-4       BSDA_1.01      
 [4] lattice_0.20-34 e1071_1.6-8     dplyr_0.5.0    
 [7] purrr_0.2.2     readr_1.0.0     tidyr_0.6.1    
[10] tibble_1.2      ggplot2_2.2.1   tidyverse_1.1.1
[13] knitr_1.15.1    magrittr_1.5    devtools_1.12.0

loaded via a namespace (and not attached):
 [1] revealjs_0.8       reshape2_1.4.2     splines_3.3.2     
 [4] haven_1.0.0        colorspace_1.3-2   htmltools_0.3.5   
 [7] mgcv_1.8-17        yaml_2.1.14        nloptr_1.0.4      
[10] foreign_0.8-67     withr_1.0.2        DBI_0.5-1         
[13] modelr_0.1.0       readxl_0.1.1       plyr_1.8.4        
[16] stringr_1.1.0      MatrixModels_0.4-1 munsell_0.4.3     
[19] gtable_0.2.0       rvest_0.3.2        psych_1.6.12      
[22] memoise_1.0.0      evaluate_0.10      labeling_0.3      
[25] forcats_0.2.0      SparseM_1.74       quantreg_5.29     
[28] pbkrtest_0.4-6     parallel_3.3.2     class_7.3-14      
[31] highr_0.6          Rcpp_0.12.9        scales_0.4.1      
[34] backports_1.0.5    jsonlite_1.2       lme4_1.1-12       
[37] mnormt_1.5-5       hms_0.3            digest_0.6.12     
[40] stringi_1.1.2      grid_3.3.2         rprojroot_1.2     
[43] tools_3.3.2        lazyeval_0.2.0     Matrix_1.2-8      
[46] MASS_7.3-45        xml2_1.1.1         lubridate_1.6.0   
[49] minqa_1.2.4        assertthat_0.1     rmarkdown_1.3     
[52] httr_1.2.1         R6_2.2.0           nnet_7.3-12       
[55] nlme_3.1-131      </code></pre>
</section>
</section></section>
    </div>
  </div>

  <script src="libs/reveal.js-3.3.0/lib/js/head.min.js"></script>
  <script src="libs/reveal.js-3.3.0/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display a presentation progress bar
        progress: true,
        // Display the page number of the current slide
        slideNumber: false,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: true,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom


        chalkboard: {
        },

        keyboard: {
          67: function() { RevealChalkboard.toggleNotesCanvas() },    // toggle notes canvas when 'c' is pressed
          66: function() { RevealChalkboard.toggleChalkboard() }, // toggle chalkboard when 'b' is pressed
          46: function() { RevealChalkboard.clear() },    // clear chalkboard when 'DEL' is pressed
           8: function() { RevealChalkboard.reset() },    // reset chalkboard data on current slide when 'BACKSPACE' is pressed
          68: function() { RevealChalkboard.download() }, // downlad recorded chalkboard drawing when 'd' is pressed
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'libs/reveal.js-3.3.0/plugin/zoom-js/zoom.js', async: true },
          { src: 'libs/reveal.js-3.3.0/plugin/chalkboard/chalkboard.js', async: true },
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>


  </body>
</html>
